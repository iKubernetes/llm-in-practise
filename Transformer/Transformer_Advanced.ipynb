{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6b66d7-3395-424d-94ee-8bd01d9b724a",
   "metadata": {},
   "source": [
    "# Transformeré«˜çº§æ¦‚å¿µç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33d368-2e4b-410a-9d06-30387ed31956",
   "metadata": {},
   "source": [
    "## Transformerå„ç»„ä»¶å˜ä½“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cae91-ed93-4a87-b52d-239b0e1bd323",
   "metadata": {},
   "source": [
    "### è‡ªæ³¨æ„åŠ›å˜ä½“ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542eac9-ed57-4101-9472-f756a64e7304",
   "metadata": {},
   "source": [
    "#### æ ‡å‡†å¤šå¤´è‡ªæ³¨æ„åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9669d6-833b-4873-85ef-6c6a399ed4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        # å¯¹Q, K, Våˆ†åˆ«è¿›è¡Œçº¿æ€§æŠ•å½±\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # è®¡ç®—çº¿æ€§æŠ•å½±\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # åˆ†å¤´\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # æ‹¼æ¥å„å¤´\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "d_model, num_heads, seq_len, batch_size = 512, 8, 64, 2\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output = mha(x)\n",
    "print(\"MHAè¾“å‡ºå½¢çŠ¶:\", output.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e5014-94d1-42d0-b50f-3569fed80c5b",
   "metadata": {},
   "source": [
    "#### GQAç¤ºä¾‹\n",
    "\n",
    "GQA å°† $ n_h $ ä¸ªæŸ¥è¯¢å¤´åˆ†æˆ $ n_g $ ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ç»„ $ K_j, V_j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f903e29-c363-4797-a67a-ff91137226cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_groups):\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        # å‚æ•°æ ¡éªŒ\n",
    "        # ç¡®ä¿æ¨¡å‹ç»´åº¦d_modelå¯è¢«å¤´æ•°num_headsæ•´é™¤ï¼Œä»¥ä¾¿åˆ†å‰²ä¸ºå¤šä¸ªæ³¨æ„åŠ›å¤´\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        # ç¡®ä¿å¤´æ•°num_headså¯è¢«åˆ†ç»„æ•°num_groupsæ•´é™¤ï¼Œç¡®ä¿æ¯ç»„åˆ†é…å‡åŒ€çš„æŸ¥è¯¢å¤´\n",
    "        assert num_heads % num_groups == 0, \"num_heads must be divisible by num_groups\"\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹å‚æ•°\n",
    "        self.d_model = d_model  # è¾“å…¥å’Œè¾“å‡ºçš„éšè—çŠ¶æ€ç»´åº¦ï¼ˆå¦‚ 512ï¼‰\n",
    "        self.num_heads = num_heads  # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚8ï¼‰ï¼Œå†³å®šå¹¶è¡Œå¤„ç†çš„å­ç©ºé—´æ•°\n",
    "        self.num_groups = num_groups  # åˆ†ç»„æ•°ï¼ˆå¦‚4ï¼‰ï¼Œå†³å®šé”®å’Œå€¼å‘é‡çš„å…±äº«ç»„æ•°\n",
    "        self.d_head = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆå¦‚512/8=64ï¼‰\n",
    "        self.heads_per_group = num_heads // num_groups  # æ¯ç»„çš„æŸ¥è¯¢å¤´æ•°ï¼ˆå¦‚8/4=2ï¼‰\n",
    "\n",
    "        # å®šä¹‰çº¿æ€§æŠ•å½±å±‚\n",
    "        # æŸ¥è¯¢ï¼ˆQueryï¼‰æŠ•å½±çŸ©é˜µï¼Œå°†è¾“å…¥æ˜ å°„åˆ°æ‰€æœ‰å¤´çš„ç»´åº¦\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "        # é”®ï¼ˆKeyï¼‰æŠ•å½±çŸ©é˜µï¼Œä»…ä¸ºnum_groupsç»„ç”Ÿæˆé”®å‘é‡ï¼Œå‡å°‘å‚æ•°é‡\n",
    "        self.W_k = nn.Linear(d_model, self.d_head * num_groups)  # å½¢çŠ¶ [d_model, d_head * num_groups]\n",
    "        # å€¼ï¼ˆValueï¼‰æŠ•å½±çŸ©é˜µï¼ŒåŒæ ·ä»…ä¸ºnum_groupsç»„ç”Ÿæˆå€¼å‘é‡\n",
    "        self.W_v = nn.Linear(d_model, self.d_head * num_groups)  # å½¢çŠ¶ [d_model, d_head * num_groups]\n",
    "        # è¾“å‡ºæŠ•å½±çŸ©é˜µï¼Œå°†å¤šå¤´è¾“å‡ºæ˜ å°„å›d_modelç»´åº¦\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch_sizeï¼šæ‰¹é‡å¤§å°ï¼Œseq_lenï¼šåºåˆ—é•¿åº¦ï¼Œd_modelï¼šéšè—çŠ¶æ€ç»´åº¦\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # çº¿æ€§æŠ•å½±ï¼Œç”ŸæˆæŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡\n",
    "        Q = self.W_q(x)  # æŸ¥è¯¢å‘é‡ï¼Œ[batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # é”®å‘é‡ï¼Œ[batch_size, seq_len, d_head * num_groups]\n",
    "        V = self.W_v(x)  # å€¼å‘é‡ï¼Œ[batch_size, seq_len, d_head * num_groups]\n",
    "        \n",
    "        # åˆ†å‰²æŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡\n",
    "        # å°†æŸ¥è¯¢å‘é‡Qåˆ†å‰²ä¸ºnum_headsä¸ªå¤´ï¼Œå½¢çŠ¶å˜ä¸º [batch_size, num_heads, seq_len, d_head]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        # å°†é”®å‘é‡Kåˆ†å‰²ä¸ºnum_groupsä¸ªç»„ï¼Œå½¢çŠ¶å˜ä¸º [batch_size, num_groups, seq_len, d_head]\n",
    "        K = K.view(batch_size, seq_len, self.num_groups, self.d_head).transpose(1, 2)\n",
    "        # å°†å€¼å‘é‡Våˆ†å‰²ä¸ºnum_groupsä¸ªç»„ï¼Œå½¢çŠ¶å˜ä¸º [batch_size, num_groups, seq_len, d_head]\n",
    "        V = V.view(batch_size, seq_len, self.num_groups, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # æ‰©å±•é”®å’Œå€¼å‘é‡ä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°\n",
    "        # ç”Ÿæˆç»„ç´¢å¼•ï¼Œé‡å¤ heads_per_group æ¬¡ï¼Œæ˜ å°„æ¯ç»„åˆ°å¯¹åº”çš„æŸ¥è¯¢å¤´\n",
    "        # ä¾‹å¦‚ï¼šnum_groups=4ï¼Œheads_per_group=2ï¼Œç”Ÿæˆ [0,0,1,1,2,2,3,3]\n",
    "        group_idx = torch.arange(self.num_groups).repeat_interleave(self.heads_per_group)\n",
    "        # é€šè¿‡ç´¢å¼•æ‰©å±• Kï¼Œä½¿æ¯ç»„é”®å‘é‡åˆ†é…ç»™ heads_per_group ä¸ªæŸ¥è¯¢å¤´\n",
    "        K = K[:, group_idx, :, :]  # [batch_size, num_heads, seq_len, d_head]\n",
    "        # åŒæ ·æ‰©å±• Vï¼Œä½¿æ¯ç»„å€¼å‘é‡åˆ†é…ç»™ heads_per_group ä¸ªæŸ¥è¯¢å¤´\n",
    "        V = V[:, group_idx, :, :]  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ ä¸ K çš„ç‚¹ç§¯ï¼Œé™¤ä»¥ sqrt(d_head) ä»¥ç¨³å®šæ¢¯åº¦\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        # åº”ç”¨ softmax å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡\n",
    "        attn = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒå€¼å‘é‡ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡\n",
    "        context = torch.matmul(attn, V)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # æ‹¼æ¥å¤šå¤´è¾“å‡º\n",
    "        # å°†ä¸Šä¸‹æ–‡å‘é‡è½¬ç½®å› [batch_size, seq_len, num_heads, d_head]ï¼Œå¹¶å±•å¹³ä¸º [batch_size, seq_len, d_model]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±ï¼Œå°†å¤šå¤´è¾“å‡ºæ˜ å°„å›åŸå§‹ç»´åº¦\n",
    "        output = self.W_o(context)  # [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# è®¾ç½®å‚æ•°ï¼šæ¨¡å‹ç»´åº¦ 512ï¼Œ8 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œ4 ä¸ªåˆ†ç»„ï¼Œåºåˆ—é•¿åº¦ 64ï¼Œæ‰¹é‡å¤§å° 2\n",
    "d_model, num_heads, num_groups, seq_len, batch_size = 512, 8, 4, 64, 2\n",
    "# åˆå§‹åŒ– GQA æ¨¡å‹\n",
    "gqa = GroupedQueryAttention(d_model, num_heads, num_groups)\n",
    "# ç”Ÿæˆéšæœºè¾“å…¥å¼ é‡\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "# æ‰§è¡Œå‰å‘ä¼ æ’­\n",
    "output = gqa(x)\n",
    "# æ‰“å°è¾“å‡ºå½¢çŠ¶ï¼Œé¢„æœŸä¸º [batch_size, seq_len, d_model]\n",
    "print(\"GQAçš„è¾“å‡ºå½¢çŠ¶:\", output.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423274ab-cb6e-4c42-82d3-e38fab1e878e",
   "metadata": {},
   "source": [
    "#### MQAç¤ºä¾‹\n",
    "\n",
    "MQA ä½¿ç”¨å•ä¸€ $ K, V $ï¼Œæ‰€æœ‰ $ n_h $ ä¸ªæŸ¥è¯¢å¤´å…±äº«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac7b63-3719-442b-b986-f6203bd0b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        # çº¿æ€§æŠ•å½±\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.d_head)  # Single K projection\n",
    "        self.W_v = nn.Linear(d_model, self.d_head)  # Single V projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # çº¿æ€§æŠ•å½±\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_head]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_head]\n",
    "        \n",
    "        # å°†Qåˆ†ä¸ºæŸ¥è¯¢å¤´ï¼Œå¹¶æ‰©å±•Kå’ŒV\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        K = K.unsqueeze(1).expand(-1, self.num_heads, -1, -1)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        V = V.unsqueeze(1).expand(-1, self.num_heads, -1, -1)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, V)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # æ‹¼æ¥å„å¤´\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.W_o(context)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "d_model, num_heads, seq_len, batch_size = 512, 8, 64, 2\n",
    "mqa = MultiQueryAttention(d_model, num_heads)\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "output = mqa(x)\n",
    "print(\"MQAçš„è¾“å‡ºå½¢çŠ¶:\", output.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bdaf5-1eb9-4df3-869c-208964b6ac20",
   "metadata": {},
   "source": [
    "#### MLAç¤ºä¾‹\n",
    "\n",
    "- MLAé€šè¿‡ä¸‹æŠ•å½±ç”Ÿæˆä½ç»´æ½œåœ¨å‘é‡ $ C^{KV}, C^Q \\in \\mathbb{R}^{L \\times d_{\\text{latent}}} $ï¼Œä»…ç¼“å­˜ $ C^{KV} $\n",
    "- è§£è€¦RoPEå°†æ½œåœ¨å‘é‡åˆ†ä¸ºéä½ç½®ï¼ˆnopeï¼‰å’Œä½ç½®ï¼ˆropeï¼‰éƒ¨åˆ†ï¼Œç®€åŒ–å®ç°ä»…å¯¹ rope éƒ¨åˆ†åº”ç”¨æ—‹è½¬\n",
    "  - RoPEæŒ‡çš„æ˜¯Rotary Position Embeddingï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ï¼Œæ˜¯ä¸€ç§åœ¨Transformeræ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨çš„ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•\n",
    "  - MLAä¸­ï¼ŒRoPEè¢«è¿›ä¸€æ­¥ä¼˜åŒ–ä¸ºè§£è€¦RoPEï¼ˆDecoupled RoPEï¼‰ï¼Œä»¥é€‚åº”MLAçš„ä½ç§©å‹ç¼©æœºåˆ¶ï¼Œç¡®ä¿ä½ç½®ä¿¡æ¯åœ¨å‹ç¼©åçš„æ½œåœ¨ç©ºé—´ä¸­èƒ½å¤Ÿæœ‰æ•ˆèå…¥æ³¨æ„åŠ›è®¡ç®—\n",
    "- è®¡ç®—å¤æ‚åº¦å¢åŠ ï¼ˆä¸‹æŠ•å½±å’Œä¸ŠæŠ•å½±ï¼‰ï¼Œå†…å­˜å¤æ‚åº¦ä¸º $ O(L \\cdot d_{\\text{latent}}) $ï¼Œè¿œä½äºMHAå’ŒGQA\n",
    "\n",
    "**æŠ•å½±å±‚**\n",
    "\n",
    "- ä¸‹æŠ•å½± $ W_{\\text{dkv}}, W_{\\text{dq}} $ å°†é«˜ç»´è¾“å…¥å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå‡å°‘å†…å­˜å ç”¨ã€‚\n",
    "- ä¸ŠæŠ•å½± $ W_{\\text{uq}}, W_{\\text{uk}}, W_{\\text{uv}} $ æ¢å¤å¤šå¤´ç»´åº¦ï¼Œä¿æŒè¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "- è¾“å‡ºæŠ•å½± $ W_o $ æ•´åˆå¤šå¤´è¾“å‡ºã€‚\n",
    "\n",
    "**å‰å‘ä¼ æ’­**\n",
    "\n",
    "- ä¸‹æŠ•å½±ï¼šå°†è¾“å…¥ $ x $ å‹ç¼©ä¸º $ C_{\\text{kv}}, C_q $ï¼Œå½¢çŠ¶ä¸º $ [batch_size, seq_len, d_{\\text{latent}}] $ã€‚\n",
    "- åº”ç”¨ RoPEï¼šå¯¹ $ C_{\\text{kv}}, C_q $ åº”ç”¨è§£è€¦ RoPEï¼Œèå…¥ä½ç½®ä¿¡æ¯ã€‚\n",
    "- ä¸ŠæŠ•å½±ï¼šå°†æ½œåœ¨å‘é‡æ¢å¤ä¸ºå¤šå¤´ $ Q, K, V $ï¼Œå½¢çŠ¶ä¸º $ [batch_size, num_heads, seq_len, d_head] $ã€‚\n",
    "- æ³¨æ„åŠ›è®¡ç®—ï¼šæ‰§è¡Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ã€‚\n",
    "- è¾“å‡ºæ•´åˆï¼šæ‹¼æ¥å¤šå¤´è¾“å‡ºå¹¶é€šè¿‡ $ W_o $ æŠ•å½±ï¼Œè¿”å›æœ€ç»ˆè¾“å‡ºå’Œ $ C_{\\text{kv}} $ï¼ˆç”¨äºæ¨ç†ç¼“å­˜ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b49cb7-0b71-41dc-93a9-93ff7388ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_latent, d_rope=64):\n",
    "        super(MultiHeadLatentAttention, self).__init__()\n",
    "        \n",
    "        # å‚æ•°æ ¡éªŒ\n",
    "        # ç¡®ä¿æ¨¡å‹ç»´åº¦d_modelå¯è¢«å¤´æ•°num_headsæ•´é™¤ï¼Œä»¥ä¾¿åˆ†å‰²ä¸ºå¤šä¸ªæ³¨æ„åŠ›å¤´\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        # ç¡®ä¿æ½œåœ¨ç»´åº¦d_latentè¶³å¤Ÿå¤§ä»¥åŒ…å«RoPEç»´åº¦d_rope\n",
    "        assert d_latent >= d_rope, \"d_latent must be at least d_rope\"\n",
    "        # ç¡®ä¿RoPEç»´åº¦ä¸ºå¶æ•°ï¼Œå› ä¸ºRoPEéœ€è¦æˆå¯¹å¤„ç†ç»´åº¦\n",
    "        assert d_rope % 2 == 0, \"d_rope must be even for RoPE\"\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹å‚æ•°\n",
    "        self.d_model = d_model  # è¾“å…¥å’Œè¾“å‡ºçš„éšè—çŠ¶æ€ç»´åº¦\n",
    "        self.num_heads = num_heads  # æ³¨æ„åŠ›å¤´æ•°\n",
    "        self.d_head = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦\n",
    "        self.d_latent = d_latent  # æ½œåœ¨ç©ºé—´ç»´åº¦ï¼Œç”¨äºå‹ç¼©é”®å’Œå€¼\n",
    "        self.d_rope = d_rope  # ç”¨äºRoPEçš„ä½ç½®ç¼–ç ç»´åº¦\n",
    "        self.d_nope = d_latent - d_rope  # éä½ç½®éƒ¨åˆ†ï¼ˆnopeï¼‰çš„ç»´åº¦\n",
    "\n",
    "        # ä¸‹æŠ•å½±å±‚ï¼šå°†è¾“å…¥ä»d_modelå‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´d_latent\n",
    "        self.W_dkv = nn.Linear(d_model, d_latent)  # é”®å’Œå€¼çš„å…±äº«ä¸‹æŠ•å½±çŸ©é˜µ\n",
    "        self.W_dq = nn.Linear(d_model, d_latent)   # æŸ¥è¯¢çš„ä¸‹æŠ•å½±çŸ©é˜µ\n",
    "        \n",
    "        # ä¸ŠæŠ•å½±å±‚ï¼šå°†æ½œåœ¨å‘é‡ä»d_latentæ¢å¤åˆ°å¤šå¤´ç»´åº¦d_model\n",
    "        self.W_uq = nn.Linear(d_latent, d_model)  # æŸ¥è¯¢ä¸ŠæŠ•å½±çŸ©é˜µ\n",
    "        self.W_uk = nn.Linear(d_latent, d_model)  # é”®ä¸ŠæŠ•å½±çŸ©é˜µ\n",
    "        self.W_uv = nn.Linear(d_latent, d_model)  # å€¼ä¸ŠæŠ•å½±çŸ©é˜µ\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±å±‚ï¼šå°†å¤šå¤´è¾“å‡ºæ˜ å°„å›d_modelç»´åº¦\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # åˆå§‹åŒ–RoPEçš„é¢‘ç‡å‚æ•°ï¼Œç”¨äºä½ç½®ç¼–ç \n",
    "        self.freqs = self._init_rope_frequencies(d_rope // 2)\n",
    "\n",
    "    def _init_rope_frequencies(self, dim):\n",
    "        # åˆå§‹åŒ–RoPEçš„é¢‘ç‡å‚æ•°\n",
    "        # ä½¿ç”¨å…¬å¼ 1 / (10000^(2i/d)) ç”Ÿæˆé¢‘ç‡ï¼Œdimä¸ºd_rope // 2\n",
    "        # torch.arange(0, dim, 1) ç”Ÿæˆ0åˆ° dim-1 çš„åºåˆ—ï¼Œä»£è¡¨é¢‘ç‡ç´¢å¼•\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 1).float() / dim))\n",
    "        return inv_freq  # è¿”å›é¢‘ç‡å¼ é‡ï¼Œå½¢çŠ¶ä¸º [dim]\n",
    "\n",
    "    def apply_decoupled_rope(self, x, positions):\n",
    "        # åº”ç”¨è§£è€¦ RoPEï¼Œå°†æ½œåœ¨å‘é‡åˆ†ä¸ºéä½ç½®ï¼ˆnopeï¼‰å’Œä½ç½®ï¼ˆropeï¼‰éƒ¨åˆ†ï¼Œä»…å¯¹ rope éƒ¨åˆ†åº”ç”¨æ—‹è½¬\n",
    "        # è¾“å…¥ x: [batch_size, seq_len, d_latent]ï¼Œpositions: [seq_len]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # å°†æ½œåœ¨å‘é‡åˆ†å‰²ä¸ºéä½ç½®éƒ¨åˆ†å’Œä½ç½®éƒ¨åˆ†\n",
    "        x_nope = x[..., :self.d_nope]  # éä½ç½®éƒ¨åˆ†ï¼Œ[batch_size, seq_len, d_nope]\n",
    "        x_rope = x[..., self.d_nope:]  # ä½ç½®éƒ¨åˆ†ï¼Œ[batch_size, seq_len, d_rope]\n",
    "        \n",
    "        # å‡†å¤‡ RoPE çš„é¢‘ç‡å‚æ•°\n",
    "        freqs = self.freqs.to(x.device)  # å°†é¢‘ç‡ç§»åˆ°è¾“å…¥è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰\n",
    "        t = positions[:, None].float()  # ä½ç½®ç´¢å¼•æ‰©å±•ä¸º [seq_len, 1]\n",
    "        freqs = t * freqs[None, :]  # è®¡ç®—æ—‹è½¬è§’åº¦ï¼Œ[seq_len, d_rope // 2]\n",
    "        cos_freq = torch.cos(freqs)  # ä½™å¼¦å€¼ï¼Œ[seq_len, d_rope // 2]\n",
    "        sin_freq = torch.sin(freqs)  # æ­£å¼¦å€¼ï¼Œ[seq_len, d_rope // 2]\n",
    "        \n",
    "        # å°† rope éƒ¨åˆ†åˆ†æˆä¸¤åŠï¼Œç”¨äºæˆå¯¹æ—‹è½¬\n",
    "        x1 = x_rope[..., :self.d_rope // 2]  # å‰åŠéƒ¨åˆ†ï¼Œ[batch_size, seq_len, d_rope // 2]\n",
    "        x2 = x_rope[..., self.d_rope // 2:]  # ååŠéƒ¨åˆ†ï¼Œ[batch_size, seq_len, d_rope // 2]\n",
    "        \n",
    "        # åº”ç”¨ RoPE æ—‹è½¬\n",
    "        # æ—‹è½¬å…¬å¼ï¼š(x1, x2) -> (x1 * cos - x2 * sin, x1 * sin + x2 * cos)\n",
    "        x_rope_rotated = torch.cat([\n",
    "            x1 * cos_freq - x2 * sin_freq,  # æ—‹è½¬åçš„å‰åŠéƒ¨åˆ†\n",
    "            x1 * sin_freq + x2 * cos_freq   # æ—‹è½¬åçš„ååŠéƒ¨åˆ†\n",
    "        ], dim=-1)  # æ‹¼æ¥ï¼Œ[batch_size, seq_len, d_rope]\n",
    "        \n",
    "        # æ‹¼æ¥éä½ç½®éƒ¨åˆ†å’Œæ—‹è½¬åçš„ä½ç½®éƒ¨åˆ†\n",
    "        return torch.cat([x_nope, x_rope_rotated], dim=-1)  # [batch_size, seq_len, d_latent]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å‰å‘ä¼ æ’­ï¼Œè¾“å…¥ x: [batch_size, seq_len, d_model]\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # ä¸‹æŠ•å½±ï¼šå°†è¾“å…¥å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´\n",
    "        C_kv = self.W_dkv(x)  # é”®å’Œå€¼æ½œåœ¨å‘é‡ï¼Œ[batch_size, seq_len, d_latent]\n",
    "        C_q = self.W_dq(x)    # æŸ¥è¯¢æ½œåœ¨å‘é‡ï¼Œ[batch_size, seq_len, d_latent]\n",
    "        \n",
    "        # åº”ç”¨è§£è€¦ RoPEï¼Œä¸ºæ½œåœ¨å‘é‡æ·»åŠ ä½ç½®ä¿¡æ¯\n",
    "        positions = torch.arange(seq_len, device=x.device)  # ä½ç½®ç´¢å¼• [0, 1, ..., seq_len-1]\n",
    "        C_kv = self.apply_decoupled_rope(C_kv, positions)  # å¯¹é”®å’Œå€¼æ½œåœ¨å‘é‡åº”ç”¨ RoPE\n",
    "        C_q = self.apply_decoupled_rope(C_q, positions)    # å¯¹æŸ¥è¯¢æ½œåœ¨å‘é‡åº”ç”¨ RoPE\n",
    "        \n",
    "        # ä¸ŠæŠ•å½±ï¼šå°†æ½œåœ¨å‘é‡æ¢å¤åˆ°å¤šå¤´ç»´åº¦\n",
    "        Q = self.W_uq(C_q).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # æŸ¥è¯¢ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        K = self.W_uk(C_kv).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # é”®ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        V = self.W_uv(C_kv).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # å€¼ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)  # æ³¨æ„åŠ›åˆ†æ•°ï¼Œ[batch_size, num_heads, seq_len, seq_len]\n",
    "        attn = torch.softmax(scores, dim=-1)  # æ³¨æ„åŠ›æƒé‡ï¼Œå½’ä¸€åŒ–\n",
    "        context = torch.matmul(attn, V)  # ä¸Šä¸‹æ–‡å‘é‡ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # æ‹¼æ¥å¤šå¤´è¾“å‡º\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        output = self.W_o(context)  # æœ€ç»ˆè¾“å‡ºï¼Œ[batch_size, seq_len, d_model]\n",
    "        return output, C_kv  # è¿”å›è¾“å‡ºå’Œé”®-å€¼æ½œåœ¨å‘é‡ï¼ˆç”¨äºç¼“å­˜ï¼‰\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "d_model, num_heads, d_latent, d_rope, seq_len, batch_size = 512, 8, 128, 64, 64, 2\n",
    "mla = MultiHeadLatentAttention(d_model, num_heads, d_latent, d_rope)\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "output, C_kv = mla(x)\n",
    "print(\"MLAçš„è¾“å‡ºå½¢çŠ¶:\", output.shape)  # [batch_size, seq_len, d_model]\n",
    "print(\"MLA KV cacheçš„å½¢çŠ¶:\", C_kv.shape)  # [batch_size, seq_len, d_latent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ce0e5-67af-40f3-aeec-5466e4a8d595",
   "metadata": {},
   "source": [
    "### å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0e301-71bc-44a7-b275-2b479e18281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, window_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        \n",
    "        # å‚æ•°æ ¡éªŒ\n",
    "        # ç¡®ä¿æ¨¡å‹ç»´åº¦d_modelå¯è¢«å¤´æ•°num_headsæ•´é™¤ï¼Œä»¥ä¾¿åˆ†å‰²ä¸ºå¤šä¸ªæ³¨æ„åŠ›å¤´\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹å‚æ•°\n",
    "        self.d_model = d_model  # è¾“å…¥å’Œè¾“å‡ºçš„éšè—çŠ¶æ€ç»´åº¦ï¼ˆå¦‚ 512ï¼‰\n",
    "        self.num_heads = num_heads  # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚8ï¼‰ï¼Œå†³å®šå¹¶è¡Œå¤„ç†çš„å­ç©ºé—´æ•°\n",
    "        self.d_head = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆå¦‚ 512/8=64ï¼‰\n",
    "        self.window_size = window_size  # æ»‘åŠ¨çª—å£çš„åŠå®½ï¼ˆå·¦å³å„ window_size ä¸ª tokenï¼‰\n",
    "\n",
    "        # å®šä¹‰çº¿æ€§æŠ•å½±å±‚\n",
    "        # æŸ¥è¯¢ï¼ˆQueryï¼‰æŠ•å½±çŸ©é˜µï¼Œå°†è¾“å…¥æ˜ å°„åˆ°æ‰€æœ‰å¤´çš„ç»´åº¦\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "        # é”®ï¼ˆKeyï¼‰æŠ•å½±çŸ©é˜µï¼Œç”Ÿæˆæ‰€æœ‰å¤´çš„é”®å‘é‡\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "        # å€¼ï¼ˆValueï¼‰æŠ•å½±çŸ©é˜µï¼Œç”Ÿæˆæ‰€æœ‰å¤´çš„å€¼å‘é‡\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "        # è¾“å‡ºæŠ•å½±çŸ©é˜µï¼Œå°†å¤šå¤´è¾“å‡ºæ˜ å°„å›d_modelç»´åº¦\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # å½¢çŠ¶ [d_model, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å‰å‘ä¼ æ’­ï¼Œè¾“å…¥ x: [batch_size, seq_len, d_model]\n",
    "        # batch_sizeï¼šæ‰¹é‡å¤§å°ï¼Œseq_lenï¼šåºåˆ—é•¿åº¦ï¼Œd_modelï¼šéšè—çŠ¶æ€ç»´åº¦\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # çº¿æ€§æŠ•å½±ï¼Œç”ŸæˆæŸ¥è¯¢ã€é”®å’Œå€¼å‘é‡\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # æŸ¥è¯¢ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # é”®ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)  # å€¼ï¼Œ[batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡å‘é‡ï¼Œç”¨äºå­˜å‚¨æ³¨æ„åŠ›è®¡ç®—ç»“æœ\n",
    "        context = torch.zeros_like(Q)  # [batch_size, num_heads, seq_len, d_head]\n",
    "        \n",
    "        # æ»‘åŠ¨çª—å£å±€éƒ¨æ³¨æ„åŠ›\n",
    "        for i in range(seq_len):\n",
    "            # å®šä¹‰å½“å‰token içš„çª—å£èŒƒå›´\n",
    "            # çª—å£è¦†ç›– [i-window_size, i+window_size]ï¼Œå¹¶å¤„ç†è¾¹ç•Œæƒ…å†µ\n",
    "            start = max(0, i - self.window_size)  # çª—å£èµ·å§‹ä½ç½®ï¼Œæœ€å°ä¸º0\n",
    "            end = min(seq_len, i + self.window_size + 1)  # çª—å£ç»“æŸä½ç½®ï¼Œæœ€å¤§ä¸ºseq_len\n",
    "            \n",
    "            # æå–å±€éƒ¨çª—å£å†…çš„é”®å’Œå€¼å‘é‡\n",
    "            K_local = K[:, :, start:end, :]  # [batch_size, num_heads, window_size, d_head]\n",
    "            V_local = V[:, :, start:end, :]  # [batch_size, num_heads, window_size, d_head]\n",
    "            \n",
    "            # è·å–å½“å‰token içš„æŸ¥è¯¢å‘é‡\n",
    "            q_i = Q[:, :, i:i+1, :]  # [batch_size, num_heads, 1, d_head]\n",
    "            \n",
    "            # è®¡ç®—å±€éƒ¨æ³¨æ„åŠ›åˆ†æ•°\n",
    "            # q_iä¸K_localç‚¹ç§¯ï¼Œé™¤ä»¥sqrt(d_head)ç¨³å®šæ¢¯åº¦\n",
    "            scores = torch.matmul(q_i, K_local.transpose(-2, -1)) / math.sqrt(self.d_head)  # [batch_size, num_heads, 1, window_size]\n",
    "            \n",
    "            # åº”ç”¨softmaxå½’ä¸€åŒ–ï¼Œå¾—åˆ°å±€éƒ¨æ³¨æ„åŠ›æƒé‡\n",
    "            attn = torch.softmax(scores, dim=-1)  # [batch_size, num_heads, 1, window_size]\n",
    "            \n",
    "            # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒå±€éƒ¨å€¼å‘é‡ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡\n",
    "            context[:, :, i:i+1, :] = torch.matmul(attn, V_local)  # [batch_size, num_heads, 1, d_head]\n",
    "        \n",
    "        # æ‹¼æ¥å¤šå¤´è¾“å‡º\n",
    "        # å°†ä¸Šä¸‹æ–‡å‘é‡è½¬ç½®å› [batch_size, seq_len, num_heads, d_head]ï¼Œå¹¶å±•å¹³ä¸º [batch_size, seq_len, d_model]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±ï¼Œå°†å¤šå¤´è¾“å‡ºæ˜ å°„å›åŸå§‹ç»´åº¦\n",
    "        output = self.W_o(context)  # [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "# è®¾ç½®å‚æ•°ï¼šæ¨¡å‹ç»´åº¦512ï¼Œ8ä¸ªæ³¨æ„åŠ›å¤´ï¼Œçª—å£åŠå®½16ï¼Œåºåˆ—é•¿åº¦64ï¼Œæ‰¹é‡å¤§å°2\n",
    "d_model, num_heads, window_size, seq_len, batch_size = 512, 8, 16, 64, 2\n",
    "# åˆå§‹åŒ–å±€éƒ¨æ³¨æ„åŠ›æ¨¡å‹\n",
    "local_attn = LocalAttention(d_model, num_heads, window_size)\n",
    "# ç”Ÿæˆéšæœºè¾“å…¥å¼ é‡\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "# æ‰§è¡Œå‰å‘ä¼ æ’­\n",
    "output = local_attn(x)\n",
    "# æ‰“å°è¾“å‡ºå½¢çŠ¶ï¼Œé¢„æœŸä¸º[batch_size, seq_len, d_model]\n",
    "print(\"Local Attentionçš„è¾“å‡ºå½¢çŠ¶:\", output.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3b0d7-f11b-41c5-8f43-acd9c9b62a29",
   "metadata": {},
   "source": [
    "## å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a57e26-a962-4f56-9ce3-7ec58abfe382",
   "metadata": {},
   "source": [
    "### å±‚å½’ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e412b-4a28-499e-a58c-caf9c8b0f42d",
   "metadata": {},
   "source": [
    "### æ®‹å·®è¿æ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e33dd-9c25-4240-abbf-1ac480ed8968",
   "metadata": {},
   "source": [
    "#### åŒæ®‹å·®è¿æ¥\n",
    "\n",
    "å°†è¾“å…¥ç‰¹å¾åˆ†æˆä¸¤æ¡è·¯å¾„åˆ†åˆ«è¿›è¡Œå¤„ç†ï¼Œä¸€ç›´è´Ÿè´£ç½‘ç»œå†…éƒ¨è®¡ç®—ï¼Œä¸€ä¸ªè´Ÿè´£ä¿è¯æ¢¯åº¦ç¨³å®šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf9806-0a25-4431-b759-530f5393804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# å®šä¹‰Decoder-Only Transformerçš„å±‚ï¼Œèå…¥æ ‡å‡†çš„åŒæ®‹å·®ResiDualè®¾è®¡\n",
    "class ResiDualTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ResiDualTransformerBlockæ¨¡å—ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - d_model: æ¨¡å‹çš„åµŒå…¥ç»´åº¦ï¼ˆhidden sizeï¼‰ã€‚\n",
    "        - n_heads: è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚\n",
    "        - d_ff: å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„ä¸­é—´ç»´åº¦ã€‚\n",
    "        - dropout: dropoutç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "        \"\"\"\n",
    "        super(ResiDualTransformerBlock, self).__init__()\n",
    "        # è‡ªæ³¨æ„åŠ›å±‚ï¼ˆmasked multi-head attentionï¼‰ï¼Œå¯¹åº”f(Attn(Â·))\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        # å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œå¯¹åº”g(Â·)ï¼Œé€šå¸¸æ˜¯ä¸¤ä¸ªçº¿æ€§å±‚åŠ æ¿€æ´»å‡½æ•°\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢\n",
    "            nn.GELU(),                 # GELUæ¿€æ´»å‡½æ•°\n",
    "            nn.Linear(d_ff, d_model),  # ç¬¬äºŒå±‚çº¿æ€§å˜æ¢\n",
    "            nn.Dropout(dropout)        # dropoutå±‚\n",
    "        )\n",
    "        # å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰æ¨¡å—ï¼Œç”¨äºéçº¿æ€§å˜æ¢\n",
    "        self.ln1 = nn.LayerNorm(d_model)  # ç¬¬ä¸€æ¬¡LNï¼Œç”¨äºè¾“å…¥å½’ä¸€åŒ–\n",
    "        self.ln2 = nn.LayerNorm(d_model)  # ç¬¬äºŒæ¬¡LNï¼Œç”¨äºç¬¬ä¸€æ¬¡æ®‹å·®åå½’ä¸€åŒ–\n",
    "        self.ln3 = nn.LayerNorm(d_model)  # ç¬¬ä¸‰æ¬¡LNï¼Œç”¨äºç¬¬äºŒæ¬¡æ®‹å·®åå½’ä¸€åŒ–\n",
    "        # dropoutå±‚ï¼Œç”¨äºæ³¨æ„åŠ›è¾“å‡º\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œå®ç°æ ‡å‡†çš„åŒæ®‹å·®ResiDualæµç¨‹ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, d_model)ã€‚\n",
    "        - attn_mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºDecoder-Onlyçš„å› æœæ³¨æ„åŠ›ï¼ˆcausal maskï¼‰ã€‚\n",
    "        \n",
    "        æµç¨‹:\n",
    "        1. å¯¹è¾“å…¥xè¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œå¾—åˆ°x_lnã€‚\n",
    "        2. è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼ˆmaskedï¼‰ï¼Œå¹¶ä¸x_lnç›¸åŠ ï¼Œå½¢æˆç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥ï¼ˆy_local1ï¼‰ã€‚\n",
    "        3. å¯¹y_local1è¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œå¾—åˆ°y_local1_lnã€‚\n",
    "        4. é€šè¿‡FFNå¤„ç†y_local1_lnï¼Œå¹¶ä¸y_local1ç›¸åŠ ï¼Œå½¢æˆç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥ï¼ˆy_local2ï¼‰ã€‚\n",
    "        5. å¯¹y_local2è¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œå¾—åˆ°ä¸­é—´è¾“å‡ºã€‚\n",
    "        6. æœ€åå°†åŸå§‹è¾“å…¥xä¸ä¸­é—´è¾“å‡ºç›¸åŠ ï¼Œå½¢æˆå…¨å±€æ®‹å·®è¿æ¥ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºyã€‚\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1: è¾“å…¥å½’ä¸€åŒ–\n",
    "        x_ln = self.ln1(x)\n",
    "        \n",
    "        # æ­¥éª¤2: è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼ˆf(Attn(x_ln))ï¼‰ï¼Œæ³¨æ„è¦ä½¿ç”¨attn_maskå®ç°Decoder-Onlyçš„å› æœæ³¨æ„åŠ›\n",
    "        attn_output, _ = self.self_attn(x_ln, x_ln, x_ln, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        \n",
    "        # ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥\n",
    "        y_local1 = x_ln + attn_output\n",
    "        \n",
    "        # æ­¥éª¤3: å¯¹y_local1è¿›è¡Œå½’ä¸€åŒ–\n",
    "        y_local1_ln = self.ln2(y_local1)\n",
    "        \n",
    "        # æ­¥éª¤4: å‰é¦ˆç½‘ç»œè®¡ç®—ï¼ˆg(FFN(y_local1_ln))ï¼‰\n",
    "        ffn_output = self.ffn(y_local1_ln)\n",
    "        \n",
    "        # ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥\n",
    "        y_local2 = y_local1 + ffn_output\n",
    "        \n",
    "        # æ­¥éª¤5: å¯¹y_local2è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°ä¸­é—´è¾“å‡º\n",
    "        intermediate = self.ln3(y_local2)\n",
    "        \n",
    "        # æ­¥éª¤6: å…¨å±€æ®‹å·®è¿æ¥: y = x + intermediate\n",
    "        y = x + intermediate\n",
    "        \n",
    "        return y\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨: åˆ›å»ºä¸€ä¸ªResiDualTransformerBlockå®ä¾‹ï¼Œå¹¶è¿›è¡Œå‰å‘ä¼ æ’­æµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    # å®šä¹‰å‚æ•°\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å—\n",
    "    block = ResiDualTransformerBlock(d_model, n_heads, d_ff)\n",
    "    \n",
    "    # ç”Ÿæˆéšæœºè¾“å…¥\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # ç”Ÿæˆå› æœæ³¨æ„åŠ›æ©ç ï¼ˆDecoder-Onlyï¼‰\n",
    "    attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * float('-inf')\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    output = block(x, attn_mask=attn_mask)\n",
    "    \n",
    "    # æ‰“å°è¾“å‡ºå½¢çŠ¶ä»¥éªŒè¯\n",
    "    print(\"è¾“å‡ºå½¢çŠ¶:\", output.shape)  # åº”ä¸º (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeddf2d-badb-4bce-a5cc-a827c3eef09e",
   "metadata": {},
   "source": [
    "#### å¹¶è¡Œç»“æ„\n",
    "\n",
    "å°†MSAå­å±‚å’ŒFFNå­å±‚å¹¶è¡Œæ”¾ç½®ï¼Œè®©å®ƒä»¬åŒæ—¶å¤„ç†è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f4d5f0-5b6c-4c58-b274-2bb0318f6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå¸¦æœ‰å¹¶è¡Œç»“æ„çš„Transformerå±‚\n",
    "class ParallelTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ParallelTransformerBlockæ¨¡å—ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - d_model: æ¨¡å‹çš„åµŒå…¥ç»´åº¦ï¼ˆhidden sizeï¼‰ã€‚\n",
    "        - n_heads: è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚\n",
    "        - d_ff: å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰çš„ä¸­é—´ç»´åº¦ã€‚\n",
    "        - dropout: dropoutç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "        \"\"\"\n",
    "        super(ParallelTransformerBlock, self).__init__()\n",
    "        \n",
    "        # å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ï¼Œç”¨äºå¹¶è¡Œåˆ†æ”¯çš„è¾“å…¥å½’ä¸€åŒ–\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼ŒåŒ…å«ä¸¤ä¸ªçº¿æ€§å±‚å’Œæ¿€æ´»å‡½æ•°\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢\n",
    "            nn.GELU(),                 # GELUæ¿€æ´»å‡½æ•°\n",
    "            nn.Linear(d_ff, d_model),  # ç¬¬äºŒå±‚çº¿æ€§å˜æ¢\n",
    "            nn.Dropout(dropout)        # dropoutå±‚\n",
    "        )\n",
    "        \n",
    "        # dropoutå±‚ï¼Œç”¨äºå¹¶è¡Œè¾“å‡ºçš„æ­£åˆ™åŒ–\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­å‡½æ•°ï¼Œå®ç°å¹¶è¡Œè‡ªæ³¨æ„åŠ›å’ŒFFNç»“æ„ã€‚\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, d_model)ã€‚\n",
    "        - attn_mask: å¯é€‰çš„æ³¨æ„åŠ›æ©ç ï¼Œç”¨äºDecoder-Onlyçš„å› æœæ³¨æ„åŠ›ï¼ˆcausal maskï¼‰ã€‚\n",
    "        \n",
    "        æµç¨‹:\n",
    "        1. å¯¹è¾“å…¥xè¿›è¡Œå±‚å½’ä¸€åŒ–ï¼Œå¾—åˆ°x_lnã€‚\n",
    "        2. å¹¶è¡Œè®¡ç®—è‡ªæ³¨æ„åŠ›è¾“å‡ºï¼ˆattn_outï¼‰å’ŒFFNè¾“å‡ºï¼ˆffn_outï¼‰ã€‚\n",
    "        3. å°†æ³¨æ„åŠ›è¾“å‡ºå’ŒFFNè¾“å‡ºä¸å½’ä¸€åŒ–åçš„è¾“å…¥x_lnç›¸åŠ ã€‚\n",
    "        4. åº”ç”¨dropoutæ­£åˆ™åŒ–ã€‚\n",
    "        5. å…¨å±€æ®‹å·®è¿æ¥ï¼Œå°†ç»“æœä¸åŸå§‹è¾“å…¥xç›¸åŠ ã€‚\n",
    "        6. è¿”å›æœ€ç»ˆè¾“å‡ºã€‚\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1: è¾“å…¥å½’ä¸€åŒ–\n",
    "        x_ln = self.ln(x)\n",
    "        \n",
    "        # æ­¥éª¤2: å¹¶è¡Œè®¡ç®—è‡ªæ³¨æ„åŠ›è¾“å‡ºå’ŒFFNè¾“å‡º\n",
    "        # è‡ªæ³¨æ„åŠ›åˆ†æ”¯\n",
    "        attn_out, _ = self.self_attn(x_ln, x_ln, x_ln, attn_mask=attn_mask)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # FFNåˆ†æ”¯\n",
    "        ffn_out = self.ffn(x_ln)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "        \n",
    "        # æ­¥éª¤3: å¹¶è¡Œèåˆï¼Œç»“åˆè‡ªæ³¨æ„åŠ›è¾“å‡ºå’ŒFFNè¾“å‡º\n",
    "        parallel_out = x_ln + attn_out + ffn_out\n",
    "        \n",
    "        # æ­¥éª¤4: åº”ç”¨dropoutæ­£åˆ™åŒ–\n",
    "        parallel_out = self.dropout(parallel_out)\n",
    "        \n",
    "        # æ­¥éª¤5: å…¨å±€æ®‹å·®è¿æ¥\n",
    "        y = x + parallel_out\n",
    "        \n",
    "        return y\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨: åˆ›å»ºä¸€ä¸ªParallelTransformerBlockå®ä¾‹ï¼Œå¹¶è¿›è¡Œå‰å‘ä¼ æ’­æµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    # å®šä¹‰å‚æ•°\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    n_heads = 8\n",
    "    d_ff = 2048\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å—\n",
    "    block = ParallelTransformerBlock(d_model, n_heads, d_ff)\n",
    "    \n",
    "    # ç”Ÿæˆéšæœºè¾“å…¥\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # ç”Ÿæˆå› æœæ³¨æ„åŠ›æ©ç ï¼ˆé€‚ç”¨äºDecoder-Onlyï¼‰\n",
    "    attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * float('-inf')\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    output = block(x, attn_mask=attn_mask)\n",
    "    \n",
    "    # æ‰“å°è¾“å‡ºå½¢çŠ¶ä»¥éªŒè¯\n",
    "    print(\"è¾“å‡ºå½¢çŠ¶:\", output.shape)  # åº”ä¸º (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be488aea-c3fb-463f-b922-e141fef6a410",
   "metadata": {},
   "source": [
    "#### éšæœºæ·±åº¦\n",
    "\n",
    "å¯¹äºæ®‹å·®è¿æ¥ğ‘¦=ğ‘¥+ğ¹(ğ‘¥)ï¼Œåœ¨è®­ç»ƒæœŸé—´ä»¥ä¸€å®šæ¦‚ç‡éšæœºä¸¢å¼ƒæ®‹å·®åˆ†æ”¯ä¸­çš„ğ¹(ğ‘¥)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3cc01-635d-42ef-bca9-b3585f91052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    def __init__(self, drop_prob: float):\n",
    "        \"\"\"\n",
    "        drop_prob: ä¸¢å¼ƒæ¦‚ç‡ (0 <= drop_prob < 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        \"\"\"\n",
    "        x: æ®‹å·®ä¸»åˆ†æ”¯ (è¾“å…¥)\n",
    "        residual: å­å±‚è¾“å‡º f(x)\n",
    "        \"\"\"\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            # æ¨ç†æ—¶ï¼ˆevalæ¨¡å¼ï¼‰æˆ–ä¸ä¸¢å¼ƒ â†’ æ ‡å‡†æ®‹å·®\n",
    "            return x + residual\n",
    "\n",
    "        # è®­ç»ƒæ—¶ï¼šç”Ÿæˆ Bernoulli æ©ç \n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        mask = torch.empty(x.shape[0], 1, 1, device=x.device).bernoulli_(keep_prob)\n",
    "        # ä¿æŒæœŸæœ›ä¸å˜ï¼Œéœ€è¦ç¼©æ”¾ residual\n",
    "        residual = residual / keep_prob\n",
    "        return x + residual * mask\n",
    "\n",
    "\n",
    "# ===== ä½¿ç”¨ç¤ºä¾‹ï¼šTransformer Block =====\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_ff, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_ff, d_model)\n",
    "        )\n",
    "        # åœ¨æ®‹å·®è¿æ¥ä¸­åº”ç”¨éšæœºæ·±åº¦\n",
    "        self.sd1 = StochasticDepth(drop_prob)\n",
    "        self.sd2 = StochasticDepth(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attention + æ®‹å·®ï¼ˆå¸¦éšæœºæ·±åº¦ï¼‰\n",
    "        attn_out, _ = self.attn(self.ln1(x), self.ln1(x), self.ln1(x))\n",
    "        x = self.sd1(x, attn_out)\n",
    "\n",
    "        # FFN + æ®‹å·®ï¼ˆå¸¦éšæœºæ·±åº¦ï¼‰\n",
    "        ffn_out = self.ffn(self.ln2(x))\n",
    "        x = self.sd2(x, ffn_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ===== æµ‹è¯•è¿è¡Œ =====\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, seq_len, d_model = 2, 10, 512\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    block = TransformerBlock(d_model=512, nhead=4, dim_ff=64, drop_prob=0.2)\n",
    "    block.train()  # è®­ç»ƒæ¨¡å¼ï¼šéšæœºä¸¢å¼ƒ\n",
    "\n",
    "    out = block(x)\n",
    "    print(\"Output shape (train):\", out.shape)\n",
    "\n",
    "    block.eval()  # æ¨ç†æ¨¡å¼ï¼šä¸ä¸¢å¼ƒ\n",
    "    out_eval = block(x)\n",
    "    print(\"Output shape (eval):\", out_eval.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2494bb-150a-4405-8a03-39347bed5022",
   "metadata": {},
   "source": [
    "## Transformer MoEæ¨¡å‹ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc1060-45de-4ba1-94a9-0a6974d2cbcb",
   "metadata": {},
   "source": [
    "### ç®€å•MoEæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67331ad9-3943-4023-a039-3f674c68c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        QK = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            QK = QK.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_scores = F.softmax(QK, dim=-1)\n",
    "        output = torch.matmul(attention_scores, V)\n",
    "        return output, attention_scores\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        output, attention_scores = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        return output, attention_scores\n",
    "\n",
    "# æ··åˆä¸“å®¶ç³»ç»Ÿ (MOE)\n",
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_experts, top_k=2):\n",
    "        super(MoEFeedForward, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # é—¨æ§ç½‘ç»œ\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        \n",
    "        # ä¸“å®¶ç½‘ç»œ\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # é—¨æ§ç½‘ç»œè¾“å‡ºä¸“å®¶é€‰æ‹©æ¦‚ç‡\n",
    "        gate_scores = F.softmax(self.gate(x), dim=-1)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # é€‰æ‹© top-k ä¸“å®¶\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(self.top_k, dim=-1)  # [batch_size, seq_len, top_k]\n",
    "        \n",
    "        # åˆå§‹åŒ–è¾“å‡º\n",
    "        output = torch.zeros_like(x)\n",
    "        \n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = top_k_indices[:, :, i]  # [batch_size, seq_len]\n",
    "            expert_scores = top_k_scores[:, :, i].unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            \n",
    "            # ä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©å¯¹åº”çš„ä¸“å®¶è¾“å‡º\n",
    "            expert_output = torch.zeros_like(x)\n",
    "            for j in range(self.num_experts):\n",
    "                mask = (expert_idx == j).unsqueeze(-1).float()\n",
    "                expert_output += mask * self.experts[j](x)\n",
    "            \n",
    "            output += expert_output * expert_scores\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Transformer Decoder å±‚\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_experts, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.moe = MoEFeedForward(d_model, d_ff, num_experts)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-LayerNorm\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, attn_scores = self.self_attention(x_norm, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Pre-LayerNorm for MoE\n",
    "        x_norm = self.norm2(x)\n",
    "        moe_output = self.moe(x_norm)\n",
    "        x = x + self.dropout(moe_output)\n",
    "        \n",
    "        return x, attn_scores\n",
    "\n",
    "# å®Œæ•´çš„ Transformer Decoder-Only æ¨¡å‹\n",
    "class TransformerDecoderOnly(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_experts, max_seq_len, dropout=0.1):\n",
    "        super(TransformerDecoderOnly, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = self.create_positional_encoding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, num_experts, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def create_positional_encoding(self, max_seq_len, d_model):\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_seq_len, d_model]\n",
    "        return pe\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        return ~mask  # [seq_len, seq_len]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        mask = self.create_causal_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # åµŒå…¥å’Œä½ç½®ç¼–ç \n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # é€šè¿‡æ‰€æœ‰è§£ç å™¨å±‚\n",
    "        attention_scores = []\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "            attention_scores.append(attn)\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        output = self.fc_out(x)\n",
    "        return output, attention_scores\n",
    "\n",
    "# ç¤ºä¾‹ä½¿ç”¨\n",
    "def main():\n",
    "    # æ¨¡å‹å‚æ•°\n",
    "    vocab_size = 10000\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    num_layers = 6\n",
    "    num_experts = 4\n",
    "    max_seq_len = 100\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    model = TransformerDecoderOnly(\n",
    "        vocab_size, d_model, num_heads, d_ff, num_layers, num_experts, max_seq_len, dropout\n",
    "    )\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè¾“å…¥\n",
    "    batch_size = 32\n",
    "    seq_len = 50\n",
    "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    output, attention_scores = model(input_ids)\n",
    "    print(f\"Output shape: {output.shape}\")  # [batch_size, seq_len, vocab_size]\n",
    "    print(f\"Number of attention score tensors: {len(attention_scores)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
