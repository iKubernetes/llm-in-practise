{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d3711b-99c4-43b2-bb43-7f2793b35c3f",
   "metadata": {},
   "source": [
    "# ç¥ç»ç½‘ç»œå¿«é€Ÿå…¥é—¨ä¸å®è·µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da47d9-50a7-43d0-aaa8-438c1e21b0df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## åŸºäºNumPyçš„ç¥ç»ç½‘ç»œæ„å»ºä¸è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe49c8-8388-4f31-b922-21bc56e9774a",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹1ï¼šæœ€ç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆy=wx+bï¼‰\n",
    "- å•ä¸ªç¥ç»å…ƒæ„æˆçš„å•å±‚ç½‘ç»œï¼ˆä»…æœ‰ä¸€ä¸ªè¾“å‡ºå±‚ï¼‰\n",
    "- è¾“å…¥ç‰¹å¾ä¸º1\n",
    "- è¾“å…¥æ ·æœ¬æ•°é‡ä¸º1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be273d-0f09-45a9-86e8-4113d9f88d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# æ•°æ®\n",
    "x = 2.0   # è¾“å…¥ï¼Œ\n",
    "y = 4.0   # çœŸå®å€¼\n",
    "\n",
    "# å‚æ•°åˆå§‹åŒ–\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "lr = 0.1\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "y_hat = w * x + b\n",
    "loss = 0.5 * (y_hat - y) ** 2\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "dL_dyhat = y_hat - y\n",
    "dL_dw = dL_dyhat * x\n",
    "dL_db = dL_dyhat * 1\n",
    "\n",
    "# å‚æ•°æ›´æ–°\n",
    "w = w - lr * dL_dw\n",
    "b = b - lr * dL_db\n",
    "\n",
    "print(f\"Updated w: {w}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a6008-807c-4e89-a50a-57d8505c60fa",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹2ï¼šå•å±‚ç½‘ç»œï¼Œè¾“å…¥ä¸ºçŸ©é˜µå½¢å¼ï¼ˆy=Xw+bï¼‰\n",
    "- æ‰©å±•å‰é¢çš„ç¤ºä¾‹ï¼š\n",
    "  - è¾“å…¥ç‰¹å¾ä¸º2\n",
    "  - æ ·æœ¬æ•°é‡ä¸º2ï¼Œæ¯ä¸ªæ ·æœ¬2ä¸ªç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddb448-5711-406a-acd9-deda2fbc487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®\n",
    "X = np.array([[1.0, 2.0], [3.0, 4.0]])  # shape: (2, 2)ï¼Œè¾“å…¥ç‰¹å¾ä¸º2ï¼Œæ ·æœ¬æ•°é‡ä¸º2\n",
    "y = np.array([[5.0], [11.0]])          # shape: (2, 1)ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªçœŸå®å€¼\n",
    "\n",
    "# åˆå§‹åŒ–å‚æ•°\n",
    "# ä¸¤ä¸ªè¾“å…¥ç‰¹å¾çš„å„åˆå§‹æƒé‡ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªç¥ç»å…ƒï¼Œ\n",
    "# è¿™é‡Œçš„æƒé‡çŸ©é˜µè¡¨ç°ä¸ºåˆ—å‘é‡çš„å½¢å¼\n",
    "b = 0.0\n",
    "w = np.array([[0.1], [0.1]])           # shape: (2, 1)\n",
    "lr = 0.01\n",
    "N = X.shape[0]\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "y_hat = X @ w + b\n",
    "loss = 0.5 * np.mean((y_hat - y) ** 2)\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "grad_yhat = (y_hat - y) / N\n",
    "grad_w = X.T @ grad_yhat\n",
    "grad_b = np.sum(grad_yhat)\n",
    "\n",
    "# æ›´æ–°å‚æ•°\n",
    "w -= lr * grad_w\n",
    "b -= lr * grad_b\n",
    "\n",
    "print(\"Updated w:\", w.ravel())\n",
    "print(\"Updated b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e41ec-5718-4e94-8407-8a9d7fd068fb",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹3ï¼šä¸¤å±‚ç½‘ç»œç¤ºä¾‹ï¼ˆy=XW+bï¼‰\n",
    "- ä¸¤å±‚ç½‘ç»œï¼šä¸€ä¸ªéšè—å±‚ï¼Œä¸€ä¸ªè¾“å‡ºå±‚\n",
    "- æ ·æœ¬æ•°ä¸ºN=2ï¼Œè¾“å…¥ç‰¹å¾ç»´åº¦ä¸ºD=2\n",
    "- éšè—å±‚ç¥ç»å…ƒæ•°H=4ï¼Œä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°\n",
    "- æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰\n",
    "  - $L = \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2$\n",
    "\n",
    "**ç¥ç»ç½‘ç»œç»“æ„ï¼š**\n",
    "```mathematica\n",
    "è¾“å…¥ X âˆˆ â„^{NÃ—D}\n",
    "   â†“\n",
    "ç¬¬ä¸€å±‚ï¼ˆçº¿æ€§ï¼‰: Zâ‚ = X @ Wâ‚ + bâ‚      # shape: (N, H)\n",
    "   â†“\n",
    "ReLU æ¿€æ´»å‡½æ•°: Aâ‚ = ReLU(Zâ‚)           # shape: (N, H)\n",
    "   â†“\n",
    "ç¬¬äºŒå±‚ï¼ˆçº¿æ€§ï¼‰: yÌ‚ = Aâ‚ @ Wâ‚‚ + bâ‚‚      # shape: (N, 1)\n",
    "   â†“\n",
    "MSE æŸå¤±å‡½æ•°: L = MSE(yÌ‚, y)\n",
    "```\n",
    "\n",
    "å…¶ä¸­çš„ç¬¦å·è¡¨ç¤ºï¼š\n",
    "- ğ·ï¼šè¾“å…¥ç‰¹å¾æ•°\n",
    "- ğ»ï¼šéšè—å±‚ç»´åº¦ï¼ˆç¥ç»å…ƒæ•°ï¼‰\n",
    "- ğ‘ï¼šæ ·æœ¬æ•°\n",
    "- ReLUæ¿€æ´»ï¼šReLU(ğ‘§)=max(0,ğ‘§)\n",
    "\n",
    "**å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­æµç¨‹å›¾**\n",
    "```scss\n",
    "å‰å‘ä¼ æ’­ï¼š\n",
    "X â†’ Linear(W1, b1) â†’ Z1 â†’ ReLU â†’ A1 â†’ Linear(W2, b2) â†’ y_hat â†’ MSE â†’ Loss\n",
    "\n",
    "åå‘ä¼ æ’­ï¼ˆé“¾å¼ï¼‰ï¼š\n",
    "Loss â†’ d(y_hat) â†’ dW2, db2 â†’ dA1 â†’ dZ1 (ReLU) â†’ dW1, db1\n",
    "```\n",
    "\n",
    "#### ä»£ç è§£æ\n",
    "##### 1. å‰å‘ä¼ æ’­\n",
    "é¦–å…ˆæ˜ç¡®å‰å‘ä¼ æ’­çš„è®¡ç®—è¿‡ç¨‹ï¼Œä»¥ä¾¿åç»­æ¨å¯¼æ¢¯åº¦ã€‚ä»£ç ä¸­çš„å‰å‘ä¼ æ’­æ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "ï¼ˆ1ï¼‰éšè—å±‚è®¡ç®—ï¼š$Z_1 = X W_1 + b_1$\n",
    "- $ X \\in \\mathbb{R}^{N \\times D} $ï¼Œ$ W_1 \\in \\mathbb{R}^{D \\times H} $ï¼Œ$ b_1 \\in \\mathbb{R}^{1 \\times H} $ã€‚\n",
    "- $ Z_1 \\in \\mathbb{R}^{N \\times H} $ æ˜¯éšè—å±‚çš„çº¿æ€§è¾“å‡ºã€‚\n",
    "- ä»£ç å®ç°\n",
    "  ```python\n",
    "  Z1 = X @ W1 + b1  # (N x H)\n",
    "  ```\n",
    "ï¼ˆ2ï¼‰ReLUæ¿€æ´»ï¼š$A_1 = \\text{ReLU}(Z_1) = \\max(0, Z_1)$\n",
    "- ReLU é€å…ƒç´ æ“ä½œï¼šå¦‚æœ  $ Z_1_{ij} > 0 $ï¼Œåˆ™ A_1_{ij} = Z_1_{ij} ï¼Œå¦åˆ™ A_1_{ij} = 0 ã€‚\n",
    "- $ A_1 \\in \\mathbb{R}^{N \\times H} $ã€‚\n",
    "- ä»£ç å®ç°\n",
    "  ```python\n",
    "  A1 = np.maximum(0, Z1)  # (N x H)\n",
    "  ```\n",
    "\n",
    "ï¼ˆ3ï¼‰è¾“å‡ºå±‚è®¡ç®—ï¼š$\\hat{y} = A_1 W_2 + b_2$\n",
    "- $ A_1 \\in \\mathbb{R}^{N \\times H} $ï¼Œ$ W_2 \\in \\mathbb{R}^{H \\times 1} $ï¼Œ$ b_2 \\in \\mathbb{R}^{1 \\times 1} $ã€‚\n",
    "- $ \\hat{y} \\in \\mathbb{R}^{N \\times 1} $ã€‚\n",
    "- ä»£ç å®ç°\n",
    "  ```python\n",
    "  y_hat = A1 @ W2 + b2  # (N x 1)\n",
    "  ```\n",
    "\n",
    "ï¼ˆ4ï¼‰æŸå¤±å‡½æ•°ï¼š$L = \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2$\n",
    "- ä»£ç è®¡ç®—å‡æ–¹è¯¯å·®çš„å¹³å‡å€¼\n",
    "  ```python\n",
    "  loss = 0.5 * np.mean((y_hat - y) ** 2)\n",
    "  ```\n",
    "\n",
    "##### 2. åå‘ä¼ æ’­\n",
    "åå‘ä¼ æ’­çš„ç›®æ ‡æ˜¯è®¡ç®—æŸå¤± $ L $ å¯¹å‚æ•° $ W_1, b_1, W_2, b_2 $ çš„æ¢¯åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚é€æ­¥æ¨å¯¼ã€‚\n",
    "\n",
    "ï¼ˆ1ï¼‰**å¯¹æŸå¤±å‡½æ•°çš„å¯¼æ•°**\n",
    "\n",
    "å¯¹ $ \\hat{y} $ æ±‚å¯¼ï¼š$\\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{1}{N} (\\hat{y}_i - y_i)$\n",
    "\n",
    "çŸ©é˜µå½¢å¼ï¼š$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{1}{N} (\\hat{y} - y) \\in \\mathbb{R}^{N \\times 1}$\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "dy_hat = (y_hat - y) / N  # (N x 1)\n",
    "```\n",
    "\n",
    "ï¼ˆ2ï¼‰**è¾“å‡ºå±‚å‚æ•°æ¢¯åº¦ ($ W_2, b_2 $)**\n",
    "\n",
    "è¾“å‡ºå±‚è®¡ç®—ä¸ºï¼š$\\hat{y} = A_1 W_2 + b_2$\n",
    "\n",
    "æ¢¯åº¦ $ \\frac{\\partial L}{\\partial W_2} $\n",
    "- æ ¹æ®é“¾å¼æ³•åˆ™ï¼š$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial W_2}$\n",
    "- å…¶ä¸­ï¼š$\\frac{\\partial \\hat{y}}{\\partial W_2} = A_1$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial W_2} = A_1^T \\cdot \\frac{\\partial L}{\\partial \\hat{y}}$\n",
    "- $ A_1^T \\in \\mathbb{R}^{H \\times N} $ï¼Œ$ \\frac{\\partial L}{\\partial \\hat{y}} \\in \\mathbb{R}^{N \\times 1} $ã€‚\n",
    "- ç»“æœï¼š$ \\frac{\\partial L}{\\partial W_2} \\in \\mathbb{R}^{H \\times 1} $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "dW2 = A1.T @ dy_hat  # (H x 1)\n",
    "```\n",
    "\n",
    "æ¢¯åº¦$ \\frac{\\partial L}{\\partial b_2} $ï¼š\n",
    "- æ ¹æ®é“¾å¼æ³•åˆ™ï¼š$\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial b_2}$\n",
    "- å…¶ä¸­ï¼š$\\frac{\\partial \\hat{y}}{\\partial b_2} = 1$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial b_2} = \\text{sum}(\\frac{\\partial L}{\\partial \\hat{y}}, \\text{axis}=0)$\n",
    "- å¯¹ $ \\frac{\\partial L}{\\partial \\hat{y}} \\in \\mathbb{R}^{N \\times 1} $ æŒ‰è¡Œæ±‚å’Œï¼Œä¿æŒç»´åº¦ä¸º $ (1 \\times 1) $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "db2 = np.sum(dy_hat, axis=0, keepdims=True)  # (1 x 1)\n",
    "```\n",
    "\n",
    "ï¼ˆ3ï¼‰**ä¼ æ’­åˆ°éšè—å±‚**\n",
    "\n",
    "éœ€è¦è®¡ç®— $ \\frac{\\partial L}{\\partial A_1} $ å’Œ $ \\frac{\\partial L}{\\partial Z_1} $ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¨å¯¼ $ W_1, b_1 $ çš„æ¢¯åº¦ã€‚\n",
    "\n",
    "æ¢¯åº¦ $ \\frac{\\partial L}{\\partial A_1} $ï¼š\n",
    "- æ ¹æ®é“¾å¼æ³•åˆ™ï¼š$\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial A_1}$\n",
    "- å…¶ä¸­ï¼š$\\hat{y} = A_1 W_2 + b_2$ï¼Œ $\\frac{\\partial \\hat{y}}{\\partial A_1} = W_2$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial A_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot W_2^T$\n",
    "- $ \\frac{\\partial L}{\\partial \\hat{y}} \\in \\mathbb{R}^{N \\times 1} $ï¼Œ$ W_2^T \\in \\mathbb{R}^{1 \\times H} $ã€‚\n",
    "- ç»“æœï¼š$ \\frac{\\partial L}{\\partial A_1} \\in \\mathbb{R}^{N \\times H} $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "dA1 = dy_hat @ W2.T  # (N x H)\n",
    "```\n",
    "\n",
    "æ¢¯åº¦ $ \\frac{\\partial L}{\\partial Z_1} $ï¼š\n",
    "- éšè—å±‚ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼Œ$A_1 = \\text{ReLU}(Z_1) = \\max(0, Z_1)$\n",
    "- ReLUçš„å¯¼æ•°ä¸ºï¼š$\\frac{\\partial A_1}{\\partial Z_1} = \\begin{cases} \n",
    "1, & \\text{if } Z_1 > 0 \\\\\n",
    "0, & \\text{if } Z_1 \\leq 0 \n",
    "\\end{cases}$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial Z_1} = \\frac{\\partial L}{\\partial A_1} \\cdot \\frac{\\partial A_1}{\\partial Z_1}$\n",
    "- $ \\frac{\\partial A_1}{\\partial Z_1} $ æ˜¯ä¸€ä¸ªé€å…ƒç´ æ“ä½œï¼Œå€¼ä¸º 1ï¼ˆå½“ $ Z_1 > 0 $ï¼‰æˆ– 0ï¼ˆå½“ $ Z_1 \\leq 0 $ï¼‰ã€‚\n",
    "- ä»£ç ä¸­ç”¨ (Z1 > 0).astype(float) å®ç°ReLUçš„å¯¼æ•°ã€‚\n",
    "- ç»“æœï¼š$ \\frac{\\partial L}{\\partial Z_1} \\in \\mathbb{R}^{N \\times H} $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "dZ1 = dA1 * (Z1 > 0).astype(float)  # (N x H)\n",
    "```\n",
    "\n",
    "ï¼ˆ4ï¼‰**éšè—å±‚å‚æ•°æ¢¯åº¦ ($ W_1, b_1 $)**\n",
    "\n",
    "éšè—å±‚è®¡ç®—ä¸ºï¼š$Z_1 = X W_1 + b_1$\n",
    "\n",
    "æ¢¯åº¦ $ \\frac{\\partial L}{\\partial W_1} $\n",
    "- æ ¹æ®é“¾å¼æ³•åˆ™ï¼š$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial W_1}$\n",
    "- å…¶ä¸­ï¼š$\\frac{\\partial Z_1}{\\partial W_1} = X$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial W_1} = X^T \\cdot \\frac{\\partial L}{\\partial Z_1}$\n",
    "- $ X^T \\in \\mathbb{R}^{D \\times N} $ï¼Œ$ \\frac{\\partial L}{\\partial Z_1} \\in \\mathbb{R}^{N \\times H} $ã€‚\n",
    "- ç»“æœï¼š$ \\frac{\\partial L}{\\partial W_1} \\in \\mathbb{R}^{D \\times H} $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "dW1 = X.T @ dZ1  # (D x H)\n",
    "```\n",
    "\n",
    "æ¢¯åº¦ $ \\frac{\\partial L}{\\partial b_1} $ï¼š\n",
    "- æ ¹æ®é“¾å¼æ³•åˆ™ï¼š$\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial Z_1} \\cdot \\frac{\\partial Z_1}{\\partial b_1}$\n",
    "- å…¶ä¸­ï¼š$\\frac{\\partial Z_1}{\\partial b_1} = 1$\n",
    "\n",
    "å› æ­¤ï¼š$\\frac{\\partial L}{\\partial b_1} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial Z_{1,i}}$\n",
    "- å¯¹ $ \\frac{\\partial L}{\\partial Z_1} \\in \\mathbb{R}^{N \\times H} $ æŒ‰è¡Œæ±‚å’Œï¼Œä¿æŒç»´åº¦ä¸º $ (1 \\times H) $ã€‚\n",
    "\n",
    "ä»£ç å®ç°ï¼š\n",
    "```python\n",
    "db1 = np.sum(dZ1, axis=0, keepdims=True)  # (1 x H)\n",
    "```\n",
    "\n",
    "##### 3. å‚æ•°æ›´æ–°\n",
    "ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ï¼š$\\theta \\gets \\theta - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}$\n",
    "\n",
    "å…¶ä¸­ $ \\eta = 0.01 $ æ˜¯å­¦ä¹ ç‡ï¼Œ$ \\theta $ ä»£è¡¨ $ W_1, b_1, W_2, b_2 $ã€‚\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab0a69-14b5-4e4a-8076-fe91ded7b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- åˆå§‹åŒ– ----------\n",
    "np.random.seed(42)\n",
    "\n",
    "# è¾“å…¥æ•°æ® X å’Œç›®æ ‡ y\n",
    "X = np.array([[1.0, 2.0], [3.0, 4.0]])  # shape: (2, 2)\n",
    "y = np.array([[5.0], [11.0]])          # shape: (2, 1)\n",
    "N, D = X.shape\n",
    "H = 4  # éšè—å±‚å•å…ƒæ•°\n",
    "lr = 0.01\n",
    "\n",
    "# æƒé‡åˆå§‹åŒ–\n",
    "W1 = np.random.randn(D, H) * 0.01     # ç¬¬ä¸€å±‚æƒé‡ (D, H)\n",
    "b1 = np.zeros((1, H))                 # ç¬¬ä¸€å±‚åç½® (1, H)\n",
    "W2 = np.random.randn(H, 1) * 0.01     # ç¬¬äºŒå±‚æƒé‡ (H, 1)\n",
    "b2 = np.zeros((1, 1)) \n",
    "\n",
    "# ---------- å‰å‘ä¼ æ’­ ----------\n",
    "Z1 = X @ W1 + b1            # (N x H)\n",
    "A1 = np.maximum(0, Z1)      # ReLU æ¿€æ´» â†’ (N x H)\n",
    "y_hat = A1 @ W2 + b2        # è¾“å‡º â†’ (N x 1)\n",
    "loss = 0.5 * np.mean((y_hat - y) ** 2)\n",
    "\n",
    "# ---------- åå‘ä¼ æ’­ ----------\n",
    "dy_hat = (y_hat - y) / N                # dL/dy_hat â†’ (N x 1)\n",
    "\n",
    "# è¾“å‡ºå±‚å‚æ•°æ¢¯åº¦\n",
    "dW2 = A1.T @ dy_hat                     # (H x 1)\n",
    "db2 = np.sum(dy_hat, axis=0, keepdims=True)  # (1 x 1)\n",
    "\n",
    "# ä¼ æ’­åˆ°éšè—å±‚\n",
    "dA1 = dy_hat @ W2.T                     # (N x H)\n",
    "dZ1 = dA1 * (Z1 > 0).astype(float)      # ReLU å¯¼æ•° â†’ (N x H)\n",
    "\n",
    "# éšè—å±‚å‚æ•°æ¢¯åº¦\n",
    "dW1 = X.T @ dZ1                         # (D x H)\n",
    "db1 = np.sum(dZ1, axis=0, keepdims=True)  # (1 x H)\n",
    "\n",
    "# ---------- å‚æ•°æ›´æ–° ----------\n",
    "W1 -= lr * dW1\n",
    "b1 -= lr * db1\n",
    "W2 -= lr * dW2\n",
    "b2 -= lr * db2\n",
    "\n",
    "# ---------- è¾“å‡º ----------\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Updated W1:\\n\", W1)\n",
    "print(\"Updated b1:\\n\", b1)\n",
    "print(\"Updated W2:\\n\", W2)\n",
    "print(\"Updated b2:\\n\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae34488-21dd-46ed-aded-00f916744cea",
   "metadata": {},
   "source": [
    "### æ‰©å±•ä»¥ä¸Šç½‘ç»œï¼Œä½¿ç”¨ä¸¤ä¸ªéšè—å±‚çš„ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3ce42-1d14-4b59-b9e3-438b9fcd12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "X = np.array([[1.0, 2.0], [3.0, 4.0]])  # shape: (2, 2)\n",
    "y = np.array([[5.0], [11.0]])          # shape: (2, 1)\n",
    "N, D = X.shape\n",
    "\n",
    "# ç½‘ç»œç»“æ„å‚æ•°\n",
    "H1 = 4  # ç¬¬ 1 ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\n",
    "H2 = 4  # ç¬¬ 2 ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\n",
    "lr = 0.01\n",
    "\n",
    "# ---------- åˆå§‹åŒ–å‚æ•° ----------\n",
    "W1 = np.random.randn(D, H1) * np.sqrt(2. / D)\n",
    "b1 = np.zeros((1, H1))\n",
    "\n",
    "W2 = np.random.randn(H1, H2) * np.sqrt(2. / H1)\n",
    "b2 = np.zeros((1, H2))\n",
    "\n",
    "W3 = np.random.randn(H2, 1) * np.sqrt(2. / H2)\n",
    "b3 = np.zeros((1, 1))\n",
    "\n",
    "# ---------- å‰å‘ä¼ æ’­ ----------\n",
    "Z1 = X @ W1 + b1         # (N, H1)\n",
    "A1 = np.maximum(0, Z1)   # ReLU â†’ (N, H1)\n",
    "\n",
    "Z2 = A1 @ W2 + b2        # (N, H2)\n",
    "A2 = np.maximum(0, Z2)   # ReLU â†’ (N, H2)\n",
    "\n",
    "y_hat = A2 @ W3 + b3     # (N, 1)\n",
    "\n",
    "loss = 0.5 * np.mean((y_hat - y) ** 2)\n",
    "\n",
    "# ---------- åå‘ä¼ æ’­ ----------\n",
    "dy_hat = (y_hat - y) / N   # (N, 1)\n",
    "\n",
    "# è¾“å‡ºå±‚æ¢¯åº¦\n",
    "dW3 = A2.T @ dy_hat        # (H2, 1)\n",
    "db3 = np.sum(dy_hat, axis=0, keepdims=True)  # (1, 1)\n",
    "\n",
    "# ç¬¬äºŒéšè—å±‚æ¢¯åº¦\n",
    "dA2 = dy_hat @ W3.T        # (N, H2)\n",
    "dZ2 = dA2 * (Z2 > 0)       # ReLU'(Z2)\n",
    "dW2 = A1.T @ dZ2           # (H1, H2)\n",
    "db2 = np.sum(dZ2, axis=0, keepdims=True)  # (1, H2)\n",
    "\n",
    "# ç¬¬ä¸€éšè—å±‚æ¢¯åº¦\n",
    "dA1 = dZ2 @ W2.T           # (N, H1)\n",
    "dZ1 = dA1 * (Z1 > 0)       # ReLU'(Z1)\n",
    "dW1 = X.T @ dZ1            # (D, H1)\n",
    "db1 = np.sum(dZ1, axis=0, keepdims=True)  # (1, H1)\n",
    "\n",
    "# ---------- å‚æ•°æ›´æ–° ----------\n",
    "W3 -= lr * dW3\n",
    "b3 -= lr * db3\n",
    "W2 -= lr * dW2\n",
    "b2 -= lr * db2\n",
    "W1 -= lr * dW1\n",
    "b1 -= lr * db1\n",
    "\n",
    "# ---------- è¾“å‡º ----------\n",
    "print(\"Loss:\", loss)\n",
    "print(\"\\nUpdated W1:\\n\", W1)\n",
    "print(\"Updated b1:\\n\", b1)\n",
    "print(\"\\nUpdated W2:\\n\", W2)\n",
    "print(\"Updated b2:\\n\", b2)\n",
    "print(\"\\nUpdated W3:\\n\", W3)\n",
    "print(\"Updated b3:\\n\", b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2dd04b-dd9d-49c5-a9bb-f1adc3f901ec",
   "metadata": {},
   "source": [
    "### ä¸ºä»¥ä¸Šä»£ç æ·»åŠ å¤šè½®è®­ç»ƒå¾ªç¯æœºåˆ¶\n",
    "- è¾“å…¥ç»´åº¦æ‰©å±•åˆ°äº†20ï¼Œéšè—å±‚ç»´åº¦æ‰©å±•ä¸º50\n",
    "- è®­ç»ƒè½®æ•°é€šè¿‡è¶…å‚æ•°epochsè¿›è¡Œæ§åˆ¶\n",
    "\n",
    "**æç¤º**ï¼šå¯é€šè¿‡æ‰‹åŠ¨è°ƒæ•´å¦‚ä¸‹ä»£ç ä¸­çš„epochå€¼ï¼ˆä¾‹å¦‚0.01 -> 0.02 -> 0.05 -> 0.1ï¼‰ï¼Œæ¥éªŒè¯ä¸åŒå­¦ä¹ ç‡ä¸‹æ¨¡å‹è®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f81c8-e0cb-4db1-8dad-24d54217387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "N, D_in, H1, H2, D_out = 256, 20, 50, 50, 1  # æ ·æœ¬æ•°é‡256ï¼Œè¾“å…¥ç»´åº¦20ï¼Œéšè—å±‚åŒä¸º50ï¼Œè¾“å‡ºç»´åº¦ä¸º1\n",
    "\n",
    "# è¾“å…¥ X å’ŒçœŸå®æ ‡ç­¾ yï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º (N, D_in) å’Œ (N, 1)\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# ---------- æƒé‡åˆå§‹åŒ– ----------\n",
    "W1 = np.random.randn(D_in, H1) * 0.01\n",
    "b1 = np.zeros((1, H1))\n",
    "\n",
    "W2 = np.random.randn(H1, H2) * 0.01\n",
    "b2 = np.zeros((1, H2))\n",
    "\n",
    "W3 = np.random.randn(H2, D_out) * 0.01\n",
    "b3 = np.zeros((1, D_out))\n",
    "\n",
    "# ---------- è¶…å‚æ•° ----------\n",
    "learning_rate = 0.05\n",
    "epochs = 500\n",
    "\n",
    "# ---------- æ¿€æ´»å‡½æ•° ----------\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# ---------- è®­ç»ƒå¾ªç¯ ----------\n",
    "for epoch in range(epochs):\n",
    "    # ---- å‰å‘ä¼ æ’­ ----\n",
    "    z1 = X @ W1 + b1      # (N, H1)\n",
    "    a1 = relu(z1)         # (N, H1)\n",
    "\n",
    "    z2 = a1 @ W2 + b2     # (N, H2)\n",
    "    a2 = relu(z2)         # (N, H2)\n",
    "\n",
    "    z3 = a2 @ W3 + b3     # (N, D_out)\n",
    "    y_pred = z3           # è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§è¾“å‡ºï¼‰\n",
    "\n",
    "    # ---- è®¡ç®—æŸå¤± ----\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss:.6f}\")\n",
    "\n",
    "    # ---- åå‘ä¼ æ’­ ----\n",
    "    dloss = 2 * (y_pred - y) / N   # MSE æŸå¤±çš„å¯¼æ•°\n",
    "\n",
    "    # ç¬¬ä¸‰å±‚æ¢¯åº¦\n",
    "    dW3 = a2.T @ dloss             # (H2, D_out)\n",
    "    db3 = np.sum(dloss, axis=0, keepdims=True)  # (1, D_out)\n",
    "\n",
    "    # ç¬¬äºŒå±‚åä¼ \n",
    "    da2 = dloss @ W3.T            # (N, H2)\n",
    "    dz2 = da2 * relu_grad(z2)     # (N, H2)\n",
    "\n",
    "    dW2 = a1.T @ dz2              # (H1, H2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    # ç¬¬ä¸€å±‚åä¼ \n",
    "    da1 = dz2 @ W2.T              # (N, H1)\n",
    "    dz1 = da1 * relu_grad(z1)     # (N, H1)\n",
    "\n",
    "    dW1 = X.T @ dz1               # (D_in, H1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # ---- å‚æ•°æ›´æ–° ----\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "# ---------- æœ€ç»ˆæŸå¤± ----------\n",
    "print(f\"Final Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37336df0-2c83-463c-b257-e1eb7e4c39a7",
   "metadata": {},
   "source": [
    "### Mini-Batchæœºåˆ¶ï¼šå°†æ ·æœ¬æŒ‰æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "**åŸå› **\n",
    "\n",
    "å°†æ ·æœ¬åˆ†æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒï¼ˆmini-batch è®­ç»ƒï¼‰çš„ä¸»è¦åŸå› åŒ…æ‹¬ï¼š\n",
    "- è®¡ç®—æ•ˆç‡ï¼šé™ä½å†…å­˜éœ€æ±‚ï¼Œé€‚åˆå¤§å‹æ•°æ®é›†ï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶å¹¶è¡Œæ€§ã€‚\n",
    "- æ¢¯åº¦ç¨³å®šæ€§ï¼šæä¾›æ¯”å•ä¸€æ ·æœ¬æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡ï¼ŒåŒæ—¶æ¯”å…¨æ‰¹é‡æ›´é«˜æ•ˆã€‚\n",
    "- æ³›åŒ–èƒ½åŠ›ï¼šé€šè¿‡éšæœºæ‰“ä¹±å’Œæ‰¹æ¬¡å¤„ç†ï¼Œå¢å¼ºæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚\n",
    "- çµæ´»æ€§ï¼šå¯è°ƒæ•´ batch å¤§å°ï¼Œé€‚åº”ä¸åŒè§„æ¨¡æ•°æ®é›†å’Œç¡¬ä»¶æ¡ä»¶ã€‚\n",
    "- æ›´å¿«æ”¶æ•›ï¼šæ›´é¢‘ç¹çš„å‚æ•°æ›´æ–°æœ‰åŠ©äºå¿«é€Ÿæ¢ç´¢å‚æ•°ç©ºé—´ã€‚\n",
    "\n",
    "ä¸‹é¢çš„ç¤ºä¾‹ä»£ç ä¸­ï¼Œmini-batchè®­ç»ƒé€šè¿‡å°†$ N = 256 $ä¸ªæ ·æœ¬åˆ†æˆ8ä¸ªbatchï¼ˆæ¯ä¸ª32ä¸ªæ ·æœ¬ï¼‰ï¼Œå®ç°äº†é«˜æ•ˆä¸”ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b409deb-7cda-4e40-8709-845d03f3dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "N, D_in, H1, H2, D_out = 256, 20, 50, 50, 1  # æ ·æœ¬æ•°é‡256ï¼Œè¾“å…¥ç»´åº¦20ï¼Œéšè—å±‚åŒä¸º50ï¼Œè¾“å‡ºç»´åº¦ä¸º1\n",
    "batch_size = 32  # æ¯ä¸ªmini-batchçš„å¤§å°\n",
    "\n",
    "# è¾“å…¥ X å’ŒçœŸå®æ ‡ç­¾ yï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º (N, D_in) å’Œ (N, 1)\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# ---------- æƒé‡åˆå§‹åŒ– ----------\n",
    "W1 = np.random.randn(D_in, H1) * 0.01\n",
    "b1 = np.zeros((1, H1))\n",
    "W2 = np.random.randn(H1, H2) * 0.01\n",
    "b2 = np.zeros((1, H2))\n",
    "W3 = np.random.randn(H2, D_out) * 0.01\n",
    "b3 = np.zeros((1, D_out))\n",
    "\n",
    "# ---------- è¶…å‚æ•° ----------\n",
    "learning_rate = 0.1\n",
    "epochs = 500\n",
    "num_batches = N // batch_size  # è®¡ç®— batch æ•°é‡\n",
    "\n",
    "# ---------- æ¿€æ´»å‡½æ•° ----------\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# ---------- è®­ç»ƒå¾ªç¯ ----------\n",
    "for epoch in range(epochs):\n",
    "    # æ‰“ä¹±æ•°æ®ï¼ˆæ¯ä¸ªepochéšæœºåŒ–æ ·æœ¬é¡ºåºï¼‰\n",
    "    indices = np.random.permutation(N)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # åˆå§‹åŒ– epoch æ€»æŸå¤±\n",
    "    epoch_loss = 0.01\n",
    "    \n",
    "    # æŒ‰mini-batchè¿­ä»£\n",
    "    for i in range(num_batches):\n",
    "        # è·å–å½“å‰ batch çš„æ•°æ®\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_shuffled[start_idx:end_idx]  # (batch_size, D_in)\n",
    "        y_batch = y_shuffled[start_idx:end_idx]  # (batch_size, D_out)\n",
    "        \n",
    "        # ---- å‰å‘ä¼ æ’­ ----\n",
    "        z1 = X_batch @ W1 + b1      # (batch_size, H1)\n",
    "        a1 = relu(z1)               # (batch_size, H1)\n",
    "        z2 = a1 @ W2 + b2           # (batch_size, H2)\n",
    "        a2 = relu(z2)               # (batch_size, H2)\n",
    "        z3 = a2 @ W3 + b3           # (batch_size, D_out)\n",
    "        y_pred = z3                 # è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§è¾“å‡ºï¼‰\n",
    "        \n",
    "        # ---- è®¡ç®—æŸå¤± ----\n",
    "        batch_loss = np.mean((y_pred - y_batch) ** 2)\n",
    "        epoch_loss += batch_loss\n",
    "        \n",
    "        # ---- åå‘ä¼ æ’­ ----\n",
    "        dloss = 2 * (y_pred - y_batch) / batch_size  # MSE æŸå¤±çš„å¯¼æ•°\n",
    "        \n",
    "        # ç¬¬ä¸‰å±‚æ¢¯åº¦\n",
    "        dW3 = a2.T @ dloss          # (H2, D_out)\n",
    "        db3 = np.sum(dloss, axis=0, keepdims=True)  # (1, D_out)\n",
    "        \n",
    "        # ç¬¬äºŒå±‚åä¼ \n",
    "        da2 = dloss @ W3.T          # (batch_size, H2)\n",
    "        dz2 = da2 * relu_grad(z2)   # (batch_size, H2)\n",
    "        \n",
    "        dW2 = a1.T @ dz2            # (H1, H2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # ç¬¬ä¸€å±‚åä¼ \n",
    "        da1 = dz2 @ W2.T            # (batch_size, H1)\n",
    "        dz1 = da1 * relu_grad(z1)   # (batch_size, H1)\n",
    "        \n",
    "        dW1 = X_batch.T @ dz1       # (D_in, H1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # ---- å‚æ•°æ›´æ–° ----\n",
    "        W3 -= learning_rate * dW3\n",
    "        b3 -= learning_rate * db3\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡ epoch æŸå¤±\n",
    "    epoch_loss /= num_batches\n",
    "    \n",
    "    # æ¯ 50 ä¸ª epoch è¾“å‡ºæŸå¤±\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# ---------- æœ€ç»ˆæŸå¤±ï¼ˆå…¨æ•°æ®é›†ï¼‰ ----------\n",
    "z1 = X @ W1 + b1\n",
    "a1 = relu(z1)\n",
    "z2 = a1 @ W2 + b2\n",
    "a2 = relu(z2)\n",
    "z3 = a2 @ W3 + b3\n",
    "y_pred = z3\n",
    "final_loss = np.mean((y_pred - y) ** 2)\n",
    "print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a4c11-030a-4a06-9a4e-d2865f71fd30",
   "metadata": {},
   "source": [
    "### æ­£åˆ™åŒ–æœºåˆ¶ï¼šé™ä½è¿‡æ‹Ÿåˆé£é™©\n",
    "æ­£åˆ™åŒ–æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒæœºåˆ¶ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¯¹æ¨¡å‹æ–½åŠ çº¦æŸï¼Œé™åˆ¶å…¶å¤æ‚åº¦ï¼Œä»è€Œæå‡åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "**æ­£åˆ™åŒ–çš„ä¸»è¦ä½œç”¨**ï¼š\n",
    "\n",
    "1. æŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼šå‡å°‘æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®å™ªå£°çš„æ•æ„Ÿåº¦\n",
    "2. æå‡æ³›åŒ–æ€§ï¼šå¢å¼ºæ¨¡å‹å¯¹æ–°æ•°æ®çš„é€‚åº”èƒ½åŠ›\n",
    "3. ç‰¹å¾é€‰æ‹©ä¸ç¨€ç–åŒ–ï¼ˆL1ï¼‰ï¼šå°†ä¸é‡è¦ç‰¹å¾çš„æƒé‡å‹ç¼©è‡³0ï¼Œå®ç°è‡ªåŠ¨ç‰¹å¾ç­›é€‰\n",
    "4. å¹³æ»‘æƒé‡åˆ†å¸ƒï¼ˆL2ï¼‰ï¼šä½¿æ‰€æœ‰æƒé‡è¶‹è¿‘äºè¾ƒå°å€¼ï¼Œé¿å…æç«¯æƒé‡\n",
    "5. åŠ é€Ÿè®­ç»ƒï¼šç®€åŒ–æ¨¡å‹ç»“æ„ï¼Œä¼˜åŒ–æ”¶æ•›æ•ˆç‡\n",
    "\n",
    "**å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹æ³•**ï¼š\n",
    "| æ­£åˆ™åŒ–æ–¹æ³•             | æè¿°                                                                 | å…¬å¼è¡¨ç¤º                              | ç‰¹ç‚¹                                                                 |\n",
    "|------------------------|----------------------------------------------------------------------|---------------------------------------|----------------------------------------------------------------------|\n",
    "| **L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰**  | åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æƒé‡çš„L1èŒƒæ•°æƒ©ç½šé¡¹ï¼Œé¼“åŠ±æ¨¡å‹äº§ç”Ÿç¨€ç–æƒé‡              | $J(\\theta) = \\text{Loss} + \\lambda \\sum_{i} \\vert\\theta_i\\vert$ | å®ç°ç‰¹å¾é€‰æ‹©ï¼Œäº§ç”Ÿç¨€ç–æ¨¡å‹ï¼Œä½†å¯èƒ½å¯¼è‡´è§£çš„ä¸ç¨³å®šæ€§                |\n",
    "| **L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰**  | åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥æƒé‡çš„L2èŒƒæ•°æƒ©ç½šé¡¹ï¼Œé˜²æ­¢æƒé‡è¿‡å¤§                      | $J(\\theta) = \\text{Loss} + \\lambda \\sum_{i} \\theta_i^2$         | é˜²æ­¢æƒé‡è¿‡å¤§ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§ï¼Œé€‚ç”¨äºå¤šé‡å…±çº¿æ€§é—®é¢˜      |\n",
    "| **Dropout**            | è®­ç»ƒä¸­éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œå‡å°‘ç¥ç»å…ƒé—´çš„å…±é€‚åº”æ€§                     | -                                     | å¼ºè¿«ç½‘ç»œå­¦ä¹ å†—ä½™ç‰¹å¾ï¼Œç±»ä¼¼é›†æˆå­¦ä¹ æ•ˆæœï¼Œä½†å¢åŠ è®­ç»ƒæ—¶é—´   |\n",
    "| **æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰** | æ ¹æ®éªŒè¯é›†æ€§èƒ½åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®                     | -                                     | ç®€å•æ˜“è¡Œï¼Œéœ€éªŒè¯é›†ç›‘æ§ï¼Œå¯èƒ½é”™è¿‡æœ€ä½³æ¨¡å‹                |\n",
    "| **æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰** | å¯¹è®­ç»ƒæ•°æ®éšæœºå˜æ¢ï¼ˆå¦‚æ—‹è½¬/è£å‰ªï¼‰ï¼Œæ‰©å±•æ•°æ®é›†å¤šæ ·æ€§          | -                                     | æå‡æ¨¡å‹é²æ£’æ€§ï¼Œå°¤å…¶é€‚ç”¨äºå›¾åƒ/æ–‡æœ¬æ•°æ®ï¼Œä½†å¢åŠ è®¡ç®—å¼€é”€ |\n",
    "| **æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰** | å¯¹æ¯å±‚è¾“å…¥å½’ä¸€åŒ–å¤„ç†ï¼Œç¨³å®šæ•°æ®åˆ†å¸ƒ                        | -                                     | åŠ é€Ÿè®­ç»ƒå¹¶é—´æ¥æ­£åˆ™åŒ–ï¼Œå‡å°‘å¯¹åˆå§‹æƒé‡çš„æ•æ„Ÿåº¦       |\n",
    "| **æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰** | ä¼˜åŒ–å™¨ä¸­åŠ å…¥æƒé‡è¡°å‡é¡¹ï¼Œç­‰ä»·äºL2æ­£åˆ™åŒ–                          | åŒL2æ­£åˆ™åŒ–                            | ç›´æ¥çº¦æŸæƒé‡å¹…åº¦ï¼Œä¸ä¼˜åŒ–å™¨ï¼ˆå¦‚AdamWï¼‰ç»“åˆä½¿ç”¨              |\n",
    "| **å‰ªæï¼ˆPruningï¼‰**    | ç§»é™¤è®­ç»ƒåä¸é‡è¦çš„è¿æ¥æˆ–ç¥ç»å…ƒï¼Œå‹ç¼©æ¨¡å‹è§„æ¨¡                         | -                                     | å‡å°‘æ¨ç†è®¡ç®—é‡ï¼Œé€‚åˆèµ„æºå—é™è®¾å¤‡ï¼Œä½†éœ€é¢å¤–è®­ç»ƒæ­¥éª¤              |\n",
    "\n",
    "ä¸‹é¢ç¤ºä¾‹ä»£ç ä¸­ä½¿ç”¨äº†L2æ­£åˆ™åŒ–æœºåˆ¶\n",
    "\n",
    "**L2æ­£åˆ™åŒ–æœºåˆ¶**\n",
    "\n",
    "1. **L2æ­£åˆ™åŒ–çš„å®šä¹‰**\n",
    "\n",
    "- L2æ­£åˆ™åŒ–åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡å¹³æ–¹å’Œçš„æƒ©ç½šé¡¹ï¼Œæ–°çš„æŸå¤±å‡½æ•°ä¸ºï¼š$L = \\text{MSE} + \\lambda \\left( \\|W_1\\|_2^2 + \\|W_2\\|_2^2 + \\|W_3\\|_2^2 \\right)$\n",
    "\n",
    "  å…¶ä¸­ï¼š\n",
    "  - $\\text{MSE} = \\frac{1}{\\text{batch\\_size}} \\sum_{i=1}^{\\text{batch\\_size}} (y_{\\text{pred},i} - y_i)^2$ æ˜¯å‡æ–¹è¯¯å·®ã€‚\n",
    "  - $\\|W_k\\|_2^2 = \\sum_{i,j} W_{k,ij}^2$ æ˜¯æƒé‡çŸ©é˜µçš„ L2 èŒƒæ•°çš„å¹³æ–¹ã€‚\n",
    "  - $\\lambda = \\text{l2\\_lambda}$ æ˜¯æ­£åˆ™åŒ–ç³»æ•°ï¼Œæ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦ã€‚\n",
    "  - \n",
    "- ä»£ç ä¸­æ·»åŠ äº†è¶…å‚æ•°l2_lambda = 0.01ï¼Œè¡¨ç¤ºæ­£åˆ™åŒ–å¼ºåº¦ã€‚\n",
    "\n",
    "2. **æŸå¤±è®¡ç®—ä¸­çš„æ”¹åŠ¨**\n",
    "   æŸå¤±ä»£ç ä¸­æ·»åŠ äº†L2æ­£åˆ™åŒ–é¡¹ã€‚\n",
    "   ```python\n",
    "   l2_loss = l2_lambda * (np.sum(W1 ** 2) + np.sum(W2 ** 2) + np.sum(W3 ** 2))\n",
    "   batch_loss = mse_loss + l2_loss\n",
    "   ```\n",
    "3. **æ¢¯åº¦è®¡ç®—ä¸­çš„æ”¹åŠ¨**\n",
    "   - L2 æ­£åˆ™åŒ–å¯¹æƒé‡ $ W_k $ çš„æ¢¯åº¦è´¡çŒ®ä¸ºï¼š$$\\frac{\\partial}{\\partial W_k} \\left( \\lambda \\|W_k\\|_2^2 \\right) = 2 \\lambda W_k$$\n",
    "   - å› æ­¤ï¼Œæƒé‡æ¢¯åº¦éœ€è¦åŠ ä¸Š L2 æ­£åˆ™åŒ–é¡¹ï¼š\n",
    "     - åŸæ¢¯åº¦ï¼šdW3 = a2.T @ dloss\n",
    "     - æ–°æ¢¯åº¦ï¼šdW3 = a2.T @ dloss + 2 * l2_lambda * W3\n",
    "     - ç±»ä¼¼åœ°ï¼ŒdW2 å’Œ dW1 ä¹Ÿå¢åŠ æ­£åˆ™åŒ–é¡¹\n",
    "   - åç½®ï¼ˆb1, b2, b3ï¼‰ä¸å‚ä¸ L2 æ­£åˆ™åŒ–ï¼Œæ¢¯åº¦è®¡ç®—ä¸å˜ã€‚\n",
    "\n",
    "4. **æ­£åˆ™åŒ–çš„æ•ˆæœ**\n",
    "   - L2 æ­£åˆ™åŒ–é€šè¿‡æƒ©ç½šå¤§æƒé‡å€¼ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ è¾ƒå°çš„æƒé‡ï¼Œå‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚\n",
    "   - ä»£ç ä¸­è®¾ç½® l2_lambda = 0.01ï¼Œè¿™æ˜¯ä¸€ä¸ªé€‚ä¸­çš„æ­£åˆ™åŒ–å¼ºåº¦ï¼Œå¯ä»¥æ ¹æ®å®éªŒæ•ˆæœè°ƒæ•´ï¼ˆä¾‹å¦‚ 0.001 æˆ– 0.1ï¼‰ã€‚\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db1bb-32d8-4c88-8921-3aa521b9a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "N, D_in, H1, H2, D_out = 256, 20, 50, 50, 1  # æ ·æœ¬æ•°é‡256ï¼Œè¾“å…¥ç»´åº¦20ï¼Œéšè—å±‚åŒä¸º50ï¼Œè¾“å‡ºç»´åº¦ä¸º1\n",
    "batch_size = 32  # æ¯ä¸ª mini-batch çš„å¤§å°\n",
    "\n",
    "# è¾“å…¥ X å’ŒçœŸå®æ ‡ç­¾ yï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º (N, D_in) å’Œ (N, 1)\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# ---------- æƒé‡åˆå§‹åŒ– ----------\n",
    "W1 = np.random.randn(D_in, H1) * 0.01\n",
    "b1 = np.zeros((1, H1))\n",
    "W2 = np.random.randn(H1, H2) * 0.01\n",
    "b2 = np.zeros((1, H2))\n",
    "W3 = np.random.randn(H2, D_out) * 0.01\n",
    "b3 = np.zeros((1, D_out))\n",
    "\n",
    "# ---------- è¶…å‚æ•° ----------\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "num_batches = N // batch_size  # è®¡ç®— batch æ•°é‡\n",
    "l2_lambda = 0.01  # L2 æ­£åˆ™åŒ–ç³»æ•°\n",
    "\n",
    "# ---------- æ¿€æ´»å‡½æ•° ----------\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# ---------- è®­ç»ƒå¾ªç¯ ----------\n",
    "for epoch in range(epochs):\n",
    "    # æ‰“ä¹±æ•°æ®ï¼ˆæ¯ä¸ª epoch éšæœºåŒ–æ ·æœ¬é¡ºåºï¼‰\n",
    "    indices = np.random.permutation(N)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    # åˆå§‹åŒ– epoch æ€»æŸå¤±\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # æŒ‰ mini-batch è¿­ä»£\n",
    "    for i in range(num_batches):\n",
    "        # è·å–å½“å‰ batch çš„æ•°æ®\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_shuffled[start_idx:end_idx]  # (batch_size, D_in)\n",
    "        y_batch = y_shuffled[start_idx:end_idx]  # (batch_size, D_out)\n",
    "        \n",
    "        # ---- å‰å‘ä¼ æ’­ ----\n",
    "        z1 = X_batch @ W1 + b1      # (batch_size, H1)\n",
    "        a1 = relu(z1)               # (batch_size, H1)\n",
    "        z2 = a1 @ W2 + b2           # (batch_size, H2)\n",
    "        a2 = relu(z2)               # (batch_size, H2)\n",
    "        z3 = a2 @ W3 + b3           # (batch_size, D_out)\n",
    "        y_pred = z3                 # è¾“å‡ºå±‚æ— æ¿€æ´»å‡½æ•°ï¼ˆçº¿æ€§è¾“å‡ºï¼‰\n",
    "        \n",
    "        # ---- è®¡ç®—æŸå¤±ï¼ˆåŒ…æ‹¬ L2 æ­£åˆ™åŒ–é¡¹ï¼‰ ----\n",
    "        mse_loss = np.mean((y_pred - y_batch) ** 2)\n",
    "        # L2 æ­£åˆ™åŒ–é¡¹ï¼šlambda * (||W1||^2 + ||W2||^2 + ||W3||^2)\n",
    "        l2_loss = l2_lambda * (np.sum(W1 ** 2) + np.sum(W2 ** 2) + np.sum(W3 ** 2))\n",
    "        batch_loss = mse_loss + l2_loss\n",
    "        epoch_loss += batch_loss\n",
    "        \n",
    "        # ---- åå‘ä¼ æ’­ ----\n",
    "        # MSE æŸå¤±çš„å¯¼æ•°\n",
    "        dloss = 2 * (y_pred - y_batch) / batch_size\n",
    "        \n",
    "        # ç¬¬ä¸‰å±‚æ¢¯åº¦ï¼ˆåŠ ä¸Š L2 æ­£åˆ™åŒ–é¡¹ï¼‰\n",
    "        dW3 = a2.T @ dloss + 2 * l2_lambda * W3  # (H2, D_out)\n",
    "        db3 = np.sum(dloss, axis=0, keepdims=True)  # (1, D_out)\n",
    "        \n",
    "        # ç¬¬äºŒå±‚åä¼ \n",
    "        da2 = dloss @ W3.T          # (batch_size, H2)\n",
    "        dz2 = da2 * relu_grad(z2)   # (batch_size, H2)\n",
    "        \n",
    "        dW2 = a1.T @ dz2 + 2 * l2_lambda * W2  # (H1, H2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # ç¬¬ä¸€å±‚åä¼ \n",
    "        da1 = dz2 @ W2.T            # (batch_size, H1)\n",
    "        dz1 = da1 * relu_grad(z1)   # (batch_size, H1)\n",
    "        \n",
    "        dW1 = X_batch.T @ dz1 + 2 * l2_lambda * W1  # (D_in, H1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # ---- å‚æ•°æ›´æ–° ----\n",
    "        W3 -= learning_rate * dW3\n",
    "        b3 -= learning_rate * db3\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡ epoch æŸå¤±\n",
    "    epoch_loss /= num_batches\n",
    "    \n",
    "    # æ¯ 50 ä¸ª epoch è¾“å‡ºæŸå¤±\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# ---------- æœ€ç»ˆæŸå¤±ï¼ˆå…¨æ•°æ®é›†ï¼‰ ----------\n",
    "z1 = X @ W1 + b1\n",
    "a1 = relu(z1)\n",
    "z2 = a1 @ W2 + b2\n",
    "a2 = relu(z2)\n",
    "z3 = a2 @ W3 + b3\n",
    "y_pred = z3\n",
    "mse_loss = np.mean((y_pred - y) ** 2)\n",
    "l2_loss = l2_lambda * (np.sum(W1 ** 2) + np.sum(W2 ** 2) + np.sum(W3 ** 2))\n",
    "final_loss = mse_loss + l2_loss\n",
    "print(f\"Final Loss: {final_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56903ce0-e7b7-47ef-b56a-03649cc8ea62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## åŸºäºPyTorchçš„ç¥ç»ç½‘ç»œæ„å»ºä¸è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae08ef-2f08-4d07-86b3-7e031e0614b6",
   "metadata": {},
   "source": [
    "### PyTorchçš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ä¸è®¡ç®—å›¾\n",
    "- è®¡ç®—å‡½æ•°y=ReLU(Wx+b)ï¼Œå¹¶å¯¹å…¶è¿›è¡Œåå‘ä¼ æ’­å’Œæ±‚æ¢¯åº¦\n",
    "\n",
    "PyTorchç›´æ¥æä¾›äº†ç¥ç»ç½‘ç»œè®­ç»ƒä¸­ç”¨åˆ°çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°å’Œå‚æ•°ä¼˜åŒ–ï¼ˆä¼˜åŒ–å™¨ï¼‰ç­‰ç»„ä»¶ä¸­çš„å¤šç§ä¸»æµå®ç°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»£ç ä¸­ç›´æ¥è°ƒç”¨è€Œæ— é¡»å†æ‰‹åŠ¨è¿›è¡Œç¼–ç å®ç°ã€‚\n",
    "\n",
    "| **ç»„ä»¶**               | **åŠŸèƒ½**                                                                 | **æ ¸å¿ƒç±»/å‡½æ•°ç¤ºä¾‹**                                 | **åº•å±‚å®ç°ä¾èµ–**               |\n",
    "| :--------------------- | :----------------------------------------------------------------------- | :-------------------------------------------------- | :----------------------------- |\n",
    "| **Tensor**             | å¤šç»´æ•°ç»„ï¼Œæ”¯æŒ GPU åŠ é€Ÿå’Œè‡ªåŠ¨å¾®åˆ†                                           | `torch.tensor([[1, 2], [3, 4]], dtype=torch.float)` | ATen å¼ é‡åº“ï¼ˆC++ï¼‰     |\n",
    "| **Autograd**           | è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼ŒåŠ¨æ€æ„å»ºè®¡ç®—å›¾å¹¶è®¡ç®—æ¢¯åº¦                                       | `x.requires_grad=True; y.backward()`                | Autograd å¼•æ“ï¼ˆC++ï¼‰    |\n",
    "| **nn.Module**          | ç¥ç»ç½‘ç»œæ¨¡å—åŸºç±»ï¼Œå°è£…å±‚ã€æŸå¤±å‡½æ•°ç­‰                                         | `class CNN(nn.Module): def __init__(self): ...`     | pybind11 ç»‘å®šï¼ˆPython-C++ æ¥å£ï¼‰ |\n",
    "| **Optimizer**          | ä¼˜åŒ–ç®—æ³•åº“ï¼ˆSGDã€Adam ç­‰ï¼‰ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°                                     | `torch.optim.Adam(model.parameters(), lr=0.001)`    | C++ ä¼˜åŒ–å™¨åç«¯             |\n",
    "| **DataLoader**         | é«˜æ•ˆæ•°æ®åŠ è½½ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€ä¹±åºã€å¹¶è¡ŒåŠ è½½                                      | `DataLoader(dataset, batch_size=64, shuffle=True)` | å¤šè¿›ç¨‹ç®¡ç†ï¼ˆC++ï¼‰      |\n",
    "| **Distributed**        | åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ˆå¤š GPU/å¤šèŠ‚ç‚¹ï¼‰                                             | `DistributedDataParallel(model)`                    | gloo/NCCL é€šä¿¡åº“ï¼ˆC++ï¼‰ |\n",
    "| **TorchScript**        | æ¨¡å‹ç¼–è¯‘ä¸ºé™æ€å›¾ï¼Œæå‡éƒ¨ç½²æ€§èƒ½                                                | `scripted_model = torch.jit.script(model)`          | JIT ç¼–è¯‘å™¨ï¼ˆC++ï¼‰      |\n",
    "| **æ‰©å±•åº“**             | é¢†åŸŸä¸“ç”¨å·¥å…·åº“ï¼ˆCV/NLP/éŸ³é¢‘ï¼‰                                               | `torchvision.models.resnet50()`                     | ç‹¬ç«‹ C++ æ¨¡å—          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ba14d-e8c0-4db3-915e-c4051d516d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# å®šä¹‰è¾“å…¥ã€æƒé‡ã€åç½®å’Œç›®æ ‡å€¼\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)  # è¾“å…¥å¼ é‡ç»´åº¦ (1, 3)\n",
    "W = torch.tensor([[0.5, -0.2, 0.1], [0.3, 0.4, -0.5]], requires_grad=True)  # æƒé‡çŸ©é˜µç»´åº¦ (2, 3)\n",
    "b = torch.tensor([[0.1], [0.2]], requires_grad=True)  # åç½®å‘é‡ (2, 1)\n",
    "target = torch.tensor([[1.0], [0.5]])  # ç›®æ ‡å€¼ (2, 1)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "# ç¤ºä¾‹å‡½æ•°ï¼šy=ReLU(Wx + b)\n",
    "def forward(x, W, b):\n",
    "    # çº¿æ€§å˜æ¢: z = Wx + b\n",
    "    z = torch.matmul(W, x.T) + b  # çŸ©é˜µä¹˜æ³• + åç½®\n",
    "    # ReLUæ¿€æ´»: y = ReLU(z)\n",
    "    y = torch.relu(z)\n",
    "    return y\n",
    "\n",
    "# æ‰§è¡Œå‰å‘ä¼ æ’­\n",
    "y = forward(x, W, b)\n",
    "print(\"Forward output (y):\", y)\n",
    "\n",
    "# è®¡ç®—MSEæŸå¤±\n",
    "loss = F.mse_loss(y, target)\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# è¾“å‡ºæ¢¯åº¦\n",
    "print(\"Gradient of x:\", x.grad)\n",
    "print(\"Gradient of W:\", W.grad)\n",
    "print(\"Gradient of b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208527c0-74a1-4966-a24f-930b9d71906c",
   "metadata": {},
   "source": [
    "### è®¡ç®—å›¾çš„ç›¸å…³å›¾å½¢\n",
    "\n",
    "ä¾èµ–äºæ“ä½œç³»ç»Ÿä¸Šå®‰è£…å¥½graphvizåŒ…ï¼Œä¾‹å¦‚å¯¹äºUbuntuç³»ç»Ÿï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤è¿›è¡Œã€‚\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a50c16-4555-4d60-bf79-7d8a176821fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "y = forward(x, W, b)\n",
    "loss = F.mse_loss(y, target)\n",
    "make_dot(loss).render(\"computational_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68032cf-7e57-451c-b246-439ede857631",
   "metadata": {},
   "source": [
    "### åŸºäºPyTorchçš„ç¥ç»ç½‘ç»œæ„å»ºä¸è®­ç»ƒ\n",
    "- å°†å‰é¢ç¤ºä¾‹â€œå¤šå±‚ç¥ç»ç½‘ç»œâ€ä¿®æ”¹ä¸ºåŸºäºPyTorchçš„å®ç°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8ac48-e476-4d40-ae90-a27e627f4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "# å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch å¼ é‡\n",
    "X = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)  # shape: (2, 2)\n",
    "y = torch.tensor([[5.0], [11.0]], dtype=torch.float32)          # shape: (2, 1)\n",
    "\n",
    "N, D = X.shape\n",
    "\n",
    "# ç½‘ç»œç»“æ„å‚æ•°\n",
    "H1 = 4  # ç¬¬ 1 ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\n",
    "H2 = 4  # ç¬¬ 2 ä¸ªéšè—å±‚ç¥ç»å…ƒä¸ªæ•°\n",
    "lr = 0.01 # å­¦ä¹ ç‡\n",
    "\n",
    "# ---------- å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ ----------\n",
    "# PyTorch çš„ nn.Module æ˜¯æ„å»ºç¥ç»ç½‘ç»œçš„åŸºç¡€\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, H1_size, H2_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # å®šä¹‰å„å±‚\n",
    "        # nn.Linear(in_features, out_features) è‡ªåŠ¨åŒ…å«æƒé‡å’Œåç½®\n",
    "        self.fc1 = nn.Linear(input_size, H1_size)\n",
    "        self.relu1 = nn.ReLU() # ReLU æ¿€æ´»å‡½æ•°\n",
    "        self.fc2 = nn.Linear(H1_size, H2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(H2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # å®šä¹‰å‰å‘ä¼ æ’­è·¯å¾„\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "model = SimpleNN(D, H1, H2, 1)\n",
    "\n",
    "# ---------- å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ----------\n",
    "# å‡æ–¹è¯¯å·®æŸå¤± (MSELoss)\n",
    "criterion = nn.MSELoss(reduction='mean') # reduction='mean'è¡¨ç¤ºå–æ‰¹æ¬¡çš„å¹³å‡æŸå¤±\n",
    "# Adamä¼˜åŒ–å™¨ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†å‚æ•°çš„æ¢¯åº¦æ›´æ–°\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# ---------- å•æ¬¡è®­ç»ƒè¿­ä»£ ----------\n",
    "print(\"--- è¿›è¡Œå•æ¬¡è®­ç»ƒè¿­ä»£ ---\")\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "y_hat = model(X)\n",
    "\n",
    "# è®¡ç®—æŸå¤±\n",
    "# PyTorchçš„MSELosså·²ç»å¤„ç†äº†0.5å’Œæ±‚å¹³å‡\n",
    "loss = criterion(y_hat, y)\n",
    "\n",
    "# åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "optimizer.zero_grad() # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦ï¼Œé¿å…ç´¯ç§¯\n",
    "loss.backward()       # æ‰§è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦\n",
    "optimizer.step()      # æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ---------- è¾“å‡ºæ›´æ–°åçš„å‚æ•° (å¯é€‰) ----------\n",
    "# æ³¨æ„ï¼šPyTorch æƒé‡çš„å½¢çŠ¶é€šå¸¸æ˜¯ (out_features, in_features)ï¼Œä¸ NumPy ä¸­çš„ (in_features, out_features) ç›¸å\n",
    "print(\"\\n--- æ›´æ–°åçš„å‚æ•° ---\")\n",
    "print(\"W1 (fc1.weight):\\n\", model.fc1.weight.data)\n",
    "print(\"b1 (fc1.bias):\\n\", model.fc1.bias.data)\n",
    "print(\"\\nW2 (fc2.weight):\\n\", model.fc2.weight.data)\n",
    "print(\"b2 (fc2.bias):\\n\", model.fc2.bias.data)\n",
    "print(\"\\nW3 (fc3.weight):\\n\", model.fc3.weight.data)\n",
    "print(\"b3 (fc3.bias):\\n\", model.fc3.bias.data)\n",
    "\n",
    "\n",
    "# ---------- éªŒè¯æœ€ç»ˆé¢„æµ‹ ----------\n",
    "with torch.no_grad(): # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œä¸è®¡ç®—æ¢¯åº¦\n",
    "    y_pred_final = model(X)\n",
    "\n",
    "print(\"\\n--- æœ€ç»ˆé¢„æµ‹ ---\")\n",
    "print(\"åŸå§‹æ•°æ® X:\\n\", X)\n",
    "print(\"åŸå§‹æ ‡ç­¾ y:\\n\", y)\n",
    "print(\"é¢„æµ‹å€¼ y_hat:\\n\", y_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7c23f-b311-4706-b12e-0a2a0f7dce2b",
   "metadata": {},
   "source": [
    "### åŸºäºç¥ç»ç½‘ç»œçš„å¤šè½®è®­ç»ƒç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cc1ac-b0a4-457c-b066-9c08b710a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- æ¨¡æ‹Ÿæ•°æ® ----------\n",
    "N, D_in, H1, H2, D_out = 256, 20, 50, 50, 1  # æ ·æœ¬æ•°é‡256ï¼Œè¾“å…¥ç»´åº¦20ï¼Œéšè—å±‚åŒä¸º50ï¼Œè¾“å‡ºç»´åº¦ä¸º1\n",
    "batch_size = 32  # æ¯ä¸ª mini-batch çš„å¤§å°\n",
    "\n",
    "# è¾“å…¥Xå’ŒçœŸå®æ ‡ç­¾yï¼Œè½¬æ¢ä¸ºPyTorchå¼ é‡\n",
    "X = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# åˆ›å»ºPyTorchæ•°æ®é›†å’Œ DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # éšæœºæ‰“ä¹±\n",
    "\n",
    "# ---------- å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ ----------\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # ç¬¬ä¸€éšè—å±‚\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # ç¬¬äºŒéšè—å±‚\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # è¾“å‡ºå±‚\n",
    "        self.linear3 = nn.Linear(H2, D_out)\n",
    "        \n",
    "        # ä½¿ç”¨Heåˆå§‹åŒ–æƒé‡\n",
    "        nn.init.kaiming_normal_(self.linear1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.kaiming_normal_(self.linear2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        nn.init.kaiming_normal_(self.linear3.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.zeros_(self.linear3.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# ---------- è¶…å‚æ•° ----------\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "l2_lambda = 0.01  # L2æ­£åˆ™åŒ–ç³»æ•°\n",
    "\n",
    "# ---------- åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ----------\n",
    "model = NeuralNetwork(D_in, H1, H2, D_out)\n",
    "criterion = nn.MSELoss(reduction='mean')  # å‡æ–¹è¯¯å·®æŸå¤±\n",
    "# ä½¿ç”¨SGDä¼˜åŒ–å™¨ï¼Œweight_decayå®ç°L2æ­£åˆ™åŒ–\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "# ---------- è®­ç»ƒå¾ªç¯ ----------\n",
    "model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # éå† mini-batches\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ˆMSEæŸå¤±ï¼ŒL2æ­£åˆ™åŒ–ç”±ä¼˜åŒ–å™¨è‡ªåŠ¨å¤„ç†ï¼‰\n",
    "        mse_loss = criterion(y_pred, y_batch)\n",
    "        epoch_loss += mse_loss.item()\n",
    "        \n",
    "        # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦\n",
    "        mse_loss.backward()    # è®¡ç®—æ¢¯åº¦\n",
    "        optimizer.step()       # æ›´æ–°å‚æ•°\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡ epoch æŸå¤±\n",
    "    epoch_loss /= num_batches\n",
    "    \n",
    "    # æ¯ 50 ä¸ª epoch è¾“å‡ºæŸå¤±\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "# ---------- æœ€ç»ˆæŸå¤±ï¼ˆå…¨æ•°æ®é›†ï¼‰ ----------\n",
    "model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "    mse_loss = criterion(y_pred, y)\n",
    "    # è®¡ç®— L2 æ­£åˆ™åŒ–é¡¹\n",
    "    l2_loss = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad and len(param.shape) > 1:  # ä»…å¯¹æƒé‡çŸ©é˜µåº”ç”¨L2æ­£åˆ™åŒ–\n",
    "            l2_loss += torch.sum(param ** 2)\n",
    "    l2_loss *= l2_lambda\n",
    "    final_loss = mse_loss + l2_loss\n",
    "    print(f\"Final Loss: {final_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7269f-68bd-4b46-9320-a74803a68efc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å¼ é‡åŠå…¶è‡ªåŠ¨æ±‚å¯¼\n",
    "\n",
    "åœ¨PyTorchçš„ä¸–ç•Œé‡Œï¼Œå¼ é‡ï¼ˆTensorï¼‰ æ˜¯æ‰€æœ‰æ“ä½œçš„åŸºçŸ³ï¼Œå°±åƒNumPyä¸­çš„ndarrayä¸€æ ·ã€‚ç„¶è€Œï¼ŒPyTorchçš„å¼ é‡è¿œä¸æ­¢ä¸€ä¸ªå¤šç»´æ•°ç»„é‚£ä¹ˆç®€å•ï¼Œå®ƒè¿˜æ‰¿è½½ç€åœ¨GPUä¸ŠåŠ é€Ÿè®¡ç®—ä»¥åŠè‡ªåŠ¨æ±‚å¯¼çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œè¿™äº›éƒ½æ˜¯æ·±åº¦å­¦ä¹ çš„å‘½è„‰ã€‚\n",
    "\n",
    "ç®€å•æ¥è¯´ï¼Œå¼ é‡å°±æ˜¯ä¸€ä¸ªå¤šç»´æ•°ç»„ï¼Œå¯ä»¥å­˜å‚¨å„ç§ç±»å‹çš„æ•°æ®ï¼Œä»ç®€å•çš„æ ‡é‡ï¼ˆä¸€ä¸ªæ•°å­—ï¼‰åˆ°å¤æ‚çš„å›¾åƒã€è§†é¢‘æ•°æ®ï¼Œç”šè‡³ç¥ç»ç½‘ç»œçš„æƒé‡å’Œæ¢¯åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95add54e-0e4d-4d74-8b69-1dd5d7fbf652",
   "metadata": {},
   "source": [
    "### å¼ é‡åˆ›å»º\n",
    "\n",
    "å¼ é‡æ”¯æŒå¤šç§åˆ›å»ºæ–¹å¼ï¼ŒåŒ…æ‹¬ä»å·²æœ‰æ•°æ®åˆ›å»ºã€ä½¿ç”¨å‡½æ•°åˆ›å»ºã€ä»æ–‡ä»¶åŠ è½½ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c80f60-c8bb-459f-9ed2-f6dfc18ad966",
   "metadata": {},
   "source": [
    "#### ä» Python åˆ—è¡¨æˆ– NumPy æ•°ç»„åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608e9f3-e1d1-4cb7-9f7a-51aa20ece603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ä»Pythonåˆ—è¡¨åˆ›å»º\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(\"ä»åˆ—è¡¨åˆ›å»º:\\n\", x_data)\n",
    "\n",
    "# ä»NumPyæ•°ç»„åˆ›å»º\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array) # æ³¨æ„ï¼šä¼šå…±äº«å†…å­˜\n",
    "print(\"ä»NumPyæ•°ç»„åˆ›å»º:\\n\", x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbf5b4-7198-40f3-bc17-18581fe8edf9",
   "metadata": {},
   "source": [
    "#### åˆ›å»ºå…¨0æˆ–å…¨1å¼ é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe7d89-0af5-4549-af74-fc8a245cec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 è¡Œ 3 åˆ—çš„å…¨ä¸€å¼ é‡\n",
    "x_ones = torch.ones(2, 3)\n",
    "print(\"å…¨ä¸€å¼ é‡:\\n\", x_ones)\n",
    "\n",
    "# 2 è¡Œ 3 åˆ—çš„å…¨é›¶å¼ é‡\n",
    "x_zeros = torch.zeros(2, 3)\n",
    "print(\"å…¨é›¶å¼ é‡:\\n\", x_zeros)\n",
    "\n",
    "# æŒ‡å®šå€¼\n",
    "x_full = torch.full((2, 3), 7)\n",
    "print(\"å…¨æŒ‡å®šå€¼å¼ é‡:\\n\", x_full)\n",
    "\n",
    "# å•ä½çŸ©é˜µ/å¯¹è§’çŸ©é˜µ\n",
    "x_eye = torch.eye(3)\n",
    "x_diag = torch.diag(torch.tensor([1, 2, 3]))\n",
    "print(\"å•ä½çŸ©é˜µ:\\n\", x_eye)\n",
    "print(\"å¯¹è§’çŸ©é˜µ:\\n\", x_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa46358-54fb-4850-a0df-f87bc77fbb2e",
   "metadata": {},
   "source": [
    "#### åˆ›å»ºéšæœºå¼ é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d093a5-8716-4f8e-9c41-34f24defd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 è¡Œ 3 åˆ—çš„éšæœºå¼ é‡ (0åˆ°1ä¹‹é—´å‡åŒ€åˆ†å¸ƒ)\n",
    "x_rand = torch.rand(2, 3)\n",
    "print(\"å‡åŒ€åˆ†å¸ƒéšæœºå¼ é‡:\\n\", x_rand)\n",
    "\n",
    "# 2è¡Œ3åˆ—çš„éšæœºå¼ é‡ (æ ‡å‡†æ­£æ€åˆ†å¸ƒ)\n",
    "x_randn = torch.randn(2, 3)\n",
    "print(\"æ­£æ€åˆ†å¸ƒéšæœºå¼ é‡:\\n\", x_randn)\n",
    "\n",
    "# [0,10)ä¹‹é—´çš„éšæœºæ•°æ„æˆçš„å¼ é‡ï¼Œç»´åº¦2x3\n",
    "torch.randint(0, 10, (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6019c8-844b-4750-a24e-d4b8f5f1161a",
   "metadata": {},
   "source": [
    "#### ç‰¹å®šåˆ†å¸ƒåˆå§‹åŒ–\n",
    "torch.normal()å¯ç”¨äºç”Ÿæˆç¬¦åˆæ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰çš„éšæœºæ•°å¼ é‡ï¼Œå®ƒæ”¯æŒå¤šç§å‚æ•°ç»„åˆï¼Œå¯æ ¹æ®éœ€æ±‚çµæ´»ç”Ÿæˆä¸åŒå‡å€¼ã€æ ‡å‡†å·®å’Œå½¢çŠ¶çš„éšæœºæ•°æ®ã€‚\n",
    "\n",
    "**æ ¸å¿ƒå‚æ•°**\n",
    "- meanï¼šæ­£æ€åˆ†å¸ƒçš„å‡å€¼ï¼ˆæ ‡é‡æˆ–å¼ é‡ï¼‰ã€‚\n",
    "- stdï¼šæ­£æ€åˆ†å¸ƒçš„æ ‡å‡†å·®ï¼ˆæ ‡é‡æˆ–å¼ é‡ï¼‰ã€‚\n",
    "- sizeï¼šè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ï¼ˆå…ƒç»„ï¼‰ï¼Œå½“ mean å’Œ std å‡ä¸ºæ ‡é‡æ—¶å¿…é¡»æŒ‡å®šã€‚\n",
    "- generatorï¼šæ§åˆ¶éšæœºæ•°ç”Ÿæˆçš„ç§å­ï¼ˆå¯é€‰ï¼‰ã€‚\n",
    "- outï¼šæŒ‡å®šè¾“å‡ºå¼ é‡ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fd53e-056f-4d4d-92f0-b4d821fd122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå½¢çŠ¶ä¸º (2, 3) çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰\n",
    "tensor = torch.normal(mean=0.0, std=1.0, size=(2, 3))\n",
    "print(\"å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰çš„éšæœºå¼ é‡:\\n\", tensor)\n",
    "\n",
    "# ä½¿ç”¨generatorå‚æ•°ï¼ˆéšæœºæ•°ç§å­ï¼‰ç¡®ä¿ç»“æœå¯å¤ç°\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "tensor = torch.normal(mean=0, std=1, size=(2,2), generator=gen)\n",
    "print(\"å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰çš„éšæœºå¼ é‡ï¼Œå¯å¤ç°:\\n\", tensor)\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹1ï¼šç¥ç»ç½‘ç»œæƒé‡åˆå§‹åŒ–\n",
    "weights = torch.normal(mean=0.0, std=0.01, size=(100, 200))  # é«˜æ–¯åˆå§‹åŒ–\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹2ï¼šæ•°æ®å¢å¼ºï¼ˆæ·»åŠ å™ªå£°ï¼‰\n",
    "clean_data = torch.randn(10, 3, 224, 224)  # åŸå§‹æ•°æ®\n",
    "noisy_data = clean_data + torch.normal(mean=0, std=0.1, size=clean_data.shape)   # .shapeæ–¹æ³•ç”¨äºè·å–æŒ‡å®štensorçš„å½¢çŠ¶\n",
    "\n",
    "# ç”Ÿæˆçº¿æ€§å›å½’æ•°æ®é›†ï¼šy = Xw + b + å™ªå£°\n",
    "X = torch.normal(0, 1, (100, 2))\n",
    "y = X @ torch.tensor([3.0, -2.0]) + 1.5 + torch.normal(0, 0.1, (100,))\n",
    "print(\"ç”Ÿæˆçš„å›å½’æ•°æ®é›†:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d50d6-4c8b-4cd1-8571-7db1a2ba73fd",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ arange()/linspace()/logspace()\n",
    "- torch.arange()ï¼šåˆ›å»ºç­‰å·®æ•°åˆ—ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰\n",
    "- torch.linspace()ï¼šåˆ›å»ºå‡åŒ€é—´éš”çš„çº¿æ€§åºåˆ—\n",
    "- torch.logspace()ï¼šåˆ›å»ºå¯¹æ•°å°ºåº¦ä¸Šçš„å‡åŒ€åºåˆ—\n",
    "\n",
    "**å¸¸ç”¨å‚æ•°**\n",
    "- startï¼šæŒ‡æ•°èµ·å§‹å€¼\n",
    "- endï¼šæŒ‡æ•°ç»“æŸå€¼ï¼ˆåŒ…å«ï¼‰\n",
    "- stepsï¼šç”Ÿæˆçš„å…ƒç´ ä¸ªæ•°\n",
    "- baseï¼šå¯¹æ•°åº•æ•°ï¼ˆé»˜è®¤10ï¼‰ï¼Œlogspace()ä¸“ç”¨\n",
    "- dtypeï¼šæ•°æ®ç±»å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97b2d7-e902-48e3-9523-8b2d8677179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ®ç‚¹æ•°åºåˆ—ï¼š1.0åˆ°4.0ï¼ˆæ­¥é•¿0.5ï¼‰\n",
    "a = torch.arange(1, 4, 0.5)             # tensor([1.0, 1.5, 2.0, 2.5, 3.0, 3.5])\n",
    "# æŒ‡å®šè®¾å¤‡å’Œæ•°æ®ç±»å‹\n",
    "b = torch.arange(0, 6, 2, dtype=torch.float32, device='cpu')  # tensor([0., 2., 4.], device='cuda:0')\n",
    "\n",
    "# ç”Ÿæˆ0åˆ°1ä¹‹é—´çš„5ä¸ªç­‰åˆ†ç‚¹\n",
    "c = torch.linspace(0, 1, 5)             # tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
    "# ç”Ÿæˆ-1åˆ°1ä¹‹é—´çš„3ä¸ªç‚¹\n",
    "d = torch.linspace(-1, 1, 3)             # tensor([-1., 0., 1.])\n",
    "\n",
    "# ç”Ÿæˆ10^0åˆ°10^2ä¹‹é—´çš„5ä¸ªç‚¹ï¼ˆåº•æ•°10ï¼‰\n",
    "e = torch.logspace(0, 2, 5)             # tensor([1.0000, 3.1623, 10.0000, 31.6228, 100.0000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70c4b5-ca37-4522-9274-946e8690b0af",
   "metadata": {},
   "source": [
    "#### å…‹éš†å’Œå¤åˆ¶\n",
    "åˆ›å»ºçš„æ–°å¼ é‡ä¸åŸå¼ é‡æ•°æ®ç›¸åŒï¼Œä½†ä¸å…±äº«å†…å­˜ï¼Œä¿®æ”¹å‰¯æœ¬ä¸å½±å“åŸå¼ é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb6a63-7d22-408c-980e-0edbb361f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = a.clone()\n",
    "b[0] = 100\n",
    "print(a)  # tensor([1, 2, 3])  åŸå¼ é‡ä¸å˜\n",
    "print(b)  # tensor([100, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdd9b8-68a8-46fd-bfcb-b79ae735ba13",
   "metadata": {},
   "source": [
    "#### ä»æ–‡ä»¶åŠ è½½Tensor\n",
    "\n",
    "é¦–å…ˆç”Ÿæˆç¤ºä¾‹CSVæ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd354e5-8c59-4018-811b-9ea1fa82d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# åˆ›å»º2Dæ•°å€¼æ•°ç»„ï¼ˆä¾‹å¦‚3x3çŸ©é˜µï¼‰\n",
    "data = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "# ä¿å­˜ä¸ºCSVï¼Œæ§åˆ¶æ ¼å¼å’Œåˆ†éš”ç¬¦\n",
    "np.savetxt(\"example_csv_data.csv\", data, \n",
    "           delimiter=\",\",   # é€—å·åˆ†éš”\n",
    "           fmt=\"%d\",        # æ•´æ•°æ ¼å¼ï¼ˆé¿å…ç§‘å­¦è®¡æ•°ï¼‰\n",
    "           #header=\"Col1,Col2,Col3\",  # è‡ªå®šä¹‰åˆ—å\n",
    "           comments=\"\")     # é¿å…headerè¢«æ³¨é‡Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c55d0-3ffc-4a44-af4d-fb539d6d453a",
   "metadata": {},
   "source": [
    "#### ä»csvåŠ è½½tensorç¤ºä¾‹\n",
    "\n",
    "éœ€è¦é¦–å…ˆç”±NumPyåŠ è½½ï¼Œè€Œåå†ä½¿ç”¨.from_numpy()æ–¹æ³•è½¬æ¢ä¸ºTensorã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f681e1-1987-44b0-a6a9-6c77468ef8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–CSVï¼ˆæ— æ ‡é¢˜è¡Œï¼‰\n",
    "array = np.loadtxt(\"example_csv_data.csv\", delimiter=\",\")\n",
    "# æ‰€æœ‰å­—æ®µå¿…é¡»ä¸ºæ•°å€¼å‹ï¼Œå¦åˆ™.float()æ–¹æ³•å¯èƒ½ä¼šå¤±è´¥\n",
    "tensor = torch.from_numpy(array).float()\n",
    "\n",
    "print(tensor.shape)  # e.g., [3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794cd45-134d-4363-bb62-09f6142766be",
   "metadata": {},
   "source": [
    "#### åŠ è½½å›¾åƒæ–‡ä»¶ä¸ºTensor\n",
    "\n",
    "é€‚ç”¨äºè®¡ç®—è§†è§‰ç±»çš„ä»»åŠ¡ï¼Œéœ€è¦é€šè¿‡torchvisionæ¨¡å‹çš„read_imageè¿›è¡Œã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬è¦å…ˆå®‰è£…å¥½torchvisionæ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98a074b-3771-48db-9fae-dd2b21d52ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da2fc1-9ec0-40cf-8abe-dd20764c957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "img_tensor = read_image(\"computational_graph.png\")  # ç»“æœä¸º[C, H, W]æ ¼å¼ï¼Œuint8ç±»å‹\n",
    "img_tensor = img_tensor.float() / 255.0  # è½¬ä¸ºfloatå¹¶å½’ä¸€åŒ–\n",
    "\n",
    "print(img_tensor.shape)  # e.g., [3, 224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8a389-08d9-4a57-b135-67146ba2e224",
   "metadata": {},
   "source": [
    "#### åŠ è½½éŸ³é¢‘æ–‡ä»¶ä¸ºTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffbbfc0-facb-470f-abc4-91af2c5031d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(\"audio.wav\")  # waveformæ˜¯ [channels, time]\n",
    "\n",
    "print(waveform.shape)      # e.g., [1, 16000]\n",
    "print(sample_rate)         # é‡‡æ ·ç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813faef4-b760-464d-a8a1-8e53b1678702",
   "metadata": {},
   "source": [
    "### å¼ é‡æ”¯æŒçš„æ“ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83317755-1304-4c12-8822-deb0279a5bca",
   "metadata": {},
   "source": [
    "#### ç´¢å¼•ä¸åˆ‡ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce002d-37ef-4326-8212-17d24bf81a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(x[0])        # ç¬¬ä¸€è¡Œ\n",
    "print(x[:, 1])     # ç¬¬äºŒåˆ—\n",
    "print(x[1, 2])     # ç¬¬2è¡Œç¬¬3åˆ—çš„å…ƒç´ \n",
    "print(x[:, 1:])    # æ‰€æœ‰è¡Œï¼Œç¬¬2åˆ—åˆ°æœ€å\n",
    "\n",
    "# é«˜çº§ç´¢å¼•\n",
    "indices = torch.tensor([0, 2])\n",
    "print(x[0, indices])  # ç¬¬1è¡Œä¸­çš„ç¬¬1åˆ—å’Œç¬¬3åˆ—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f7e59-7b37-4db5-a7b8-fbcd79f672d4",
   "metadata": {},
   "source": [
    "#### å˜å½¢æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9c085-401e-4e17-880f-e114426442d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)\n",
    "\n",
    "print(\"åŸå§‹:\", x)\n",
    "print(\"reshape:\", x.view(3, 4))\n",
    "print(\"æ·»åŠ ç»´åº¦:\", x.unsqueeze(0).shape)\n",
    "print(\"å»é™¤ç»´åº¦:\", x.unsqueeze(0).squeeze().shape)\n",
    "print(\"è½¬ç½®:\", torch.randn(2, 3).t())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae53535-2bf3-40e8-b92b-12271e7380ad",
   "metadata": {},
   "source": [
    "#### è¿æ¥æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f89030-a69b-4c30-9ad7-9186c6771a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(2, 3)\n",
    "\n",
    "print(\"æ‹¼æ¥ (dim=0):\", torch.cat([a, b], dim=0))  # 4x3\n",
    "print(\"æ‹¼æ¥ (dim=1):\", torch.cat([a, b], dim=1))  # 2x6\n",
    "\n",
    "# å †å ï¼ˆå¢åŠ æ–°ç»´åº¦ï¼‰\n",
    "print(\"å †å :\", torch.stack([a, b], dim=0))  # 2x2x3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f4f8e-eea4-47ae-bfbb-73437e02ffe8",
   "metadata": {},
   "source": [
    "#### æ•°å­¦æ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e54369-3985-4395-aa9c-86bed8001da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"åŠ :\", x + y)\n",
    "print(\"ä¹˜:\", x * y)\n",
    "print(\"ç‚¹ç§¯:\", torch.dot(x, y))\n",
    "print(\"çŸ©é˜µä¹˜:\", torch.matmul(x.unsqueeze(0), y.unsqueeze(1)))  # 1x1\n",
    "print(\"æŒ‡æ•°:\", torch.exp(x))\n",
    "print(\"å¯¹æ•°:\", torch.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83726d-6451-4def-82f0-8eb8eada342c",
   "metadata": {},
   "source": [
    "#### èšåˆæ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e72b64-0f07-450c-b9ee-7b1a54c006d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(\"æ€»å’Œ:\", x.sum())\n",
    "print(\"å‡å€¼:\", x.float().mean())\n",
    "print(\"æœ€å°/æœ€å¤§:\", x.min(), x.max())\n",
    "print(\"æŒ‰è¡Œæ±‚å’Œ:\", x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a255902-1b97-4515-bc42-206008c5d27a",
   "metadata": {},
   "source": [
    "### è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de88195-a820-42d2-a9c1-8011e302877e",
   "metadata": {},
   "source": [
    "#### è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181b332-1b30-4bf6-b8ae-77c1064e73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†ï¼šNumPyä¸PyTorch Tensor\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ä» NumPy åˆ›å»º Tensor\n",
    "np_array = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "tensor_from_np = torch.from_numpy(np_array)\n",
    "print(\"NumPyè½¬Tensorï¼š\\n\", tensor_from_np)\n",
    "\n",
    "# ä» Tensor åˆ›å»º NumPy\n",
    "tensor = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "np_from_tensor = tensor.numpy()\n",
    "print(\"Tensorè½¬NumPyï¼š\\n\", np_from_tensor)\n",
    "\n",
    "# ä¿®æ”¹ä¼šåŒæ­¥ï¼ˆå…±äº«å†…å­˜ï¼‰\n",
    "tensor_from_np[0, 0] = 99\n",
    "print(\"ä¿®æ”¹åçš„NumPyï¼š\\n\", np_array)\n",
    "\n",
    "## ç¬¬äºŒéƒ¨åˆ†ï¼šä»€ä¹ˆæ˜¯è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ï¼Ÿ\n",
    "\n",
    "# NumPyä¸æ”¯æŒè‡ªåŠ¨æ±‚å¯¼ï¼Œåªèƒ½æ‰‹åŠ¨å†™å¯¼æ•°\n",
    "def f_np(x):\n",
    "    return x**2 + 3*x + 5\n",
    "\n",
    "x_val = 2.0\n",
    "delta = 1e-5\n",
    "numerical_grad = (f_np(x_val + delta) - f_np(x_val - delta)) / (2 * delta)\n",
    "print(\"æ•°å€¼æ±‚å¯¼ç»“æœï¼ˆNumPyæ‰‹åŠ¨ï¼‰ï¼š\", numerical_grad)\n",
    "\n",
    "## ç¬¬ä¸‰éƒ¨åˆ†ï¼šPyTorchè‡ªåŠ¨æ±‚å¯¼æœºåˆ¶\n",
    "\n",
    "# PyTorch ä¸­çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**2 + 3 * x + 5\n",
    "y.backward()  # è‡ªåŠ¨åå‘ä¼ æ’­\n",
    "print(\"yå¯¹xçš„æ¢¯åº¦å€¼ï¼š\", x.grad)\n",
    "\n",
    "# è‡ªåŠ¨æ±‚å¯¼æ”¯æŒæ›´å¤æ‚çš„æ¨¡å‹\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([0.1, 0.2, 0.3], requires_grad=True)\n",
    "b = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "# çº¿æ€§æ¨¡å‹\n",
    "y_pred = x @ w + b\n",
    "loss = y_pred.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"wçš„æ¢¯åº¦ï¼š\", w.grad)\n",
    "print(\"bçš„æ¢¯åº¦ï¼š\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7494059-09de-4b62-a2dd-a10623d7c1fa",
   "metadata": {},
   "source": [
    "#### Tensorä¸Šçš„grad_fnå±æ€§\n",
    "\n",
    "- æ¯ä¸ªå…·æœ‰requires_grad=Trueçš„å¼ é‡ï¼ˆé™¤äº†ç”¨æˆ·è‡ªå·±åˆ›å»ºçš„å¶å­å¼ é‡å¤–ï¼‰éƒ½æœ‰ä¸€ä¸ª**grad_fn**å±æ€§ï¼Œè¯¥å±æ€§æ˜¯éå¶å­å¼ é‡ä¸“æœ‰\n",
    "- è¯¥å±æ€§æŒ‡å‘ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°è®°å½•äº†åˆ›å»ºå½“å‰å¼ é‡æ‰€ç”¨çš„æ“ä½œï¼Œå¹¶ä¸”çŸ¥é“å¦‚ä½•è®¡ç®—è¯¥æ“ä½œçš„æ¢¯åº¦\n",
    "\n",
    "\n",
    "\n",
    "ä»¥ä¸‹è„šæœ¬æ‰§è¡Œåè¿”å›çš„MulBackward0å’ŒAddBackward0å°±æ˜¯PyTorchå†…éƒ¨ç”¨æ¥è®¡ç®—æ¢¯åº¦é“¾å¼æ³•åˆ™çš„å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559cea8-1720-4042-88dd-45ebdcae1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True) # å¶å­å¼ é‡\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")\n",
    "\n",
    "y = x * 2 # éå¶å­å¼ é‡ï¼Œç”±ä¹˜æ³•æ“ä½œç”Ÿæˆ\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "\n",
    "z = y + 3 # éå¶å­å¼ é‡ï¼Œç”±åŠ æ³•æ“ä½œç”Ÿæˆ\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106deb1e-69d7-44bf-b95b-7625479fa836",
   "metadata": {},
   "source": [
    "#### gradå±æ€§\n",
    "\n",
    "loss.backward()è¢«è°ƒç”¨åï¼Œrequires_grad=Trueçš„**å¶å­å¼ é‡**çš„.gradå±æ€§ä¼šç´¯ç§¯å…¶æ¢¯åº¦å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cff2f-49b6-4de5-bf28-7af485a2f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**2 + 5 * x + 3\n",
    "y.backward() # è®¡ç®—æ¢¯åº¦\n",
    "\n",
    "print(f\"x çš„æ¢¯åº¦: {x.grad}\") # dy/dx = 2x + 5 = 2*2 + 5 = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a552c-6e77-4581-a5ba-f84ebbc48036",
   "metadata": {},
   "source": [
    "#### è‡ªåŠ¨æ±‚å¯¼çš„è¿‡ç¨‹ç¤ºä¾‹\n",
    "\n",
    "ä¸ºäº†è¯¦ç»†è¯´æ˜PyTorchçš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ï¼Œä»¥å‡½æ•°$ y = W(\\text{ReLU}(WX + b)) + b $ä¸ºä¾‹ï¼Œæˆ‘ä»¬å°†åˆ†æå“ªäº›æ˜¯å¶å¼ é‡ï¼ˆleaf tensorsï¼‰ã€å“ªäº›æ˜¯éå¶å¼ é‡ï¼ˆnon-leaf tensorsï¼‰ï¼Œä»¥åŠè‡ªåŠ¨æ±‚å¯¼çš„å®Œæ•´è¿‡ç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69ceda-4fbb-4d42-9e75-1ff8e5731ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# å®šä¹‰å¼ é‡\n",
    "X = torch.randn(2, 3, requires_grad=True)    # è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ (2, 3)\n",
    "W1 = torch.randn(4, 3, requires_grad=True)   # ç¬¬ä¸€å±‚æƒé‡ï¼Œå½¢çŠ¶ (4, 3)\n",
    "b1 = torch.randn(4, requires_grad=True)      # ç¬¬ä¸€å±‚åç½®ï¼Œå½¢çŠ¶ (4,)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)   # ç¬¬äºŒå±‚æƒé‡ï¼Œå½¢çŠ¶ (4, 4)\n",
    "b2 = torch.randn(4, requires_grad=True)      # ç¬¬äºŒå±‚åç½®ï¼Œå½¢çŠ¶ (4,)\n",
    "\n",
    "# å‰å‘è®¡ç®—\n",
    "z = torch.matmul(X, W1.t()) + b1             # W1X + b1ï¼Œå½¢çŠ¶ (2, 4)\n",
    "\n",
    "z_relu = torch.relu(z)                       # ReLU(W1X + b1)ï¼Œå½¢çŠ¶ (2, 4)\n",
    "\n",
    "y = torch.matmul(z_relu, W2) + b2            # W2(ReLU(W1X + b1)) + b2ï¼Œå½¢çŠ¶ (2, 4)\n",
    "\n",
    "loss = y.sum()                               # æ ‡é‡æŸå¤±\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# æ‰“å°å¶å¼ é‡ä¿¡æ¯\n",
    "print(\"X.is_leaf:\", X.is_leaf)               # True\n",
    "print(\"W1.is_leaf:\", W1.is_leaf)             # True\n",
    "print(\"b1.is_leaf:\", b1.is_leaf)             # True\n",
    "print(\"W2.is_leaf:\", W2.is_leaf)             # True\n",
    "print(\"b2.is_leaf:\", b2.is_leaf)             # True\n",
    "print(\"z.is_leaf:\", z.is_leaf)               # False\n",
    "print(\"z_relu.is_leaf:\", z_relu.is_leaf)     # False\n",
    "print(\"y.is_leaf:\", y.is_leaf)               # False\n",
    "print(\"loss.is_leaf:\", loss.is_leaf)         # False\n",
    "\n",
    "# æ‰“å°éå¶å¼ é‡çš„grad_fn\n",
    "print(f\"éå¶å¼ é‡zçš„grad_fn:\\n{z.grad_fn}\")\n",
    "print(f\"éå¶å¼ é‡z_reluçš„grad_fn:\\n{z_relu.grad_fn}\")\n",
    "print(f\"éå¶å¼ é‡yçš„grad_fn:\\n{y.grad_fn}\")\n",
    "\n",
    "# æ‰“å°å¶å¼ é‡çš„grad\n",
    "print(\"X.grad:\", X.grad)                     # æ¢¯åº¦\n",
    "print(\"W1.grad:\", W1.grad)\n",
    "print(\"b1.grad:\", b1.grad)\n",
    "print(\"W2.grad:\", W2.grad)\n",
    "print(\"b2.grad:\", b2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05348ce0-2dc0-4d73-b42c-3659449964bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ„å»ºå¹¶è®­ç»ƒç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d483e79-35de-4f3f-a847-9423c1ad1d7e",
   "metadata": {},
   "source": [
    "### æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºæœ¬æ­¥éª¤\n",
    "\n",
    "**æ„å»ºç¥ç»ç½‘ç»œçš„åŸºæœ¬æ­¥éª¤**\n",
    "- å‡†å¤‡æ•°æ®ï¼šä½¿ç”¨torch.utils.dataåŠ è½½å’Œé¢„å¤„ç†æ•°æ®\n",
    "- å®šä¹‰ç½‘ç»œç»“æ„ï¼šç»§æ‰¿ nn.Module ç±»ï¼Œå®šä¹‰ç½‘ç»œçš„å±‚å’Œå‰å‘ä¼ æ’­é€»è¾‘\n",
    "- åˆå§‹åŒ–ç½‘ç»œï¼šè®¾ç½®å±‚çš„å‚æ•°ï¼ˆå¦‚è¾“å…¥/è¾“å‡ºç»´åº¦ï¼‰å’Œæ¿€æ´»å‡½æ•°\n",
    "- é€‰æ‹©æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼šå¦‚äº¤å‰ç†µæŸå¤±å’ŒSGD ä¼˜åŒ–å™¨\n",
    "- è®­ç»ƒç½‘ç»œï¼šé€šè¿‡å‰å‘ä¼ æ’­ã€è®¡ç®—æŸå¤±ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°å®Œæˆè®­ç»ƒ\n",
    "- è¯„ä¼°æ¨¡å‹ï¼šåœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹æ€§èƒ½\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089efda6-d079-425a-b3f6-b8c9e46be771",
   "metadata": {},
   "source": [
    "#### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰ç¥ç»ç½‘ç»œç»“æ„\n",
    "- ç›®æ ‡ï¼šè®¾è®¡ç¥ç»ç½‘ç»œçš„æ¶æ„ï¼ŒæŒ‡å®šå±‚ã€æ¿€æ´»å‡½æ•°å’Œå…¶ä»–ç»„ä»¶ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.nn.Moduleï¼šæ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç±»ï¼Œç”¨äºå®šä¹‰è‡ªå®šä¹‰ç½‘ç»œã€‚\n",
    "  - torch.nnï¼šæä¾›æ„å»ºå—ï¼Œå¦‚å…¨è¿æ¥å±‚ (nn.Linear)ã€å·ç§¯å±‚ (nn.Conv2d)ã€æ¿€æ´»å‡½æ•° (nn.ReLU)ã€æ± åŒ–å±‚ (nn.MaxPool2d) ç­‰ã€‚\n",
    "  - torch.nn.Sequentialï¼šç”¨äºå¿«é€Ÿæ„å»ºæŒ‰é¡ºåºè¿æ¥çš„å±‚ã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - ç»§æ‰¿ nn.Moduleï¼Œåœ¨ __init__ ä¸­å®šä¹‰å±‚ï¼Œåœ¨ forward æ–¹æ³•ä¸­å®ç°å‰å‘ä¼ æ’­é€»è¾‘ã€‚\n",
    "  - å°†å±‚æ³¨å†Œä¸ºç±»å±æ€§ä»¥è‡ªåŠ¨è·Ÿè¸ªå‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda3a19-85e0-43ea-87c9-da81d1885c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # è¾“å…¥ 784ï¼Œè¾“å‡º 128\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)   # è¾“å‡º 10 ä¸ªç±»åˆ«\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # å±•å¹³è¾“å…¥\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f2142-ddd5-4ddc-9f3b-8826b4433f42",
   "metadata": {},
   "source": [
    "#### ç¬¬äºŒæ­¥ï¼šå‡†å¤‡æ•°æ®\n",
    "- ç›®æ ‡ï¼šåŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼Œç»„ç»‡ä¸ºé€‚åˆè®­ç»ƒçš„æ ¼å¼ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.utils.data.Datasetï¼šå®šä¹‰æ•°æ®é›†çš„æ¥å£ï¼Œç”¨äºåŠ è½½æ•°æ®ã€‚\n",
    "  - torch.utils.data.DataLoaderï¼šæ‰¹é‡åŠ è½½æ•°æ®ï¼Œæ”¯æŒæ‰“ä¹±å’Œå¤šçº¿ç¨‹ã€‚\n",
    "  - torchvision.datasetsï¼šæä¾›å¸¸ç”¨æ•°æ®é›†ï¼ˆå¦‚ MNISTã€CIFAR-10ï¼‰ã€‚\n",
    "  - torchvision.transformsï¼šæä¾›æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºå·¥å…·ã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - ä½¿ç”¨ Dataset æˆ–ç°æˆæ•°æ®é›†åŠ è½½æ•°æ®ã€‚\n",
    "  - åº”ç”¨å˜æ¢ï¼ˆå¦‚å½’ä¸€åŒ–ã€æ•°æ®å¢å¼ºï¼‰ã€‚\n",
    "  - ä½¿ç”¨DataLoaderåˆ›å»ºæ‰¹é‡è¿­ä»£å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1d4e3-7069-404f-bacd-b4f419618e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bd6dd-e5fa-4ff5-9d9b-d218c30d0000",
   "metadata": {},
   "source": [
    "#### ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "- ç›®æ ‡ï¼šè®¾ç½®æ¨¡å‹å®ä¾‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.nnï¼šæä¾›æŸå¤±å‡½æ•°ï¼Œå¦‚ nn.CrossEntropyLossï¼ˆåˆ†ç±»ï¼‰ã€nn.MSELossï¼ˆå›å½’ï¼‰ã€‚\n",
    "  - torch.optimï¼šæä¾›ä¼˜åŒ–å™¨ï¼Œå¦‚ optim.SGDã€optim.Adamã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - å®ä¾‹åŒ–ç½‘ç»œå¹¶ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆCPU/GPUï¼‰ã€‚\n",
    "  - é€‰æ‹©é€‚åˆä»»åŠ¡çš„æŸå¤±å‡½æ•°ã€‚\n",
    "  - é…ç½®ä¼˜åŒ–å™¨ï¼Œä¼ å…¥æ¨¡å‹å‚æ•°å’Œè¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10633b86-2553-4837-9ba8-ef41ef776886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b9d01-5f36-4ed3-b014-c281cf1248ea",
   "metadata": {},
   "source": [
    "#### ç¬¬å››æ­¥ï¼šè®­ç»ƒæ¨¡å‹\n",
    "- ç›®æ ‡ï¼šé€šè¿‡å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°è®­ç»ƒæ¨¡å‹ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.nn.Moduleï¼šé€šè¿‡ model(input) æ‰§è¡Œå‰å‘ä¼ æ’­ã€‚\n",
    "  - torch.autogradï¼šè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆéšå¼ä½¿ç”¨ï¼‰ã€‚\n",
    "  - torch.optimï¼šä¼˜åŒ–å™¨æ›´æ–°å‚æ•°ã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - éå†æ•°æ®é›†ï¼Œæ‰§è¡Œä»¥ä¸‹å¾ªç¯ï¼š\n",
    "    1. æ¸…é›¶æ¢¯åº¦ï¼ˆoptimizer.zero_grad()ï¼‰ã€‚\n",
    "    2. å‰å‘ä¼ æ’­ï¼Œè®¡ç®—è¾“å‡ºã€‚\n",
    "    3. è®¡ç®—æŸå¤±ï¼ˆcriterion(output, target)ï¼‰ã€‚\n",
    "    4. åå‘ä¼ æ’­ï¼ˆloss.backward()ï¼‰ã€‚\n",
    "    5. æ›´æ–°å‚æ•°ï¼ˆoptimizer.step()ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9cfc6-bb08-4642-bd80-c38281dad300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5187b-9dc0-4074-9bef-696b96c916f3",
   "metadata": {},
   "source": [
    "#### ç¬¬äº”æ­¥ï¼šè¯„ä¼°æ¨¡å‹\n",
    "- ç›®æ ‡ï¼šåœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹æ€§èƒ½ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.nn.Moduleï¼šä½¿ç”¨ model.eval() åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ã€‚\n",
    "  - torch.no_gradï¼šç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - åŠ è½½æµ‹è¯•æ•°æ®ã€‚\n",
    "  - è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆmodel.eval()ï¼‰ã€‚\n",
    "  - ä½¿ç”¨ torch.no_grad() è®¡ç®—é¢„æµ‹å¹¶è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023a61f-b33e-42f3-b025-a9b50248f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114de64b-9a0c-4b57-a869-cd32dd187147",
   "metadata": {},
   "source": [
    "#### ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹\n",
    "- ç›®æ ‡ï¼šä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°æˆ–æ•´ä¸ªæ¨¡å‹ï¼Œå¹¶åœ¨éœ€è¦æ—¶åŠ è½½ã€‚\n",
    "- ä¸»è¦æ¨¡å—ï¼š\n",
    "  - torch.nn.Moduleï¼šæä¾› state_dict() å’Œ load_state_dict() æ–¹æ³•ã€‚\n",
    "  - torch.save å’Œ torch.loadï¼šç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ–ã€‚\n",
    "- æ­¥éª¤ï¼š\n",
    "  - ä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰æˆ–æ•´ä¸ªæ¨¡å‹ã€‚\n",
    "  - åŠ è½½å‚æ•°å¹¶æ¢å¤æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a987d-db47-40fa-ae89-45f1c03c51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40601096-70c2-4a09-8cbe-a79c31a6ea1f",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ¨¡å‹çš„ç®€å•ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d9b50-6179-4bb4-9df2-af8e8ef364d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "model = SimpleNet().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c4331-a11e-45b4-bfc2-b18efb22e881",
   "metadata": {},
   "source": [
    "### å®Œæ•´çš„ç¤ºä¾‹\n",
    "ä¸åŒ…å«æ¨¡å‹çš„ä¿å­˜å’ŒåŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3e8dd-2623-4c25-a686-03af6922e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# å®šä¹‰ç¥ç»ç½‘ç»œ\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥å±‚åˆ°éšè—å±‚ï¼ˆ784 -> 128ï¼‰\n",
    "        self.relu = nn.ReLU()               # æ¿€æ´»å‡½æ•°\n",
    "        self.fc2 = nn.Linear(128, 64)       # éšè—å±‚ï¼ˆ128 -> 64ï¼‰\n",
    "        self.fc3 = nn.Linear(64, 10)        # è¾“å‡ºå±‚ï¼ˆ64 -> 10ï¼Œ10ä¸ªç±»åˆ«ï¼‰\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # å±•å¹³è¾“å…¥ï¼ˆbatch_size, 1, 28, 28ï¼‰-> (batch_size, 784)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)          # è¾“å‡º logits\n",
    "        return x\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "transform = transforms.ToTensor()\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# åˆå§‹åŒ–ç½‘ç»œã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# è®­ç»ƒç½‘ç»œ\n",
    "for epoch in range(5):  # è®­ç»ƒ 5 ä¸ª epoch\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()          # æ¸…é›¶æ¢¯åº¦\n",
    "        outputs = model(images)        # å‰å‘ä¼ æ’­\n",
    "        loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±\n",
    "        loss.backward()               # åå‘ä¼ æ’­\n",
    "        optimizer.step()              # æ›´æ–°å‚æ•°\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}')\n",
    "\n",
    "print('è®­ç»ƒå®Œæˆï¼')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c06dc0-8193-4e79-9dcd-119519c2a801",
   "metadata": {},
   "source": [
    "### PyTorchæ¨¡å‹ç»„åˆç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e320e-ef8d-4fbd-bd3a-ff88e624a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0443c-e2eb-4cd7-9369-9e76802a1d62",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨nn.Sequentialæ„å»ºé¡ºåºç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b07c1-4ed3-45e7-8f4f-9d16df5d5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# æµ‹è¯•\n",
    "x = torch.randn(4, 784)\n",
    "seq_model = SequentialModel()\n",
    "print(\"Sequential output shape:\", seq_model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdbcde4-fc8d-4dd2-94c9-a81279feae71",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ nn.ModuleList æ„å»ºé‡å¤ç»“æ„ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06b2e9-eec7-40fa-8695-3440b1c298e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleListModel(nn.Module):\n",
    "    def __init__(self, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(784, 256)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(256, 256) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# æµ‹è¯•\n",
    "x = torch.randn(4, 784)\n",
    "list_model = ModuleListModel()\n",
    "print(\"ModuleList output shape:\", list_model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a4b0a-0178-4769-a899-8cab5f295bf5",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ nn.ModuleDict æ„å»ºåˆ†æ”¯ç»“æ„ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992229f8-cd20-4244-b46a-de4ec412ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleDictModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Linear(784, 256)\n",
    "        self.heads = nn.ModuleDict({\n",
    "            'classification': nn.Linear(256, 10),\n",
    "            'regression': nn.Linear(256, 1)\n",
    "        })\n",
    "    \n",
    "    def forward(self, x, mode='classification'):\n",
    "        x = F.relu(self.shared(x))\n",
    "        return self.heads[mode](x)\n",
    "\n",
    "# æµ‹è¯•\n",
    "x = torch.randn(4, 784)\n",
    "dict_model = ModuleDictModel()\n",
    "print(\"ModuleDict classification head:\", dict_model(x, mode='classification').shape)\n",
    "print(\"ModuleDict regression head:\", dict_model(x, mode='regression').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dec1a5-993e-4e56-9f3d-149f37354ecd",
   "metadata": {},
   "source": [
    "### æ¿€æ´»å‡½æ•°ç¤ºä¾‹\n",
    "\n",
    "**ä¸»æµæ¿€æ´»å‡½æ•°çš„ç‰¹æ€§å¯¹æ¯”**\n",
    "\n",
    "| ç‰¹æ€§/å‡½æ•°        | Softmax                                    | Sigmoid                                  | Tanh                                     | ReLU                                   | GELU                                           | Swish                                          |\n",
    "| :--------------- | :----------------------------------------- | :--------------------------------------- | :--------------------------------------- | :------------------------------------- | :--------------------------------------------- | :--------------------------------------------- |\n",
    "| **æ•°å­¦è¡¨è¾¾å¼** | $P_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$ | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$       | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $f(x) = \\max(0, x)$                    | $f(x) = x \\cdot P(X \\le x)$ (æ ‡å‡†æ­£æ€CDF)       | $f(x) = x \\cdot \\sigma(\\beta x)$               |\n",
    "| **è¾“å‡ºèŒƒå›´** | $(0, 1)$ (æ‰€æœ‰è¾“å‡ºå’Œä¸º 1)                  | $(0, 1)$                                 | $(-1, 1)$                                | $[0, +\\infty)$                         | $(-\\infty, +\\infty)$                           | $(-\\infty, +\\infty)$                           |\n",
    "| **é›¶ä¸­å¿ƒè¾“å‡º** | å¦ï¼ˆå•ç‹¬çœ‹æ¯ä¸ªè¾“å‡ºï¼‰                       | å¦                                       | **æ˜¯** | å¦                                     | **æ˜¯** (è¾“å‡ºå‡å€¼è¶‹è¿‘äº 0)                  | **æ˜¯** (è¾“å‡ºå‡å€¼è¶‹è¿‘äº 0)                  |\n",
    "| **å¹³æ»‘æ€§** | å¹³æ»‘                                       | å¹³æ»‘                                     | å¹³æ»‘                                     | ä¸å¹³æ»‘ (åœ¨ $x=0$ å¤„å°–é”)             | **å¹³æ»‘** | **å¹³æ»‘** |\n",
    "| **å•è°ƒæ€§** | æ— æ³•ç›´æ¥åˆ¤æ–­ (æ˜¯æ•´ä¸ªå‘é‡çš„è½¬æ¢)              | **å•è°ƒé€’å¢** | **å•è°ƒé€’å¢** | **å•è°ƒé€’å¢** | **éå•è°ƒ** | **éå•è°ƒ** |\n",
    "| **æ˜¯å¦å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±** | å¦ï¼ˆé€šå¸¸ä¸é¥±å’Œï¼‰ï¼Œä½†æœ‰æ•°å€¼ç¨³å®šæ€§é—®é¢˜       | **æ˜¯** (ä¸¤ç«¯é¥±å’ŒåŒº)                      | **æ˜¯** (ä¸¤ç«¯é¥±å’ŒåŒº)                      | **å¦** (æ­£å€¼åŒº)ï¼Œä½†æœ‰â€œæ­»äº¡ ReLUâ€ | **å¦** (æ­£å€¼åŒºï¼Œå¹³æ»‘è¿‡æ¸¡)ï¼Œä½†è®¡ç®—å¤æ‚ | **å¦** (æ­£è´Ÿå€¼åŒºéƒ½æœ‰æ¢¯åº¦)ï¼Œä½†è®¡ç®—å¤æ‚ |\n",
    "| **â€œæ­»äº¡ç¥ç»å…ƒâ€é—®é¢˜** | ä¸é€‚ç”¨                                     | ä¸é€‚ç”¨                                   | ä¸é€‚ç”¨                                   | **æ˜¯** (è´Ÿå€¼åŒºæ¢¯åº¦ä¸º 0)                | å¦                                             | å¦                                             |\n",
    "| **è®¡ç®—æˆæœ¬** | é«˜ (æ¶‰åŠæŒ‡æ•°å’Œæ±‚å’Œ)                        | é«˜ (æ¶‰åŠæŒ‡æ•°)                            | é«˜ (æ¶‰åŠæŒ‡æ•°)                            | **æœ€ä½** (æ¯”è¾ƒæ“ä½œ)                    | è¾ƒé«˜ (æ¶‰åŠ $\\tanh$ å’Œå¤šé¡¹å¼)                  | è¾ƒé«˜ (æ¶‰åŠ Sigmoid å’Œä¹˜æ³•)                     |\n",
    "| **å…¸å‹åº”ç”¨åœºæ™¯** | **å¤šåˆ†ç±»é—®é¢˜**çš„**è¾“å‡ºå±‚**ï¼Œè¾“å‡ºç±»åˆ«æ¦‚ç‡   | **äºŒåˆ†ç±»é—®é¢˜**çš„**è¾“å‡ºå±‚**ï¼Œå¤šæ ‡ç­¾åˆ†ç±» | æ—©æœŸç¥ç»ç½‘ç»œçš„**éšè—å±‚**ï¼ŒRNNs             | **å¤§å¤šæ•°éšè—å±‚**ï¼ŒCNNs                 | **Transformer æ¶æ„** (BERT, GPT), å¤æ‚æ¨¡å‹  | **æ·±å±‚ç½‘ç»œ**ï¼Œè§†è§‰ä»»åŠ¡ (å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹)    |\n",
    "| **ä¸»è¦ä¼˜ç‚¹** | å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œé€‚ç”¨äºäº’æ–¥å¤šåˆ†ç±»       | è¾“å‡ºæ¦‚ç‡åŒ–ï¼Œå¯å¾®                           | è¾“å‡ºé›¶ä¸­å¿ƒï¼Œæ”¶æ•›å¿«äº Sigmoid           | ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—é«˜æ•ˆï¼Œå¼•å…¥ç¨€ç–æ€§     | æ€§èƒ½ä¼˜è¶Šï¼Œå¹³æ»‘ï¼Œè‡ªé€‚åº”é—¨æ§ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±       | æ€§èƒ½ä¼˜è¶Šï¼Œå¹³æ»‘ï¼Œéå•è°ƒï¼Œè‡ªé—¨æ§ï¼Œæ— æ­»äº¡ ReLU |\n",
    "| **ä¸»è¦ç¼ºç‚¹** | æ•°å€¼ç¨³å®šæ€§ï¼Œå¯èƒ½å¯¼è‡´æ¢¯åº¦æ¥è¿‘ 0               | æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œéé›¶ä¸­å¿ƒè¾“å‡º       | æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—æˆæœ¬é«˜                     | â€œæ­»äº¡ ReLUâ€ï¼Œéé›¶ä¸­å¿ƒè¾“å‡º              | è®¡ç®—æˆæœ¬é«˜ï¼Œæ¦‚å¿µå¤æ‚                           | è®¡ç®—æˆæœ¬é«˜                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6da11-6244-4203-9253-892759a9c663",
   "metadata": {},
   "source": [
    "#### ç¤ºä¾‹1ï¼šä½¿ç”¨ä¸å¸¦æ¿€æ´»å‡½æ•°çš„æ¨¡å‹æ‹Ÿåˆçº¯æ€§æ•°æ®ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3e158-df4a-40f3-9fa7-2b492a6bc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "# ========== Step 1: æ„é€ æ•°æ® ==========\n",
    "np.random.seed(0)\n",
    "x_np = np.linspace(-3, 3, 200)\n",
    "noise = np.random.normal(0, 0.3, x_np.shape)\n",
    "y_np = 2 * x_np + 1 + noise\n",
    "\n",
    "X_tensor = torch.tensor(x_np, dtype=torch.float32).view(-1, 1)\n",
    "Y_tensor = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ========== Step 2: å®šä¹‰æ¨¡å‹ ==========\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # y = wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "# ========== Step 3: å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ==========\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# ========== Step 4: è®­ç»ƒå¹¶è®°å½•æ¯éš”å‡ æ­¥çš„é¢„æµ‹ ==========\n",
    "epochs = 200\n",
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = criterion(output, Y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # æ¯ 5 è½®è®°å½•ä¸€æ¬¡å½“å‰é¢„æµ‹å€¼\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_tensor).detach()\n",
    "            history.append(pred_y.numpy())\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åŠ¨ç”»...\")\n",
    "\n",
    "# ========== Step 5: åŠ¨æ€å¯è§†åŒ–æ‹Ÿåˆè¿‡ç¨‹ ==========\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(x_np, y_np, c='blue', label=\"Data\")\n",
    "line, = ax.plot([], [], 'r-', linewidth=2, label=\"Prediction\")\n",
    "text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(min(y_np)-1, max(y_np)+1)\n",
    "ax.legend()\n",
    "ax.set_title(\"Linear Regression Fitting Animation\")\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    text.set_text('')\n",
    "    return line, text\n",
    "\n",
    "def animate(i):\n",
    "    y_pred = history[i].flatten()\n",
    "    line.set_data(x_np, y_pred)\n",
    "    loss_val = np.mean((y_np - y_pred)**2)\n",
    "    text.set_text(f\"Epoch: {i*5:>3}, Loss: {loss_val:.4f}\")\n",
    "    return line, text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                              frames=len(history), interval=150, blit=True)\n",
    "\n",
    "plt.close()  # é˜²æ­¢é‡å¤æ˜¾ç¤º\n",
    "ani\n",
    "\n",
    "# æ˜¾ç¤ºåŠ¨ç”»ï¼ˆåœ¨Jupyterä¸­ï¼‰\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6993267-72ec-4b3e-b82e-de9df7982114",
   "metadata": {},
   "source": [
    "#### ç¤ºä¾‹2ï¼šä½¿ç”¨ä¸å¸¦æ¿€æ´»å‡½æ•°çš„æ¨¡å‹æ‹Ÿåˆéçº¿æ€§æ•°æ®ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bfebf-2142-4d83-9fd2-7a1880f4880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "# ========== Step 1: ç”Ÿæˆéçº¿æ€§æ•°æ® ==========\n",
    "np.random.seed(0)\n",
    "x_np = np.linspace(-3, 3, 200)\n",
    "noise = np.random.normal(0, 0.5, x_np.shape)\n",
    "#y_np = x_np**2 + noise  # éçº¿æ€§å…³ç³»ç¤ºä¾‹1ï¼šå¹‚å‡½æ•°\n",
    "y_np = np.sin(x_np * 2) + noise  # éçº¿æ€§å…³ç³»ç¤ºä¾‹2ï¼šä¸‰è§’å‡½æ•°\n",
    "\n",
    "X_tensor = torch.tensor(x_np, dtype=torch.float32).view(-1, 1)\n",
    "Y_tensor = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ========== Step 2: å®šä¹‰çº¿æ€§æ¨¡å‹ ==========\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # çº¿æ€§æ¨¡å‹ y = wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# ========== Step 3: æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ==========\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# ========== Step 4: è®­ç»ƒå¹¶è®°å½•è¿‡ç¨‹ ==========\n",
    "epochs = 200\n",
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = criterion(output, Y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # æ¯ 5 epoch è®°å½•ä¸€æ¬¡\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_tensor).detach()\n",
    "            history.append(pred_y.numpy())\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åŠ¨ç”»...\")\n",
    "\n",
    "# ========== Step 5: Matplotlib åŠ¨ç”»å¯è§†åŒ– ==========\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(x_np, y_np, c='blue', label=\"Nonlinear Data\")\n",
    "line, = ax.plot([], [], 'r-', linewidth=2, label=\"Linear Fit\")\n",
    "text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(min(y_np) - 1, max(y_np) + 1)\n",
    "ax.legend()\n",
    "ax.set_title(\"Linear Model Fitting Nonlinear Data\")\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    text.set_text('')\n",
    "    return line, text\n",
    "\n",
    "def animate(i):\n",
    "    y_pred = history[i].flatten()\n",
    "    line.set_data(x_np, y_pred)\n",
    "    loss_val = np.mean((y_np - y_pred)**2)\n",
    "    text.set_text(f\"Epoch: {i*5:>3}, Loss: {loss_val:.4f}\")\n",
    "    return line, text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                              frames=len(history), interval=150, blit=True)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# æ˜¾ç¤ºåŠ¨ç”»ï¼ˆåœ¨Jupyterä¸­ï¼‰\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f3b9e-5634-4ecf-ae5d-ae4d5b958c5b",
   "metadata": {},
   "source": [
    "#### ç¤ºä¾‹3ï¼šåœ¨æ¨¡å‹ä¸Šæ·»åŠ ReLUæ¿€æ´»å‡½æ•°å¹¶æ‹Ÿåˆéçº¿æ€§æ•°æ®ç‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8db355-9ffb-4c3e-84eb-f498347cf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "\n",
    "# ========== Step 1: ç”Ÿæˆéçº¿æ€§æ•°æ® ==========\n",
    "np.random.seed(0)\n",
    "x_np = np.linspace(-3, 3, 200)\n",
    "noise = np.random.normal(0, 0.5, x_np.shape)\n",
    "#y_np = x_np**2 + noise  # éçº¿æ€§å…³ç³»ç¤ºä¾‹1ï¼šå¹‚å‡½æ•°\n",
    "y_np = np.sin(x_np * 2) + noise  # éçº¿æ€§å…³ç³»ç¤ºä¾‹2ï¼šä¸‰è§’å‡½æ•°\n",
    "\n",
    "X_tensor = torch.tensor(x_np, dtype=torch.float32).view(-1, 1)\n",
    "Y_tensor = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ========== Step 2: å®šä¹‰éçº¿æ€§æ¨¡å‹ï¼ˆå¸¦æ¿€æ´»å‡½æ•°ï¼‰ ==========\n",
    "class NonlinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = NonlinearModel()\n",
    "\n",
    "# ========== Step 3: æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ ==========\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# ========== Step 4: è®­ç»ƒå¹¶è®°å½•è¿‡ç¨‹ ==========\n",
    "epochs = 200\n",
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_tensor)\n",
    "    loss = criterion(output, Y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(X_tensor).detach()\n",
    "            history.append(pred_y.numpy())\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åŠ¨ç”»...\")\n",
    "\n",
    "# ========== Step 5: åŠ¨æ€å¯è§†åŒ–æ‹Ÿåˆè¿‡ç¨‹ ==========\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(x_np, y_np, c='skyblue', label=\"Nonlinear Data\")\n",
    "line, = ax.plot([], [], 'r-', linewidth=2, label=\"Model Fit\")\n",
    "text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(min(y_np) - 1, max(y_np) + 1)\n",
    "ax.legend()\n",
    "ax.set_title(\"MLP with ReLU Fitting Nonlinear Data\")\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    text.set_text('')\n",
    "    return line, text\n",
    "\n",
    "def animate(i):\n",
    "    y_pred = history[i].flatten()\n",
    "    line.set_data(x_np, y_pred)\n",
    "    loss_val = np.mean((y_np - y_pred)**2)\n",
    "    text.set_text(f\"Epoch: {i*5:>3}, Loss: {loss_val:.4f}\")\n",
    "    return line, text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                              frames=len(history), interval=150, blit=True)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# æ˜¾ç¤ºåŠ¨ç”»ï¼ˆåœ¨Jupyterä¸­ï¼‰\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90606145-7c0c-4a50-a6cd-d4028b59a245",
   "metadata": {},
   "source": [
    "### torch.nnçš„å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4461f-3819-4a6f-a13e-63d6844b1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "dir(nn)  # è¾“å‡º nn æ¨¡å—ä¸‹æ‰€æœ‰ç±»å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca62110-6cfb-403f-bcf0-01303983a0ce",
   "metadata": {},
   "source": [
    "### æŸå¤±å‡½æ•°ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619b409-3fd3-4dc7-87d9-0e1135f7a81f",
   "metadata": {},
   "source": [
    "#### æŸå¤±å‡½æ•°åŸºç¡€åº”ç”¨ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb6630-87c8-43a6-91a8-dfd62df353f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# 2. å‡†å¤‡æ•°æ®ï¼Œæ‰‹åŠ¨æä¾›inputï¼ˆé¢„æµ‹å€¼ï¼‰\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "target = torch.tensor([[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]])\n",
    "\n",
    "# 3. è®¡ç®—æŸå¤±\n",
    "loss = criterion(input, target)\n",
    "\n",
    "# 4. åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# 5. è¾“å‡ºæŸå¤±\n",
    "print(f\"Loss (mean): {loss.item()}\")\n",
    "\n",
    "# å¯é€‰ï¼šé€å…ƒç´ æŸå¤±\n",
    "criterion_none = nn.MSELoss(reduction='none')  # ä¸è¿›è¡Œèšåˆï¼Œè¾“å‡ºåˆ™ä¸ºä¸input/targetåŒå½¢çŠ¶çš„å¼ é‡\n",
    "loss_none = criterion_none(input, target)\n",
    "print(f\"Element-wise loss:\\n{loss_none}\")\n",
    "\n",
    "# éªŒè¯æ±‚å’ŒæŸå¤±ï¼ˆreduction='sum'ï¼‰\n",
    "criterion_sum = nn.L1Loss(reduction='sum')\n",
    "loss_sum = criterion_sum(input, target)\n",
    "print(f\"Sum loss: {loss_sum.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fd174-7941-41bb-ac21-3c84aaa7cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.L1Loss(reduction='mean')  # é»˜è®¤å‡å€¼\n",
    "\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "target = torch.tensor([[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]])\n",
    "\n",
    "# è®¡ç®—æŸå¤±\n",
    "loss = criterion(input, target)\n",
    "\n",
    "# è¾“å‡ºæŸå¤±\n",
    "print(f\"Loss (mean): {loss.item()}\")  # å¹³å‡ç»å¯¹è¯¯å·®\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# éªŒè¯é€å…ƒç´ æŸå¤±ï¼ˆreduction='none'ï¼‰\n",
    "criterion_none = nn.L1Loss(reduction='none')\n",
    "loss_none = criterion_none(input, target)\n",
    "print(f\"Element-wise loss:\\n{loss_none}\")\n",
    "\n",
    "# éªŒè¯æ±‚å’ŒæŸå¤±ï¼ˆreduction='sum'ï¼‰\n",
    "criterion_sum = nn.L1Loss(reduction='sum')\n",
    "loss_sum = criterion_sum(input, target)\n",
    "print(f\"Sum loss: {loss_sum.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af5353-93e6-4916-b3ac-ae6589bb6cc1",
   "metadata": {},
   "source": [
    "#### HuberæŸå¤±ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311ca14-b976-4174-a3f7-fc724426a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.SmoothL1Loss(reduction='mean', beta=1.0)\n",
    "\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "target = torch.tensor([[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]])\n",
    "\n",
    "# è®¡ç®—æŸå¤±\n",
    "loss = criterion(input, target)\n",
    "\n",
    "# è¾“å‡ºæŸå¤±\n",
    "print(f\"Loss (mean): {loss.item()}\")  # å¹³æ»‘ L1 æŸå¤±\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()\n",
    "\n",
    "# éªŒè¯é€å…ƒç´ æŸå¤±ï¼ˆreduction='none'ï¼‰\n",
    "criterion_none = nn.SmoothL1Loss(reduction='none', beta=1.0)\n",
    "loss_none = criterion_none(input, target)\n",
    "print(f\"Element-wise loss:\\n{loss_none}\")\n",
    "\n",
    "# éªŒè¯æ±‚å’ŒæŸå¤±ï¼ˆreduction='sum'ï¼‰\n",
    "criterion_sum = nn.SmoothL1Loss(reduction='sum', beta=1.0)\n",
    "loss_sum = criterion_sum(input, target)\n",
    "print(f\"Sum loss: {loss_sum.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e662e-2891-4860-bf5c-edf0aeb98311",
   "metadata": {},
   "source": [
    "### åˆ†ç±»æŸå¤±å‡½æ•°\n",
    "\n",
    "| **æŸå¤±å‡½æ•°**                | **è¾“å…¥è¦æ±‚**                                                                 | **ç›®æ ‡è¦æ±‚**                          | **ä¸»è¦åŠŸèƒ½**                              | **é€‚ç”¨åœºæ™¯**                                                                 | **ä¼˜ç‚¹**                                                                 | **å±€é™**                                                                 | **PyTorch å®ç°**                     |\n",
    "|-----------------------------|-----------------------------------------------------------------------------|---------------------------------------|------------------------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------------------------|\n",
    "| **CrossEntropyLoss**        | Logitsï¼ˆæœªç»è¿‡ softmaxï¼‰ï¼Œå½¢çŠ¶ \\([N, C, ...]\\)                               | ç±»åˆ«ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰ï¼Œå½¢çŠ¶ \\([N, ...]\\)    | ç»“åˆ log-softmax å’Œ NLLï¼Œè®¡ç®—å¤šåˆ†ç±»äº¤å‰ç†µ | å•æ ‡ç­¾å¤šåˆ†ç±»ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ï¼‰                                         | æ•°å€¼ç¨³å®šï¼Œé€‚åˆæ ‡å‡†å¤šåˆ†ç±»ä»»åŠ¡                                              | ä¸æ”¯æŒå¤šæ ‡ç­¾æˆ–è½¯æ ‡ç­¾                                                     | `torch.nn.CrossEntropyLoss`          |\n",
    "| **BCEWithLogitsLoss**       | Logitsï¼ˆæœªç»è¿‡ Sigmoidï¼‰ï¼Œå½¢çŠ¶ä»»æ„                                           | äºŒå€¼æ ‡ç­¾ï¼ˆ0/1 æˆ–æ¦‚ç‡ï¼‰ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ | ç»“åˆ Sigmoid å’ŒäºŒå…ƒäº¤å‰ç†µï¼Œè®¡ç®—äºŒåˆ†ç±»æŸå¤± | å•æ ‡ç­¾/å¤šæ ‡ç­¾äºŒåˆ†ç±»ã€å›¾åƒåˆ†å‰²ã€GAN åˆ¤åˆ«å™¨                                    | æ•°å€¼ç¨³å®šï¼Œæ”¯æŒå¤šæ ‡ç­¾å’Œä¸å¹³è¡¡æ•°æ®ï¼ˆé€šè¿‡ `pos_weight`ï¼‰                      | ä»…é™äºŒåˆ†ç±»ï¼Œä¸é€‚åˆå¤šåˆ†ç±»                                                 | `torch.nn.BCEWithLogitsLoss`         |\n",
    "| **BCELoss**                 | æ¦‚ç‡ï¼ˆç»è¿‡ Sigmoidï¼Œ[0, 1]ï¼‰ï¼Œå½¢çŠ¶ä»»æ„                                       | äºŒå€¼æ ‡ç­¾ï¼ˆ0/1 æˆ–æ¦‚ç‡ï¼‰ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ | è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±                       | äºŒåˆ†ç±»ã€å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆéœ€æ‰‹åŠ¨ Sigmoidï¼‰                                         | é€‚åˆå·²è®¡ç®—æ¦‚ç‡çš„åœºæ™¯                                                     | æ•°å€¼ç¨³å®šæ€§ç¨å·®ï¼Œéœ€æ‰‹åŠ¨ Sigmoid                                           | `torch.nn.BCELoss`                   |\n",
    "| **NLLLoss**                 | Log æ¦‚ç‡ï¼ˆç»è¿‡ log-softmaxï¼‰ï¼Œå½¢çŠ¶ \\([N, C, ...]\\)                           | ç±»åˆ«ç´¢å¼•ï¼ˆæ•´æ•°ï¼‰ï¼Œå½¢çŠ¶ \\([N, ...]\\)    | è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±                       | å¤šåˆ†ç±»ï¼ˆéœ€æ‰‹åŠ¨ log-softmaxï¼‰ã€è¯­è¨€æ¨¡å‹                                      | é€‚åˆè‡ªå®šä¹‰ log-softmax çš„åœºæ™¯                                             | éœ€è¦æ‰‹åŠ¨ log-softmaxï¼Œä¸å¦‚ CrossEntropyLoss æ–¹ä¾¿                          | `torch.nn.NLLLoss`                   |\n",
    "| **KLDivLoss**               | Log æ¦‚ç‡ï¼ˆé€šå¸¸ log-softmax è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä»»æ„                                  | ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ           | è®¡ç®— KL æ•£åº¦ï¼ˆäº¤å‰ç†µçš„ä¸€éƒ¨åˆ†ï¼‰            | åˆ†å¸ƒåŒ¹é…ã€è½¯æ ‡ç­¾åˆ†ç±»ã€çŸ¥è¯†è’¸é¦ã€VAE                                          | é€‚åˆè½¯æ ‡ç­¾å’Œåˆ†å¸ƒä¼˜åŒ–                                                     | éœ€è¦ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒï¼Œè®¡ç®—å¤æ‚                                               | `torch.nn.KLDivLoss`                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db696072-ad7d-464e-ab73-12c45b29b8d9",
   "metadata": {},
   "source": [
    "#### BCELossç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422972fa-01e5-44fb-bb82-74318b74b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# æ¨¡æ‹Ÿé¢„æµ‹å€¼ï¼ˆéœ€è¦äº‹å…ˆç»è¿‡sigmoidæ¿€æ´»ï¼‰\n",
    "input = torch.tensor([0.9, 0.2, 0.8], requires_grad=True)\n",
    "\n",
    "# çœŸå®æ ‡ç­¾ï¼ˆå¿…é¡»æ˜¯0æˆ–1ï¼‰\n",
    "target = torch.tensor([1.0, 0.0, 1.0])\n",
    "\n",
    "# åˆå§‹åŒ–æŸå¤±å‡½æ•°\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# è®¡ç®—æŸå¤±\n",
    "loss = criterion(input, target)\n",
    "\n",
    "print(\"BCELoss:\", loss.item())\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9241eb-cf7d-46ba-9a7c-bccb3ba15613",
   "metadata": {},
   "source": [
    "#### BCEWithLogitsLossç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d045f2-c2ad-4ca9-8425-facfa6b67472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# logitsï¼ˆæœªç»è¿‡sigmoidï¼‰\n",
    "input = torch.tensor([0.2, -1.0, 3.0], requires_grad=True)\n",
    "\n",
    "# æ ‡ç­¾å€¼\n",
    "target = torch.tensor([1.0, 0.0, 1.0])\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# è®¡ç®—æŸå¤±\n",
    "loss = criterion(input, target)\n",
    "print(\"BCEWithLogitsLoss:\", loss.item())\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf71ec6-885e-4554-922c-53e1348c8767",
   "metadata": {},
   "source": [
    "#### NLLLossç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbc82d-2421-4444-a2e9-e06e56037c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®ä¾‹åŒ– NLLLossï¼Œä½¿ç”¨é»˜è®¤çš„ reduction='mean'\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# ç¤ºä¾‹ 1: ç®€å•å¤šåˆ†ç±»ï¼Œbatch_size=2\n",
    "# æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºçš„åŸå§‹ logits\n",
    "raw_logits = torch.tensor([\n",
    "    [0.1, 2.0, 0.3],  # æ ·æœ¬1\n",
    "    [1.5, 0.2, 0.8]   # æ ·æœ¬2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# å¿…é¡»å…ˆç»è¿‡LogSoftmaxæ¿€æ´»å‡½æ•°\n",
    "log_probabilities = torch.log_softmax(raw_logits, dim=1)\n",
    "\n",
    "# çœŸå®ç±»åˆ«æ ‡ç­¾ (æ•´æ•°ç´¢å¼•)\n",
    "targets = torch.tensor([1, 0], dtype=torch.long) # æ ·æœ¬1çœŸå®ç±»åˆ«æ˜¯1ï¼Œæ ·æœ¬2çœŸå®ç±»åˆ«æ˜¯0\n",
    "\n",
    "loss = nll_loss(log_probabilities, targets)\n",
    "print(f\"NLLLoss (mean): {loss.item()}\") # è¾“å‡º: NLLLoss (mean): 0.2422...\n",
    "\n",
    "# ç¤ºä¾‹ 2: ä½¿ç”¨ weight å‚æ•°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡\n",
    "# å‡è®¾ç±»åˆ«0æƒé‡1.0ï¼Œç±»åˆ«1æƒé‡10.0ï¼ˆå°‘æ•°ç±»ï¼‰ï¼Œç±»åˆ«2æƒé‡1.0\n",
    "class_weights = torch.tensor([1.0, 10.0, 1.0], dtype=torch.float32)\n",
    "nll_loss_weighted = nn.NLLLoss(weight=class_weights)\n",
    "\n",
    "# æŸå¤±è®¡ç®—å°†æ ¹æ®çœŸå®ç±»åˆ«å¯¹åº”çš„æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡\n",
    "loss_weighted = nll_loss_weighted(log_probabilities, targets)\n",
    "print(f\"NLLLoss (with class weights): {loss_weighted.item()}\") # è¾“å‡º: NLLLoss (with class weights): 1.0675..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668488c-d0eb-4df5-9a9c-636e1e2cdb91",
   "metadata": {},
   "source": [
    "#### CrossEntropyLossç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68837d3e-a4d5-4b9b-8a5c-098bb1e2154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®ä¾‹åŒ– CrossEntropyLossï¼Œä½¿ç”¨é»˜è®¤çš„ reduction='mean'\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# ç¤ºä¾‹ 1: ç®€å•å¤šåˆ†ç±»ï¼Œbatch_size=2ï¼Œ3ä¸ªç±»åˆ«\n",
    "# input æ˜¯æ¨¡å‹è¾“å‡ºçš„åŸå§‹ logits (ä¸éœ€è¦Softmax)\n",
    "# å½¢çŠ¶: (batch_size, num_classes)\n",
    "predictions_logits = torch.tensor([\n",
    "    [0.1, 2.0, 0.3],  # æ ·æœ¬1\n",
    "    [1.5, 0.2, 0.8]   # æ ·æœ¬2\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# çœŸå®ç±»åˆ«æ ‡ç­¾ï¼Œå½¢çŠ¶: (batch_size,)ï¼Œæ˜¯æ•´æ•°ç´¢å¼•\n",
    "targets = torch.tensor([1, 0], dtype=torch.long) # æ ·æœ¬1çœŸå®ç±»åˆ«æ˜¯1ï¼Œæ ·æœ¬2çœŸå®ç±»åˆ«æ˜¯0\n",
    "\n",
    "loss = cross_entropy_loss(predictions_logits, targets)\n",
    "print(f\"CrossEntropyLoss (mean): {loss.item()}\") # è¾“å‡º: CrossEntropyLoss (mean): 0.2422...\n",
    "\n",
    "# ç¤ºä¾‹ 2: ä½¿ç”¨ weight å‚æ•°å¤„ç†ç±»åˆ«ä¸å¹³è¡¡\n",
    "# å‡è®¾ç±»åˆ«0æƒé‡1.0ï¼Œç±»åˆ«1æƒé‡10.0ï¼ˆå°‘æ•°ç±»ï¼‰ï¼Œç±»åˆ«2æƒé‡1.0\n",
    "class_weights = torch.tensor([1.0, 10.0, 1.0], dtype=torch.float32)\n",
    "cross_entropy_loss_weighted = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "loss_weighted = cross_entropy_loss_weighted(predictions_logits, targets)\n",
    "print(f\"CrossEntropyLoss (with class weights): {loss_weighted.item()}\") # è¾“å‡º: CrossEntropyLoss (with class weights)\n",
    "\n",
    "# ç¤ºä¾‹ 3: å¸¦æœ‰ignore_indexçš„å¤šç»´è¾“å…¥ (ä¾‹å¦‚åœ¨è¯­ä¹‰åˆ†å‰²ä¸­)\n",
    "# å‡è®¾å›¾åƒåˆ†å‰²çš„logitså½¢çŠ¶ä¸º (batch_size, num_classes, H, W)\n",
    "# target å½¢çŠ¶ä¸º (batch_size, H, W)ï¼ŒåŒ…å«åƒç´ çº§åˆ«çš„ç±»åˆ«ç´¢å¼•\n",
    "predictions_segmentation_logits = torch.randn(2, 3, 4, 4) # æ‰¹æ¬¡2ï¼Œ3ä¸ªç±»åˆ«ï¼Œ4x4å›¾åƒ\n",
    "targets_segmentation = torch.randint(0, 3, (2, 4, 4), dtype=torch.long)\n",
    "# å‡è®¾ç±»åˆ« 0 æ˜¯è¦å¿½ç•¥çš„èƒŒæ™¯\n",
    "targets_segmentation[0, 0, 0] = 0 # å°†æŸä¸ªä½ç½®çš„æ ‡ç­¾è®¾ä¸ºè¦å¿½ç•¥çš„ç´¢å¼•\n",
    "targets_segmentation[1, 2, 3] = 0 # å¦ä¸€ä¸ªä½ç½®ä¹Ÿè®¾ä¸ºå¿½ç•¥\n",
    "\n",
    "loss_fn_ignore = nn.CrossEntropyLoss(ignore_index=0) # å¿½ç•¥ç±»åˆ«0çš„æŸå¤±\n",
    "loss_segmentation = loss_fn_ignore(predictions_segmentation_logits, targets_segmentation)\n",
    "print(f\"CrossEntropyLoss (with ignore_index): {loss_segmentation.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce665074-34e0-4cd6-bbc4-71d6ea3b3e56",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–å™¨\n",
    "\n",
    "| ä¼˜åŒ–ç­–ç•¥                     | ç®€ä»‹                                                                 | ç‰¹ç‚¹                                                                                     | ä¼˜ç‚¹                                                                 | ç¼ºç‚¹                                                                 |\n",
    "|------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **æ ‡å‡†æ¢¯åº¦ä¸‹é™ (GD)**        | ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è®¡ç®—æ¢¯åº¦ï¼Œä¸€æ¬¡æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚                           | å…¨é‡æ•°æ®ï¼Œæ¢¯åº¦å‡†ç¡®ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚                                                         | ç¨³å®šï¼Œæ”¶æ•›å¯é ï¼ˆå‡¸é—®é¢˜ï¼‰ã€‚                                           | è®¡ç®—æ…¢ï¼Œå†…å­˜éœ€æ±‚å¤§ï¼Œä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®ã€‚                               |\n",
    "| **éšæœºæ¢¯åº¦ä¸‹é™ (SGD)**       | æ¯æ¬¡éšæœºæŠ½å–ä¸€ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ã€‚                             | å•æ ·æœ¬æ›´æ–°ï¼Œè®¡ç®—å¿«ï¼Œå™ªå£°å¤§ã€‚                                                             | æ•ˆç‡é«˜ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼Œå™ªå£°å¯å¸®åŠ©é€ƒç¦»å±€éƒ¨æå€¼ã€‚                     | æ¢¯åº¦æ³¢åŠ¨å¤§ï¼Œæ”¶æ•›ä¸ç¨³å®šã€‚                                             |\n",
    "| **å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD)** | ä½¿ç”¨å°æ‰¹é‡æ ·æœ¬è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ã€‚                                   | å¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œbatch å¤§å°é€šå¸¸ä¸º 32ã€64 ç­‰ã€‚                                           | å…¼é¡¾æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œå¹¿æ³›åº”ç”¨ã€‚                                         | Batch å¤§å°éœ€è°ƒå‚ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æå€¼ã€‚                                 |\n",
    "| **åŠ¨é‡æ³• (Momentum)**        | å¼•å…¥åŠ¨é‡é¡¹ï¼Œç´¯ç§¯å†å²æ¢¯åº¦æ–¹å‘ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚                             | å¹³æ»‘æ›´æ–°æ–¹å‘ï¼ŒåŠ¨é‡å‚æ•°é€šå¸¸ä¸º 0.9ã€‚                                                       | åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡ï¼Œé€‚åˆæ·±å±‚ç½‘ç»œã€‚                                   | éœ€è°ƒå‚ï¼Œå¯èƒ½è¶…è°ƒã€‚                                                   |\n",
    "| **Nesterov åŠ é€Ÿæ¢¯åº¦ (NAG)**  | åŠ¨é‡æ³•æ”¹è¿›ï¼Œæå‰è®¡ç®—å‰ç»æ¢¯åº¦ã€‚                                       | åŸºäºâ€œæœªæ¥â€ä½ç½®æ¢¯åº¦æ›´æ–°ï¼Œæ›´ç²¾ç¡®ã€‚                                                         | æ”¶æ•›æ›´å¿«ï¼Œé€‚åˆæ›²ç‡å¤§åŒºåŸŸã€‚                                           | å®ç°å¤æ‚ï¼Œéœ€è°ƒå‚ã€‚                                                   |\n",
    "| **Adagrad**                  | è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ ¹æ®å†å²æ¢¯åº¦å¹³æ–¹å’Œç¼©æ”¾ã€‚                           | ç¨€ç–ç‰¹å¾æ›´æ–°å¿«ï¼Œé¢‘ç¹ç‰¹å¾æ›´æ–°æ…¢ã€‚                                                         | é€‚åˆç¨€ç–æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒå­¦ä¹ ç‡ã€‚                                     | å­¦ä¹ ç‡è¿‡æ—©è¡°å‡ï¼Œå¯èƒ½åœæ­¢æ›´æ–°ã€‚                                       |\n",
    "| **RMSprop**                  | Adagrad æ”¹è¿›ï¼Œç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘æ¢¯åº¦å¹³æ–¹ç´¯ç§¯ã€‚                       | é˜²æ­¢å­¦ä¹ ç‡è¿‡æ—©è¡°å‡ï¼Œè¡°å‡ç‡é€šå¸¸ä¸º 0.9ã€‚                                                   | é€‚åˆéå¹³ç¨³ç›®æ ‡ï¼Œæ”¶æ•›ç¨³å®šã€‚                                           | éœ€è°ƒæ•´åˆå§‹å­¦ä¹ ç‡ã€‚                                                   |\n",
    "| **Adam**                     | ç»“åˆåŠ¨é‡æ³•å’Œ RMSpropï¼Œç»´æŠ¤ä¸€é˜¶å’ŒäºŒé˜¶æ—¶åˆ»çš„ç§»åŠ¨å¹³å‡ã€‚                 | è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œåå·®æ ¡æ­£ï¼Œå‚æ•° Î²â‚â‰ˆ0.9ï¼ŒÎ²â‚‚â‰ˆ0.999ã€‚                                          | æ”¶æ•›å¿«ï¼Œé€‚åº”æ€§å¼ºï¼Œé»˜è®¤å‚æ•°è¡¨ç°å¥½ã€‚                                   | æ³›åŒ–æ€§èƒ½å¯èƒ½é€Šäº SGDï¼Œéœ€è°ƒå‚ã€‚                                       |\n",
    "| **AdamW**                    | Adam æ”¹è¿›ï¼Œæ·»åŠ æƒé‡è¡°å‡ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚                             | è§£è€¦æƒé‡è¡°å‡ä¸è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚                                                             | æ³›åŒ–æ€§èƒ½ä¼˜äº Adamï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ã€‚                                     | éœ€è°ƒæƒé‡è¡°å‡ç³»æ•°ã€‚                                                   |\n",
    "| **AdaDelta**                 | RMSprop æ‰©å±•ï¼Œæ— éœ€è®¾ç½®å…¨å±€å­¦ä¹ ç‡ï¼Œç”¨æ›´æ–°é‡ç§»åŠ¨å¹³å‡ç¼©æ”¾æ›´æ–°ã€‚         | åŸºäºæ›´æ–°é‡ç§»åŠ¨å¹³å‡è°ƒæ•´ï¼Œå…å»å­¦ä¹ ç‡è®¾ç½®ã€‚                                                 | æ— éœ€è°ƒå­¦ä¹ ç‡ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ã€‚                                         | å®ç°å¤æ‚ï¼Œä½¿ç”¨è¾ƒå°‘ã€‚                                                 |\n",
    "\n",
    "**PyTorchå®ç°çš„13ä¼˜åŒ–ç®—æ³•**\n",
    "\n",
    "| ä¼˜åŒ–ç®—æ³•       | åŸç†                                                                 | å…³é”®å‚æ•°                                                                 | é€‚ç”¨åœºæ™¯                                                   | ä¼˜ç‚¹                                                                 | ç¼ºç‚¹                                                                 |\n",
    "|----------------|----------------------------------------------------------------------|--------------------------------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **SGD**        | éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæ”¯æŒåŠ¨é‡å’Œ Nesterov åŠ¨é‡ï¼ŒåŸºäºå°æ‰¹é‡æ¢¯åº¦æ›´æ–°ã€‚         | `lr`, `momentum`, `dampening`, `weight_decay`, `nesterov`                | å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€ç²¾ç»†æ§åˆ¶çš„ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ï¼‰ã€‚              | ç®€å•ï¼Œæ³›åŒ–æ€§èƒ½å¥½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼ŒåŠ¨é‡åŠ é€Ÿæ”¶æ•›ã€‚             | éœ€æ‰‹åŠ¨è°ƒå­¦ä¹ ç‡ï¼Œæ”¶æ•›å¯èƒ½è¾ƒæ…¢ï¼Œæ˜“å—å™ªå£°å½±å“ã€‚                 |\n",
    "| **Adam**       | ç»“åˆä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡çš„è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•ï¼Œæ”¶æ•›å¿«ã€‚                       | `lr`, `betas`, `eps`, `weight_decay`, `amsgrad`                          | æ·±åº¦å­¦ä¹ é€šç”¨ä»»åŠ¡ï¼ˆå¦‚ CNNã€RNNã€Transformerï¼‰ã€‚              | æ”¶æ•›å¿«ï¼Œé»˜è®¤å‚æ•°é²æ£’ï¼Œé€‚åº”å¤šç§é—®é¢˜ã€‚                         | æ³›åŒ–æ€§èƒ½å¯èƒ½é€Šäº SGDï¼Œéœ€è°ƒå‚ã€‚                               |\n",
    "| **AdamW**      | Adam çš„æ”¹è¿›ï¼Œè§£è€¦æƒé‡è¡°å‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¢å¼ºæ³›åŒ–ã€‚                 | `lr`, `betas`, `eps`, `weight_decay`                                    | éœ€è¦æ›´å¥½æ³›åŒ–çš„ä»»åŠ¡ï¼ˆå¦‚ NLPã€è®¡ç®—æœºè§†è§‰ï¼‰ã€‚                 | æ³›åŒ–æ€§èƒ½ä¼˜äº Adamï¼Œæ”¶æ•›ç¨³å®šã€‚                                | éœ€è°ƒæƒé‡è¡°å‡ç³»æ•°ï¼Œè®¡ç®—ç¨å¤æ‚ã€‚                               |\n",
    "| **RMSprop**    | ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘æ¢¯åº¦å¹³æ–¹ï¼Œé˜²æ­¢å­¦ä¹ ç‡è¿‡æ—©è¡°å‡ã€‚                     | `lr`, `alpha`, `eps`, `weight_decay`, `momentum`, `centered`             | éå¹³ç¨³ç›®æ ‡ï¼ˆå¦‚ RNNã€å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚                           | é€‚åˆéå¹³ç¨³é—®é¢˜ï¼Œæ”¶æ•›ç¨³å®šã€‚                                   | éœ€è°ƒåˆå§‹å­¦ä¹ ç‡ï¼Œæ³›åŒ–æ€§èƒ½å¯èƒ½ä¸€èˆ¬ã€‚                           |\n",
    "| **Adagrad**    | è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼ŒåŸºäºå†å²æ¢¯åº¦å¹³æ–¹å’Œï¼Œé€‚åˆç¨€ç–æ•°æ®ã€‚                 | `lr`, `lr_decay`, `weight_decay`, `eps`                                 | ç¨€ç–æ•°æ®ä»»åŠ¡ï¼ˆå¦‚æ¨èç³»ç»Ÿã€NLP è¯åµŒå…¥ï¼‰ã€‚                   | æ— éœ€æ‰‹åŠ¨è°ƒå­¦ä¹ ç‡ï¼Œé€‚åˆç¨€ç–æ¢¯åº¦ã€‚                             | å­¦ä¹ ç‡è¿‡æ—©è¡°å‡ï¼Œå¯èƒ½åœæ­¢æ›´æ–°ã€‚                               |\n",
    "| **Adadelta**   | RMSprop æ‰©å±•ï¼Œç”¨æ›´æ–°é‡ç§»åŠ¨å¹³å‡ä»£æ›¿å…¨å±€å­¦ä¹ ç‡ã€‚                       | `lr`, `rho`, `eps`, `weight_decay`                                      | å¤æ‚ä»»åŠ¡ï¼Œéœ€å…è°ƒå­¦ä¹ ç‡ï¼ˆå¦‚æ·±åº¦ç½‘ç»œï¼‰ã€‚                     | æ— éœ€è®¾ç½®å…¨å±€å­¦ä¹ ç‡ï¼Œé€‚åº”æ€§å¼ºã€‚                               | å®ç°å¤æ‚ï¼Œä½¿ç”¨è¾ƒå°‘ï¼Œæ”¶æ•›å¯èƒ½è¾ƒæ…¢ã€‚                           |\n",
    "| **Adamax**     | Adam å˜ä½“ï¼Œç”¨æ¢¯åº¦æ— ç©·èŒƒæ•°ä»£æ›¿äºŒé˜¶åŠ¨é‡ï¼Œé€‚åˆç¨€ç–æ¢¯åº¦ã€‚               | `lr`, `betas`, `eps`, `weight_decay`                                    | ç¨€ç–æ¢¯åº¦åœºæ™¯ï¼ˆå¦‚ NLPï¼‰ã€‚                                   | é€‚åˆç¨€ç–æ•°æ®ï¼Œè®¡ç®—ç®€å•ã€‚                                     | åº”ç”¨èŒƒå›´è¾ƒçª„ï¼Œæ³›åŒ–æ€§èƒ½ä¸€èˆ¬ã€‚                                 |\n",
    "| **NAdam**      | ç»“åˆ Adam å’Œ Nesterov åŠ¨é‡ï¼Œæ›´æ–°æ›´ç²¾ç¡®ã€‚                             | `lr`, `betas`, `eps`, `weight_decay`, `momentum_decay`                   | éœ€é«˜ç²¾åº¦æ›´æ–°çš„ä»»åŠ¡ï¼ˆå¦‚ Transformerï¼‰ã€‚                     | æ”¶æ•›æ›´å¿«ï¼Œæ›´æ–°æ–¹å‘æ›´ç²¾ç¡®ã€‚                                   | è®¡ç®—å¤æ‚åº¦ç¨é«˜ï¼Œéœ€è°ƒå‚ã€‚                                     |\n",
    "| **RAdam**      | Adam çš„æ”¹è¿›ï¼Œé€šè¿‡è‡ªé€‚åº”æ•´æµå‡å°‘åˆæœŸæ–¹å·®ã€‚                            | `lr`, `betas`, `eps`, `weight_decay`                                    | éœ€ç¨³å®šåˆå§‹è®­ç»ƒçš„ä»»åŠ¡ï¼ˆå¦‚æ·±åº¦ç½‘ç»œï¼‰ã€‚                       | åˆæœŸæ›´ç¨³å®šï¼Œæ”¶æ•›æ€§èƒ½å¥½ã€‚                                     | åº”ç”¨è¾ƒæ–°ï¼Œæ³›åŒ–æ€§èƒ½éœ€éªŒè¯ã€‚                                   |\n",
    "| **ASGD**       | å¯¹ SGD å‚æ•°æ›´æ–°è¿›è¡Œå¹³å‡ï¼Œé€‚åˆå‡¸é—®é¢˜ã€‚                                | `lr`, `lambd`, `alpha`, `t0`, `weight_decay`                            | å‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆå¦‚çº¿æ€§å›å½’ï¼‰ã€‚                                 | é€‚åˆå‡¸é—®é¢˜ï¼Œæ”¶æ•›ç¨³å®šã€‚                                       | æ·±åº¦å­¦ä¹ ä¸­è¾ƒå°‘ä½¿ç”¨ï¼Œæ•ˆç‡è¾ƒä½ã€‚                               |\n",
    "| **LBFGS**      | äºŒé˜¶ä¼˜åŒ–ï¼Œä½¿ç”¨æœ‰é™å†…å­˜è¿‘ä¼¼ Hessianï¼Œé€‚åˆå°æ•°æ®é›†ã€‚                   | `lr`, `max_iter`, `max_eval`, `history_size`                            | å°æ•°æ®é›†ï¼Œå‡¸ä¼˜åŒ–é—®é¢˜ã€‚                                     | é«˜ç²¾åº¦ï¼Œé€‚åˆç®€å•é—®é¢˜ã€‚                                       | è®¡ç®—å¯†é›†ï¼Œéœ€ closure å‡½æ•°ï¼Œä¸é€‚åˆæ·±åº¦å­¦ä¹ å¤§æ¨¡å‹ã€‚             |\n",
    "| **SparseAdam** | Adam çš„ç¨€ç–ç‰ˆæœ¬ï¼Œä¼˜åŒ–ç¨€ç–å¼ é‡æ¢¯åº¦ã€‚                                  | `lr`, `betas`, `eps`                                                    | ç¨€ç–æ¢¯åº¦ä»»åŠ¡ï¼ˆå¦‚åµŒå…¥å±‚ä¼˜åŒ–ï¼‰ã€‚                             | é€‚åˆç¨€ç–æ•°æ®ï¼Œå†…å­˜æ•ˆç‡é«˜ã€‚                                   | ä»…é™ç¨€ç–å¼ é‡ï¼Œåº”ç”¨å—é™ã€‚                                     |\n",
    "| **Rprop**      | åŸºäºæ¢¯åº¦ç¬¦å·è°ƒæ•´æ­¥é•¿ï¼Œå¿½ç•¥æ¢¯åº¦å¤§å°ã€‚                                 | `lr`, `etas`, `step_sizes`                                              | å…¨æ‰¹é‡ä¼˜åŒ–ï¼Œç®€å•é—®é¢˜ã€‚                                     | ç®€å•ï¼Œé€‚åˆå…¨æ‰¹é‡ä»»åŠ¡ã€‚                                       | ä¸é€‚åˆæ·±åº¦å­¦ä¹ ï¼Œæ”¶æ•›æ…¢ã€‚                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b1e61-0265-4992-93d8-2cfb81f687f5",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ä¼˜åŒ–å™¨çš„æ ‡å‡†æ­¥éª¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3308-99c6-4a1f-9ebc-2d4020dcbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. å®šä¹‰æ¨¡å‹ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨ä»…åŒ…å«ä¸€ä¸ªçº¿æ€§å±‚çš„æ¨¡å‹\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# æ‰“å°æ¨¡å‹çš„åˆå§‹å‚æ•°\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"Name: {name} | Shape: {param.shape} | Value:\\n{param.data}\")\n",
    "\n",
    "# 2. å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 3. å®ä¾‹åŒ–ä¼˜åŒ–å™¨ï¼ˆæ¯”å¦‚SGDï¼‰\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randn(32, 1)\n",
    "\n",
    "# 4. ï¼ˆè®­ç»ƒå¾ªç¯ï¼‰ä¸ºç®€åŒ–ä»£ç ï¼Œè¿™é‡Œä»…æä¾›ä¸€æ¬¡è¿­ä»£\n",
    "optimizer.zero_grad()   # æ¸…ç©ºæ—§æ¢¯åº¦\n",
    "output = model(x)       # å‰å‘ä¼ æ’­\n",
    "loss = criterion(output, y)  # è®¡ç®—æŸå¤±\n",
    "loss.backward()         # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "optimizer.step()        # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "\n",
    "# è®­ç»ƒåï¼Œæ‰“å°æ¨¡å‹çš„å‚æ•°\n",
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"Name: {name} | Shape: {param.shape} | Value:\\n{param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710390e7-a7ef-4c81-8e28-db70640ddf79",
   "metadata": {},
   "source": [
    "#### æ ‡å‡†çš„æ¨¡å‹è®­ç»ƒæµç¨‹\n",
    "- é‡ç‚¹åœ¨äºä¼˜åŒ–å™¨çš„ä½¿ç”¨æ­¥éª¤\n",
    "- å®šä¹‰äº†è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844d090-5afd-4aab-a24e-0085cc76dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. å®šä¹‰æ¨¡å‹ï¼Œé€šè¿‡ç»§æ‰¿nn.Moduleè¿›è¡Œï¼Œä»…æä¾›äº†ä¸€ä¸ªçº¯æ€§å±‚\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "# 2. å®šä¹‰æŸå¤±å‡½æ•°\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 3. å®šä¹‰ä¼˜åŒ–å™¨\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. å‡†å¤‡æ•°æ®\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 5. è®­ç»ƒå¾ªç¯\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()        # æ¸…é›¶æ¢¯åº¦\n",
    "        outputs = model(batch_X)     # å‰å‘ä¼ æ’­\n",
    "        loss = criterion(outputs, batch_y)  # è®¡ç®—æŸå¤±\n",
    "        loss.backward()             # åå‘ä¼ æ’­\n",
    "        optimizer.step()            # æ›´æ–°å‚æ•°\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 6. ï¼ˆå¯é€‰ï¼‰ä¿å­˜æ¨¡å‹\n",
    "#torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a4584-57b5-4c10-a6d3-0d38fea6ad5e",
   "metadata": {},
   "source": [
    "### optim.SGDä¼˜åŒ–ç®—æ³•ç¤ºä¾‹\n",
    "- æ¯”è¾ƒSGDã€åŠ¨é‡SGDå’ŒNesterovåŠ¨é‡çš„å‚æ•°æ”¶æ•›æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd12b6-dd3b-4106-9b0f-c73f44074ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å®šä¹‰è¦ä¼˜åŒ–çš„å‡½æ•° f(x) = (x-5)^2\n",
    "def func(x):\n",
    "    return (x - 5) ** 2\n",
    "\n",
    "# åˆå§‹åŒ–å‚æ•°çš„èµ·å§‹å€¼\n",
    "start_x = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# è®¾ç½®ä¼˜åŒ–å‚æ•°\n",
    "learning_rate = 0.1\n",
    "num_iterations = 50 # è¿­ä»£æ¬¡æ•°\n",
    "\n",
    "# --- å®éªŒä¸€ï¼šæ ‡å‡† SGD (momentum = 0) ---\n",
    "print(\"--- æ ‡å‡† SGD (momentum = 0) ---\")\n",
    "x_sgd = start_x.clone().detach().requires_grad_(True) # å…‹éš†èµ·å§‹å€¼ï¼Œç”¨äºSGD\n",
    "optimizer_sgd = optim.SGD([x_sgd], lr=learning_rate, momentum=0)\n",
    "\n",
    "sgd_history = [x_sgd.item()]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    optimizer_sgd.zero_grad() # æ¸…é›¶ä¹‹å‰çš„æ¢¯åº¦\n",
    "    loss = func(x_sgd)        # è®¡ç®—æŸå¤±\n",
    "    loss.backward()           # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "    optimizer_sgd.step()      # æ›´æ–°å‚æ•°\n",
    "\n",
    "    sgd_history.append(x_sgd.item())\n",
    "\n",
    "    # if (i + 1) % 10 == 0:\n",
    "    #     print(f\"Iteration {i+1}: x = {x_sgd.item():.4f}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"æœ€ç»ˆå‚æ•°x (æ ‡å‡†SGD): {x_sgd.item():.4f}\")\n",
    "\n",
    "\n",
    "# --- å®éªŒäºŒï¼šåŠ¨é‡ SGD (momentum = 0.9) ---\n",
    "print(\"\\n--- åŠ¨é‡ SGD (momentum = 0.9) ---\")\n",
    "x_momentum = start_x.clone().detach().requires_grad_(True) # å…‹éš†èµ·å§‹å€¼ï¼Œç”¨äºåŠ¨é‡SGD\n",
    "# æ³¨æ„ï¼šè¿™é‡Œè®¾ç½®äº† momentum å‚æ•°\n",
    "optimizer_momentum = optim.SGD([x_momentum], lr=learning_rate, momentum=0.9)\n",
    "\n",
    "momentum_history = [x_momentum.item()]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    optimizer_momentum.zero_grad() # æ¸…é›¶ä¹‹å‰çš„æ¢¯åº¦\n",
    "    loss = func(x_momentum)     # è®¡ç®—æŸå¤±\n",
    "    loss.backward()          # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "    optimizer_momentum.step()   # æ›´æ–°å‚æ•°\n",
    "\n",
    "    momentum_history.append(x_momentum.item())\n",
    "\n",
    "    # if (i + 1) % 10 == 0:\n",
    "    #     print(f\"Iteration {i+1}: x = {x_momentum.item():.4f}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"æœ€ç»ˆå‚æ•°x (åŠ¨é‡SGD): {x_momentum.item():.4f}\")\n",
    "\n",
    "# --- å®éªŒä¸‰ï¼šNesterovåŠ¨é‡SGD (momentum = 0.9, nesterov = True) ---\n",
    "print(\"\\n--- NesterovåŠ¨é‡SGD (momentum = 0.9, nesterov = True) ---\")\n",
    "x_nesterov = start_x.clone().detach().requires_grad_(True) # å…‹éš†èµ·å§‹å€¼ï¼Œç”¨äº Nesterov SGD\n",
    "# æ³¨æ„ï¼šè¿™é‡Œè®¾ç½®äº† momentum å’Œ nesterov å‚æ•°\n",
    "optimizer_nesterov = optim.SGD([x_nesterov], lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "nesterov_history = [x_nesterov.item()]\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    optimizer_nesterov.zero_grad() # æ¸…é›¶ä¹‹å‰çš„æ¢¯åº¦\n",
    "    loss = func(x_nesterov)     # è®¡ç®—æŸå¤±\n",
    "    loss.backward()          # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "    optimizer_nesterov.step()   # æ›´æ–°å‚æ•°\n",
    "\n",
    "    nesterov_history.append(x_nesterov.item())\n",
    "\n",
    "    # if (i + 1) % 10 == 0:\n",
    "    #     print(f\"Iteration {i+1}: x = {x_nesterov.item():.4f}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"æœ€ç»ˆå‚æ•°x (NesterovåŠ¨é‡SGD): {x_nesterov.item():.4f}\")\n",
    "\n",
    "\n",
    "# --- ç»˜åˆ¶å‚æ•°éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–å›¾ ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sgd_history, label='Standard SGD (momentum=0)', marker='o', linestyle='-')\n",
    "plt.plot(momentum_history, label='Momentum SGD (momentum=0.9)', marker='x', linestyle='--')\n",
    "plt.plot(nesterov_history, label='Nesterov Momentum SGD (momentum=0.9)', marker='*', linestyle='-.')\n",
    "plt.axhline(y=5.0, color='r', linestyle='-', label='Optimal x = 5.0') # ç»˜åˆ¶ç›®æ ‡å€¼\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Parameter x value\")\n",
    "plt.title(\"Parameter x Value vs. Iteration for different SGD variants\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93554136-98f2-4561-9aa2-d5468503099d",
   "metadata": {},
   "source": [
    "### è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•\n",
    "\n",
    "| ç‰¹æ€§ / ç®—æ³• | Adagrad (Adaptive Gradient) | RMSprop (Root Mean Square Propagation) | AdaDelta |\n",
    "| :---------------- | :--------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **æ ¸å¿ƒæ€æƒ³** | æ ¹æ®**è¿‡å»æ‰€æœ‰æ¢¯åº¦å¹³æ–¹çš„ç´¯åŠ å’Œ**æ¥è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚ | æ ¹æ®**è¿‡å»æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡**ï¼ˆExponentially Weighted Moving Average, EWMAï¼‰æ¥è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ã€‚ | ç»“åˆäº†æ¢¯åº¦å¹³æ–¹çš„ EWMA å’Œå‚æ•°æ›´æ–°çš„ EWMAï¼Œ**åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ— éœ€æ‰‹åŠ¨è®¾ç½®å­¦ä¹ ç‡**ã€‚ |\n",
    "| **å­¦ä¹ ç‡è°ƒæ•´** | é¢‘ç¹æ›´æ–°çš„å‚æ•°å­¦ä¹ ç‡ä¸‹é™æ›´å¿«ï¼Œä¸é¢‘ç¹æ›´æ–°çš„å‚æ•°å­¦ä¹ ç‡ä¸‹é™æ›´æ…¢ã€‚ | è§£å†³äº† Adagrad å­¦ä¹ ç‡å•è°ƒé€’å‡è¿‡å¿«çš„é—®é¢˜ï¼Œèƒ½æ›´çµæ´»åœ°é€‚åº”éç¨³æ€å’Œéå‡¸é—®é¢˜ã€‚ | å½»åº•æ¶ˆé™¤äº†å¯¹å…¨å±€å­¦ä¹ ç‡çš„ä¾èµ–ï¼Œèƒ½å¤ŸåŠ¨æ€è°ƒæ•´å­¦ä¹ æ­¥é•¿ã€‚ |\n",
    "| **ä¼˜ç‚¹** | - é€‚ç”¨äº**ç¨€ç–æ•°æ®**ï¼ˆå¦‚ NLP ä¸­çš„è¯åµŒå…¥ï¼‰ï¼Œèƒ½æœ‰æ•ˆæé«˜ä½é¢‘ç‰¹å¾çš„å­¦ä¹ æ•ˆç‡ã€‚<br>- æ— éœ€æ‰‹åŠ¨è°ƒæ•´æ¯ç»´å­¦ä¹ ç‡ã€‚ | - ç¼“è§£äº† Adagrad å­¦ä¹ ç‡è¿‡æ—©è¡°å‡çš„é—®é¢˜ã€‚<br>- åœ¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç­‰ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚<br>- æ¯” Adagrad é€‚ç”¨èŒƒå›´æ›´å¹¿ã€‚ | - **æ— éœ€æ‰‹åŠ¨è®¾ç½®å­¦ä¹ ç‡**ã€‚<br>- è®­ç»ƒåæœŸå­¦ä¹ ç‡ä¸ä¼šè¿‡å°ï¼Œæœ‰åŠ©äºæŒç»­å­¦ä¹ ã€‚<br>- å¯¹å­¦ä¹ ç‡è¶…å‚æ•°ä¸æ•æ„Ÿã€‚ |\n",
    "| **ç¼ºç‚¹** | - **å­¦ä¹ ç‡ä¼šæŒç»­å•è°ƒé€’å‡**ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒåæœŸå­¦ä¹ ç‡è¿‡å°ï¼Œæ¨¡å‹è¿‡æ—©åœæ­¢å­¦ä¹ ã€‚ | - ä»ç„¶éœ€è¦æ‰‹åŠ¨è®¾ç½®ä¸€ä¸ª**å…¨å±€å­¦ä¹ ç‡**ã€‚<br>- ç¼ºä¹å¯¹åŠ¨é‡çš„å†…ç½®æ”¯æŒï¼ˆé€šå¸¸éœ€ä¸ Momentum ç»“åˆï¼‰ã€‚ | - ç›¸è¾ƒäº Adamï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šæ”¶æ•›é€Ÿåº¦å¯èƒ½ç•¥æ…¢ã€‚<br>- å®ç°ç›¸å¯¹å¤æ‚ã€‚ |\n",
    "| **ä¸»è¦è¶…å‚æ•°** | åˆå§‹å­¦ä¹ ç‡ ($\\eta$)ï¼Œå¸¸æ•° $\\epsilon$ï¼ˆå°å¸¸æ•°ï¼Œé˜²æ­¢é™¤é›¶ï¼‰ | åˆå§‹å­¦ä¹ ç‡ ($\\eta$)ï¼Œè¡°å‡ç‡ ($\\rho$)ï¼Œå¸¸æ•° $\\epsilon$ | è¡°å‡ç‡ ($\\rho$)ï¼Œå¸¸æ•° $\\epsilon$ |\n",
    "| **æ•°å­¦å…¬å¼** | $\\mathbf{g}_t = \\nabla L(\\theta_{t-1})$ <br>$\\mathbf{G}_t = \\mathbf{G}_{t-1} + \\mathbf{g}_t \\odot \\mathbf{g}_t$ <br>$\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{G}_t + \\epsilon}} \\odot \\mathbf{g}_t$ | $\\mathbf{g}_t = \\nabla L(\\theta_{t-1})$ <br>$\\mathbf{E}[\\mathbf{g}^2]_t = \\rho \\mathbf{E}[\\mathbf{g}^2]_{t-1} + (1-\\rho) \\mathbf{g}_t \\odot \\mathbf{g}_t$ <br>$\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{E}[\\mathbf{g}^2]_t + \\epsilon}} \\odot \\mathbf{g}_t$ | $\\mathbf{g}_t = \\nabla L(\\theta_{t-1})$ <br>$\\mathbf{E}[\\mathbf{g}^2]_t = \\rho \\mathbf{E}[\\mathbf{g}^2]_{t-1} + (1-\\rho) \\mathbf{g}_t \\odot \\mathbf{g}_t$ <br>$\\Delta \\theta_t = - \\frac{\\sqrt{\\mathbf{E}[\\Delta \\theta^2]_{t-1} + \\epsilon}}{\\sqrt{\\mathbf{E}[\\mathbf{g}^2]_t + \\epsilon}} \\odot \\mathbf{g}_t$ <br>$\\mathbf{E}[\\Delta \\theta^2]_t = \\rho \\mathbf{E}[\\Delta \\theta^2]_{t-1} + (1-\\rho) \\Delta \\theta_t \\odot \\Delta \\theta_t$ <br>$\\theta_t = \\theta_{t-1} + \\Delta \\theta_t$ |\n",
    "| **å¤‡æ³¨** | - $\\theta$: æ¨¡å‹å‚æ•°<br>- $L$: æŸå¤±å‡½æ•°<br>- $\\nabla L(\\theta)$: æ¢¯åº¦<br>- $\\eta$: åˆå§‹å­¦ä¹ ç‡<br>- $\\mathbf{g}_t$: å½“å‰æ¢¯åº¦<br>- $\\mathbf{G}_t$: å†å²æ¢¯åº¦å¹³æ–¹çš„ç´¯åŠ å’Œï¼ˆå¯¹è§’çŸ©é˜µï¼‰<br>- $\\mathbf{E}[\\mathbf{g}^2]_t$: æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡<br>- $\\mathbf{E}[\\Delta \\theta^2]_t$: å‚æ•°æ›´æ–°å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡<br>- $\\rho$: è¡°å‡ç‡ (RMSprop å’Œ Adadelta ä¸­çš„è¶…å‚æ•°ï¼Œé€šå¸¸ä¸º 0.9 æˆ– 0.99)<br>- $\\epsilon$: æå°å¸¸æ•°ï¼Œé€šå¸¸ä¸º $10^{-8}$ï¼Œé˜²æ­¢åˆ†æ¯ä¸ºé›¶<br>- $\\odot$: å…ƒç´ çº§ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰ | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a63064-018b-4d10-ac9e-f1cdd5073dad",
   "metadata": {},
   "source": [
    "#### ç¤ºä¾‹ï¼šå¯¹æ¯”ä¸‰ä¸ªè‡ªé€‚åº”å­¦ä¹ ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ca8f5-5bfa-415b-97fc-a49918fb1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "X = torch.linspace(-5, 5, 1000).reshape(-1, 1)\n",
    "y = X**2 + 0.5 * X + torch.randn(X.size()) * 0.5  # y = x^2 + 0.5x + noise\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# è®­ç»ƒå‡½æ•°\n",
    "def train_model(optimizer_class, optimizer_params, num_epochs=50):\n",
    "    model = SimpleNet()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_params)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_X.size(0)\n",
    "        losses.append(epoch_loss / len(dataset))\n",
    "    return losses\n",
    "\n",
    "# ä¼˜åŒ–å™¨å‚æ•°\n",
    "optimizers = [\n",
    "    (optim.Adagrad, {'lr': 0.01, 'eps': 1e-10}, 'Adagrad'),\n",
    "    (optim.RMSprop, {'lr': 0.01, 'alpha': 0.99, 'eps': 1e-8}, 'RMSprop'),\n",
    "    (optim.Adadelta, {'lr': 1.0, 'rho': 0.9, 'eps': 1e-6}, 'Adadelta')\n",
    "]\n",
    "\n",
    "# è®­ç»ƒå¹¶è®°å½•æŸå¤±\n",
    "results = {}\n",
    "for opt_class, params, name in optimizers:\n",
    "    losses = train_model(opt_class, params)\n",
    "    results[name] = losses\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, losses in results.items():\n",
    "    plt.plot(losses, label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Adagrad, RMSpropå’ŒAdadeltaçš„æŸå¤±æ›²çº¿')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5804984-82db-4a74-8a2b-24ef241afd87",
   "metadata": {},
   "source": [
    "### è‡ªé€‚åº”åŠ¨é‡ä¼˜åŒ–ç®—æ³•\n",
    "\n",
    "| **ä¼˜åŒ–ç®—æ³•** | **èåˆæœºåˆ¶** | **PyTorchç±»å** | **å…³é”®ç‰¹æ€§** | **é€‚ç”¨åœºæ™¯** |\n",
    "|--------------|--------------|------------------|--------------|--------------|\n",
    "| **Adam** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆä¸€é˜¶å’ŒäºŒé˜¶çŸ©çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ | `torch.optim.Adam` | - ç»“åˆä¸€é˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦å‡å€¼ï¼‰å’ŒäºŒé˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦å¹³æ–¹å‡å€¼ï¼‰<br>- åå·®ä¿®æ­£ç¡®ä¿æ—©æœŸè¿­ä»£ç¨³å®šæ€§<br>- é»˜è®¤å‚æ•°ï¼ˆlr=0.001, betas=(0.9, 0.999)ï¼‰é²æ£’ | - å‡ ä¹æ‰€æœ‰æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚CNNã€RNNã€Transformerï¼‰<br>- éå‡¸ä¼˜åŒ–é—®é¢˜<br>- é€šç”¨é¦–é€‰ä¼˜åŒ–å™¨ |\n",
    "| **AdamW** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ + è§£è€¦æƒé‡è¡°å‡ | `torch.optim.AdamW` | - æ”¹è¿›Adamçš„æƒé‡è¡°å‡ï¼Œè§£è€¦æ­£åˆ™åŒ–ä¸æ¢¯åº¦æ›´æ–°<br>- é»˜è®¤weight_decay=0.01ï¼Œå¢å¼ºæ³›åŒ–<br>- æ”¯æŒAMSGradå˜ä½“ | - éœ€è¦å¼ºæ­£åˆ™åŒ–çš„ä»»åŠ¡ï¼ˆå¦‚Transformerã€BERTï¼‰<br>- å¤§å‹æ¨¡å‹æˆ–æ˜“è¿‡æ‹Ÿåˆåœºæ™¯ |\n",
    "| **AMSGrad** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ + äºŒé˜¶åŠ¨é‡æœ€å¤§å€¼ | `Adam(amsgrad=True)` æˆ–<br> `AdamW(amsgrad=True)` | - ç»´æŠ¤äºŒé˜¶åŠ¨é‡å†å²æœ€å¤§å€¼ï¼Œå¢å¼ºæ”¶æ•›ç¨³å®šæ€§<br>- è§£å†³Adamåœ¨æŸäº›éå‡¸é—®é¢˜ä¸­çš„ä¸ç¨³å®š<br>- æ— éœ€å•ç‹¬ç±»ï¼Œé€šè¿‡`amsgrad=True`å¯ç”¨ | - éå‡¸ä¼˜åŒ–é—®é¢˜<br>- Adamè¡¨ç°ä¸ç¨³å®šæˆ–ç¨€ç–æ¢¯åº¦åœºæ™¯ |\n",
    "| **NAdam** | åŠ¨é‡æ³•ï¼ˆNesterovåŠ é€Ÿï¼‰ + è‡ªé€‚åº”å­¦ä¹ ç‡ | `torch.optim.NAdam` | - å¼•å…¥NesterovåŠ¨é‡ï¼Œæå‰é¢„æµ‹æ¢¯åº¦æ–¹å‘<br>- åŠ é€Ÿæ”¶æ•›ï¼Œç†è®ºä¸Šä¼˜äºAdam<br>- å¢åŠ momentum_decayå‚æ•°ï¼ˆé»˜è®¤0.004ï¼‰ | - NLPä»»åŠ¡ï¼ˆå¦‚RNNã€Transformerï¼‰<br>- éœ€è¦å¿«é€Ÿæ”¶æ•›çš„åœºæ™¯ |\n",
    "| **RAdam** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ + åŠ¨æ€äºŒé˜¶åŠ¨é‡æ ¡æ­£ | `torch.optim.RAdam` | - æ ¡æ­£æ—©æœŸäºŒé˜¶åŠ¨é‡æ–¹å·®ï¼Œå‡å°‘åˆå§‹ä¸ç¨³å®šæ€§<br>- è®­ç»ƒåˆæœŸæ›´ç¨³å®šï¼Œæ¥è¿‘AdamWæ€§èƒ½<br>- é»˜è®¤lr=0.001 | - CVã€NLPä»»åŠ¡<br>- éœ€è¦ç¨³å®šåˆå§‹è®­ç»ƒçš„åœºæ™¯ |\n",
    "| **Adamax** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆæ— ç©·èŒƒæ•°ï¼‰ | `torch.optim.Adamax` | - ä½¿ç”¨æ¢¯åº¦æ— ç©·èŒƒæ•°ä»£æ›¿äºŒé˜¶åŠ¨é‡<br>- å¯¹ç¨€ç–æ¢¯åº¦æˆ–å™ªå£°å¤§åœºæ™¯æ›´ç¨³å®š<br>- é»˜è®¤lr=0.002ï¼Œè®¡ç®—æ›´ç®€å• | - ç¨€ç–æ•°æ®ï¼ˆå¦‚NLPåµŒå…¥å±‚ï¼‰<br>- å™ªå£°è¾ƒå¤§çš„ä¼˜åŒ–ä»»åŠ¡ |\n",
    "| **SparseAdam** | åŠ¨é‡æ³• + è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆç¨€ç–ä¼˜åŒ–ï¼‰ | `torch.optim.SparseAdam` | - ä¸“ä¸ºç¨€ç–æ¢¯åº¦è®¾è®¡ï¼Œä»…æ›´æ–°éé›¶æ¢¯åº¦<br>- èŠ‚çœè®¡ç®—å’Œå†…å­˜<br>- ä¸æ”¯æŒweight_decay | - ç¨€ç–æ¢¯åº¦åœºæ™¯ï¼ˆå¦‚nn.Embedding(sparse=True)ï¼‰<br>- NLPä¸­çš„è¯åµŒå…¥ä¼˜åŒ– |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2ca38-302c-4bba-b78c-ec54e77d26d4",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1ebaf-00ae-4f38-8fa4-74959d1eb827",
   "metadata": {},
   "source": [
    "#### Datasetç¤ºä¾‹\n",
    "ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è‡ªå®šä¹‰Datasetç¤ºä¾‹ï¼Œå‡è®¾æ•°æ®é›†åŒ…å«å›¾åƒå’Œæ ‡ç­¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af7924-0210-4a8d-964f-a07df71f772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ•°æ®é›†\n",
    "        Args:\n",
    "            data: ç‰¹å¾æ•°æ®ï¼ˆå¦‚å›¾åƒæˆ–æ•°å€¼æ•°ç»„ï¼‰\n",
    "            labels: å¯¹åº”çš„æ ‡ç­¾\n",
    "            transform: æ•°æ®å¢å¼ºæˆ–é¢„å¤„ç†å‡½æ•°\n",
    "        \"\"\"\n",
    "        self.data = data  # å‡è®¾ä¸ºNumPyæ•°ç»„\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"è¿”å›æ•°æ®é›†å¤§å°\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"æ ¹æ®ç´¢å¼•è¿”å›å•ä¸ªæ ·æœ¬\"\"\"\n",
    "        sample = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # åº”ç”¨æ•°æ®å¢å¼ºæˆ–é¢„å¤„ç†\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # è½¬æ¢ä¸º PyTorch Tensor\n",
    "        sample = torch.tensor(sample, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0248d1-8d92-4924-9ae0-e083af7cfe1d",
   "metadata": {},
   "source": [
    "#### å¼•ç”¨Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc42d55-a74f-46c8-a95b-11994e0ab504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè™šæ‹Ÿæ•°æ®\n",
    "data = np.random.randn(100, 3, 32, 32)  # æ¨¡æ‹Ÿç”Ÿæˆ100å¼ 3x32x32çš„å›¾åƒ\n",
    "labels = np.random.randint(0, 10, size=100)  # 10ä¸ªæ ‡ç­¾ (0-9)\n",
    "dataset = SimpleDataset(data, labels)\n",
    "\n",
    "# è®¿é—®æ•°æ®é›†\n",
    "sample, label = dataset[0]\n",
    "print(f\"Sample shape: {sample.shape}, Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f5d42-7fd7-45b7-bc42-cb3d248b47ca",
   "metadata": {},
   "source": [
    "#### ç”±DataLoaderåŠ è½½æ•°æ®\n",
    "\n",
    "**å…³é”®å‚æ•°**\n",
    "- batch_sizeï¼šæ¯æ‰¹æ ·æœ¬æ•°ï¼Œéœ€æ ¹æ®GPUå†…å­˜è°ƒæ•´\n",
    "- shuffleï¼šè®­ç»ƒæ—¶è®¾ä¸ºTrueï¼ŒéªŒè¯/æµ‹è¯•æ—¶è®¾ä¸º Falseã€‚\n",
    "- num_workersï¼šå¤šçº¿ç¨‹åŠ è½½ï¼Œå»ºè®®4-8ï¼ˆè§†CPUæ ¸å¿ƒæ•°ï¼‰ã€‚\n",
    "- pin_memoryï¼šå¯ç”¨ä»¥åŠ é€ŸGPUæ•°æ®ä¼ è¾“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1151a-c410-4218-aad6-90168ae51c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,      # æ‰¹æ¬¡å¤§å°\n",
    "    shuffle=True,       # æ‰“ä¹±æ•°æ®\n",
    "    num_workers=4,      # å¤šçº¿ç¨‹åŠ è½½\n",
    "    #pin_memory=True     # åŠ é€ŸGPUä¼ è¾“\n",
    ")\n",
    "\n",
    "# éå†æ•°æ®\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    print(f\"Batch data shape: {batch_data.shape}, Batch labels shape: {batch_labels.shape}\")\n",
    "    # é€å…¥æ¨¡å‹è®­ç»ƒ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140d3c2-7bbb-4f19-94cc-000ba51e3bf2",
   "metadata": {},
   "source": [
    "#### å®Œæ•´çš„æç®€ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b3d80-8523-49ac-ad14-93f988816ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: å®šä¹‰ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼ˆç»§æ‰¿ Datasetï¼‰\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # éšæœºç”Ÿæˆ 100 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ 10 ç»´ç‰¹å¾å‘é‡\n",
    "        self.data = torch.randn(100, 10)\n",
    "        # éšæœºç”Ÿæˆæ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰\n",
    "        self.labels = torch.randint(0, 2, (100,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "# Step 2: å®ä¾‹åŒ–æ•°æ®é›†\n",
    "dataset = MyDataset()\n",
    "\n",
    "# Step 3: ç”¨ DataLoader å°è£…æ•°æ®é›†\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Step 4: æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­è¯»å–æ•°æ®\n",
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"  Features: {features.shape}\")  # åº”è¯¥æ˜¯ [8, 10]\n",
    "    print(f\"  Labels:   {labels}\")\n",
    "    \n",
    "    # æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ­¥éª¤\n",
    "    # output = model(features)\n",
    "    # loss = loss_fn(output, labels)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    if batch_idx == 2:  # æ¼”ç¤ºç”¨ï¼šåªæ‰“å°å‰ 3 ä¸ªæ‰¹æ¬¡\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f7da7-3dc9-469c-9c41-78afdea14a0a",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨è‡ªå®šä¹‰ä¹‰collate_fnå¤„ç†å˜é•¿åºåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1e63a-8200-4172-b9f8-c1e045724e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- 1. å®šä¹‰ TextDataset (æ¥è‡ªä¸Šä¸€ä¸ªæ•™ç¨‹) ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        # å‡è®¾æ–‡æœ¬å·²è½¬æ¢ä¸ºIDåºåˆ—çš„Tensor\n",
    "        self.texts = [torch.tensor(t, dtype=torch.long) for t in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# --- 2. å®šä¹‰è‡ªå®šä¹‰ collate_fn ---\n",
    "def custom_collate_fn_for_padding(batch):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰ collate_fn æ¥å¤„ç†å˜é•¿åºåˆ—å¡«å……ã€‚\n",
    "    Args:\n",
    "        batch (list): åŒ…å« (text_tensor, label) å…ƒç»„çš„åˆ—è¡¨ã€‚\n",
    "                      ä¾‹å¦‚ï¼š[(tensor([1,2,3]), 0), (tensor([4,5]), 1)]\n",
    "    Returns:\n",
    "        tuple: åŒ…å«å¡«å……åçš„æ–‡æœ¬æ‰¹æ¬¡å’Œæ ‡ç­¾æ‰¹æ¬¡çš„å…ƒç»„ã€‚\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*batch) # è§£åŒ…æ‰¹æ¬¡ï¼Œå¾—åˆ°æ–‡æœ¬åˆ—è¡¨å’Œæ ‡ç­¾åˆ—è¡¨\n",
    "\n",
    "    # è·å–æ‰¹æ¬¡ä¸­æœ€é•¿çš„æ–‡æœ¬åºåˆ—é•¿åº¦\n",
    "    max_len = max([len(t) for t in texts])\n",
    "\n",
    "    # å¯¹æ‰€æœ‰æ–‡æœ¬åºåˆ—è¿›è¡Œå¡«å……ï¼Œä½¿å…¶è¾¾åˆ° max_len\n",
    "    # å¡«å……å€¼é€šå¸¸é€‰æ‹©0æˆ–å…¶ä»–ç‰¹æ®Štoken_id\n",
    "    padded_texts = []\n",
    "    for t in texts:\n",
    "        padded_text = torch.cat([t, torch.zeros(max_len - len(t), dtype=t.dtype)])\n",
    "        padded_texts.append(padded_text)\n",
    "\n",
    "    # å°†å¡«å……åçš„æ–‡æœ¬åºåˆ—å †å æˆä¸€ä¸ªæ‰¹æ¬¡Tensor\n",
    "    texts_batch = torch.stack(padded_texts)\n",
    "    labels_batch = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return texts_batch, labels_batch\n",
    "\n",
    "# --- 3. å‡†å¤‡æ¨¡æ‹Ÿå˜é•¿æ–‡æœ¬æ•°æ® ---\n",
    "mock_texts = [\n",
    "    [101, 200, 301, 405],\n",
    "    [101, 502],\n",
    "    [101, 603, 704, 805, 906, 107],\n",
    "    [101]\n",
    "] # å‡è®¾æ˜¯è¯IDåºåˆ—ï¼Œ101æ˜¯CLS token\n",
    "mock_text_labels = [0, 1, 0, 1]\n",
    "\n",
    "# --- 4. åˆ›å»º Dataset å®ä¾‹ ---\n",
    "text_dataset = TextDataset(mock_texts, mock_text_labels)\n",
    "\n",
    "# --- 5. åˆ›å»º DataLoader å®ä¾‹ï¼Œå¹¶ä¼ å…¥ custom_collate_fn_for_padding ---\n",
    "text_loader = DataLoader(\n",
    "    dataset=text_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False, # ä¸ºäº†æ–¹ä¾¿è§‚å¯Ÿå¡«å……æ•ˆæœï¼Œä¸æ´—ç‰Œ\n",
    "    collate_fn=custom_collate_fn_for_padding # ä½¿ç”¨è‡ªå®šä¹‰çš„åˆå¹¶å‡½æ•°\n",
    ")\n",
    "\n",
    "# --- 6. è¿­ä»£ DataLoader å¹¶è§‚å¯Ÿè¾“å‡º ---\n",
    "print(\"\\n--- Using DataLoader with custom collate_fn for variable-length sequences ---\")\n",
    "for batch_idx, (text_batch, label_batch) in enumerate(text_loader):\n",
    "    print(f\"\\nBatch {batch_idx}:\")\n",
    "    print(f\"  Text batch shape: {text_batch.shape}\")\n",
    "    print(f\"  Text batch content:\\n{text_batch}\")\n",
    "    print(f\"  Label batch: {label_batch.tolist()}\")\n",
    "\n",
    "    # é¢„æœŸè¾“å‡ºï¼š\n",
    "    # Batch 0: (æœ€å¤§é•¿åº¦ä¸º 4)\n",
    "    #   Text batch shape: torch.Size([2, 4])\n",
    "    #   Text batch content:\n",
    "    #   tensor([[101, 200, 301, 405],\n",
    "    #           [101, 502,   0,   0]])\n",
    "    #   Label batch: [0, 1]\n",
    "    #\n",
    "    # Batch 1: (æœ€å¤§é•¿åº¦ä¸º 6)\n",
    "    #   Text batch shape: torch.Size([2, 6])\n",
    "    #   Text batch content:\n",
    "    #   tensor([[101, 603, 704, 805, 906, 107],\n",
    "    #           [101,   0,   0,   0,   0,   0]])\n",
    "    #   Label batch: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d25f72-554b-4c62-addc-94256539d62a",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨torchvision.datasetsç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6270605-f03c-4d1b-bfbe-b8c7f3fa5463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# å®šä¹‰æ•°æ®å¢å¼º\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # è½¬æ¢ä¸º Tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST å‡å€¼å’Œæ ‡å‡†å·®\n",
    "])\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',      # æ•°æ®å­˜å‚¨è·¯å¾„\n",
    "    train=True,         # è®­ç»ƒé›†\n",
    "    download=True,      # è‡ªåŠ¨ä¸‹è½½\n",
    "    transform=transform  # æ•°æ®å¢å¼º\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# éå†æ•°æ®\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68448d6-d7b7-437a-bbcd-698bd18d601d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MLPç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad85f5-4d65-4b68-bc89-ec318af8e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# è®¾å¤‡é…ç½®\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# å®šä¹‰ MLP æ¨¡å‹\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, output_size=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # å±•å¹³è¾“å…¥ (batch_size, 784)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # è¾“å‡ºå±‚ä¸åŠ æ¿€æ´»ï¼ˆç”±æŸå¤±å‡½æ•°å¤„ç†ï¼‰\n",
    "        return x\n",
    "\n",
    "# æ•°æ®å‡†å¤‡\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦\n",
    "        outputs = model(images)  # å‰å‘ä¼ æ’­\n",
    "        loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±\n",
    "        loss.backward()  # åå‘ä¼ æ’­\n",
    "        optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # æµ‹è¯•\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(model.state_dict(), 'mlp_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d01686-1633-4718-8f53-30d905f1c82e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ¨¡å‹å‚æ•°åˆå§‹åŒ–\n",
    "\n",
    "ä¸‹é¢ç”¨PyTorchæ¼”ç¤ºä¸€ä¸ªå®Œæ•´çš„â€œå‚æ•°åˆå§‹åŒ–â€çš„ä»£ç æµç¨‹ï¼Œå†…å®¹åŒ…æ‹¬ï¼š\n",
    "- æ„å»ºä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆåŒ…å«Linearå’ŒBatchNormå±‚ï¼‰\n",
    "- æŸ¥çœ‹é»˜è®¤çš„åˆå§‹åŒ–ç»“æœ\n",
    "- è‡ªå®šä¹‰åˆå§‹åŒ–æ–¹æ³•ï¼ˆå¦‚Xavierå’ŒKaimingï¼‰\n",
    "- åº”ç”¨è‡ªå®šä¹‰åˆå§‹åŒ–å¹¶éªŒè¯æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f1132-bcb1-4118-b4ba-4499027d0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šå®šä¹‰ä¸€ä¸ªç®€å•çš„MLPæ¨¡å‹\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 32)      # è¾“å…¥å±‚ -> éšè—å±‚\n",
    "        self.bn1 = nn.BatchNorm1d(32)     # æ‰¹å½’ä¸€åŒ–\n",
    "        self.relu = nn.ReLU()             # æ¿€æ´»å‡½æ•°\n",
    "        self.fc2 = nn.Linear(32, 2)       # éšè—å±‚ -> è¾“å‡ºå±‚ï¼ˆ2ç±»ï¼‰\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "model = SimpleMLP()\n",
    "\n",
    "# ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹é»˜è®¤åˆå§‹åŒ–çš„å‚æ•°åˆ†å¸ƒ\n",
    "print(\"é»˜è®¤åˆå§‹åŒ–å‚æ•°ï¼ˆå‡å€¼ & æ ‡å‡†å·®ï¼‰:\")\n",
    "for name, param in model.named_parameters():\n",
    "    #print(name, param)\n",
    "    if 'weight' in name or 'bias' in name:\n",
    "        print(f\"{name:20s} | mean = {param.mean():.4f}, std = {param.std():.4f}\")\n",
    "\n",
    "# ç¬¬å››æ­¥ï¼šå®šä¹‰è‡ªå®šä¹‰åˆå§‹åŒ–å‡½æ•°\n",
    "def custom_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)     # ä½¿ç”¨ Xavier å‡åŒ€åˆå§‹åŒ–æƒé‡\n",
    "        # è¯´æ˜ï¼šå¯ç”¨ä»¥ä¸‹ä¸‰ç§åˆå§‹åŒ–æ–¹æ³•æ›¿ä»£ä¸Šé¢çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¿›è¡Œå¯¹æ¯”æµ‹è¯•\n",
    "        # nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # Kaimingåˆå§‹åŒ–ï¼ˆé€‚ç”¨äºReLUæ¿€æ´»ï¼‰\n",
    "        # nn.init.normal_(m.weight, mean=0.0, std=0.02)  # æ­£æ€åˆ†å¸ƒ\n",
    "        # nn.init.uniform_(m.weight, a=-0.1, b=0.1)   # å‡åŒ€åˆ†å¸ƒ\n",
    "        nn.init.constant_(m.bias, 0)          # åç½®åˆå§‹åŒ–ä¸º 0\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)        # Î³ = 1\n",
    "        nn.init.constant_(m.bias, 0)          # Î² = 0\n",
    "\n",
    "# åº”ç”¨è‡ªå®šä¹‰åˆå§‹åŒ–\n",
    "model.apply(custom_init)\n",
    "\n",
    "# ç¬¬äº”æ­¥ï¼šæŸ¥çœ‹åˆå§‹åŒ–åçš„å‚æ•°åˆ†å¸ƒ\n",
    "print(\"\\nè‡ªå®šä¹‰åˆå§‹åŒ–å‚æ•°ï¼ˆå‡å€¼ & æ ‡å‡†å·®ï¼‰:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name or 'bias' in name:\n",
    "        print(f\"{name:20s} | mean = {param.mean():.4f}, std = {param.std():.4f}\")\n",
    "\n",
    "# ç¬¬å…­æ­¥ï¼ˆå¯é€‰ï¼‰ï¼šéªŒè¯æ¨¡å‹è¾“å‡ºæ˜¯å¦æ­£å¸¸\n",
    "# æ„é€ ä¸€ä¸ªéšæœºè¾“å…¥batchï¼ˆå¤§å°ä¸º4ï¼Œç‰¹å¾ç»´åº¦ä¸º10ï¼‰\n",
    "x = torch.randn(4, 10)\n",
    "output = model(x)\n",
    "\n",
    "print(\"\\næ¨¡å‹è¾“å‡º:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606ba3a-2459-45af-8a4b-dc5fa88355a8",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¸­éœ€è¦è¿›è¡Œåˆå§‹åŒ–çš„ä½ç½®\n",
    "\n",
    "| å±‚ç±»å‹                     | éœ€è¦åˆå§‹åŒ–çš„å‚æ•°                                                                 | å¸¸è§åˆå§‹åŒ–æ–¹æ³•                                                                 |\n",
    "|----------------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| å…¨è¿æ¥å±‚                   | æƒé‡çŸ©é˜µ \\( W \\)ï¼ˆå½¢çŠ¶ï¼š$[n_{\\text{in}}, n_{\\text{out}}]$ï¼‰<br>åç½®å‘é‡ \\( b \\)ï¼ˆå½¢çŠ¶ï¼š$[n_{\\text{out}}]$ï¼‰ | æƒé‡ï¼šXavierï¼ˆtanh/sigmoidï¼‰ã€Heï¼ˆReLUï¼‰ã€æ­£äº¤åˆå§‹åŒ–<br>åç½®ï¼šé›¶åˆå§‹åŒ–æˆ–å°å¸¸æ•°ï¼ˆå¦‚0.01ï¼‰ |\n",
    "| å·ç§¯å±‚                     | å·ç§¯æ ¸ï¼ˆæƒé‡ï¼‰ï¼ˆå½¢çŠ¶ï¼š$[k_h, k_w, c_{\\text{in}}, c_{\\text{out}}]$ï¼‰<br>åç½®å‘é‡ï¼ˆå½¢çŠ¶ï¼š$[c_{\\text{out}}]$ï¼‰ | æƒé‡ï¼šHeï¼ˆReLUï¼‰ã€Xavier<br>åç½®ï¼šé›¶åˆå§‹åŒ–                                       |\n",
    "| å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN/LSTM/GRUï¼‰ | æƒé‡çŸ©é˜µï¼ˆå¦‚ $ W_{xh}, W_{hh} $ï¼Œå½¢çŠ¶ä¾å±‚è®¾è®¡ï¼‰<br>åç½®å‘é‡ï¼ˆå½¢çŠ¶ï¼š$[n_{\\text{hidden}}]$ï¼‰ | æƒé‡ï¼šæ­£äº¤åˆå§‹åŒ–ã€Xavier<br>åç½®ï¼šé›¶åˆå§‹åŒ–ï¼ŒLSTMé—å¿˜é—¨åç½®å¯ä¸º1                |\n",
    "| æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰      | æƒé‡çŸ©é˜µï¼ˆå¦‚ $ W_Q, W_K, W_V $ï¼Œå½¢çŠ¶ï¼š$[d_{\\text{model}}, d_k/d_v]$ï¼‰<br>åç½®å‘é‡ï¼ˆéƒ¨åˆ†å®ç°ï¼‰ | æƒé‡ï¼šXavierã€He<br>åç½®ï¼šé›¶åˆå§‹åŒ–                                              |\n",
    "| åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰         | åµŒå…¥çŸ©é˜µï¼ˆå½¢çŠ¶ï¼š$[n_{\\text{vocab}}, d_{\\text{emb}}]$ï¼‰                          | éšæœºåˆå§‹åŒ–ï¼ˆå‡åŒ€/æ­£æ€åˆ†å¸ƒï¼‰ã€é¢„è®­ç»ƒåµŒå…¥ï¼ˆå¦‚GloVeï¼‰ã€æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç             |\n",
    "| æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normï¼‰      | ç¼©æ”¾å‚æ•° $ \\gamma $ï¼ˆé€šå¸¸ä¸º1ï¼‰<br>åç§»å‚æ•° $ \\beta $ï¼ˆé€šå¸¸ä¸º0ï¼‰                | $ \\gamma \\sim 1 $ï¼Œ$ \\beta \\sim 0 $                                         |\n",
    "\n",
    "**å¤‡æ³¨**ï¼š\n",
    "- æ± åŒ–å±‚ã€æ¿€æ´»å‡½æ•°ã€Dropoutå±‚é€šå¸¸æ— éœ€åˆå§‹åŒ–å‚æ•°ã€‚\n",
    "- åˆå§‹åŒ–æ–¹æ³•éœ€æ ¹æ®æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUã€tanhï¼‰å’Œç½‘ç»œæ·±åº¦é€‰æ‹©ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab3c65-50a3-4cb3-9a2b-b02e2f0359d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ­£åˆ™åŒ–æŠ€æœ¯\n",
    "\n",
    "| æ­£åˆ™åŒ–æŠ€æœ¯ | å®šä¹‰ | ä½œç”¨ | ä¼˜ç‚¹ | ç¼ºç‚¹ | åº”ç”¨åœºæ™¯ |\n",
    "|------------|------|------|------|------|----------|\n",
    "| **L1æ­£åˆ™åŒ–** | åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥å‚æ•°çš„L1èŒƒæ•°ï¼ˆç»å¯¹å€¼å’Œï¼‰ | äº§ç”Ÿç¨€ç–è§£ï¼Œå®ç°ç‰¹å¾é€‰æ‹©ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦ | 1. ç¨€ç–æ¨¡å‹ä¾¿äºè§£é‡Š<br>2. é€‚åˆé«˜ç»´æ•°æ® | 1. éå…‰æ»‘ä¼˜åŒ–é—®é¢˜<br>2. å¯¹å™ªå£°æ•æ„Ÿ | çº¿æ€§æ¨¡å‹ï¼ˆå¦‚Lassoå›å½’ï¼‰ã€ç¨€ç–ç¥ç»ç½‘ç»œ |\n",
    "| **L2æ­£åˆ™åŒ–** | åœ¨æŸå¤±å‡½æ•°ä¸­åŠ å…¥å‚æ•°çš„L2èŒƒæ•°ï¼ˆå¹³æ–¹å’Œï¼‰ | å¹³æ»‘æƒé‡ï¼Œé™ä½å¯¹å™ªå£°çš„æ•æ„Ÿæ€§ | 1. ä¼˜åŒ–å…‰æ»‘ï¼Œä¾¿äºæ¢¯åº¦ä¸‹é™<br>2. å¢å¼ºæ¨¡å‹ç¨³å®šæ€§ | 1. ä¸äº§ç”Ÿç¨€ç–è§£<br>2. éœ€è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦ | çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œï¼ˆæƒé‡è¡°å‡ï¼‰ |\n",
    "| **Dropout** | è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒ | é˜²æ­¢è¿‡åº¦ä¾èµ–æŸäº›ç¥ç»å…ƒï¼Œæ¨¡æ‹Ÿæ¨¡å‹é›†æˆ | 1. ç®€å•é«˜æ•ˆ<br>2. å‡å°‘è¿‡æ‹Ÿåˆ | 1. å¢åŠ è®­ç»ƒæ—¶é—´<br>2. æµ‹è¯•æ—¶éœ€æƒé‡è°ƒæ•´ | æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚CNNã€RNNï¼‰ |\n",
    "| **æ•°æ®å¢å¼º** | å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå˜æ¢ï¼ˆå¦‚æ—‹è½¬ã€ç¿»è½¬ï¼‰ | å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ› | 1. ä¸æ”¹å˜æ¨¡å‹ç»“æ„<br>2. é€‚åˆè§†è§‰/è¯­éŸ³ä»»åŠ¡ | 1. éœ€è®¾è®¡åˆé€‚ç­–ç•¥<br>2. å¢åŠ é¢„å¤„ç†æˆæœ¬ | å›¾åƒåˆ†ç±»ã€è¯­éŸ³å¤„ç†ã€NLP |\n",
    "| **æ—©åœ** | éªŒè¯é›†è¯¯å·®ä¸å†ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ | é™åˆ¶è®­ç»ƒè½®æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ | 1. ç®€å•æ˜“å®ç°<br>2. æ— éœ€æ”¹æ¨¡å‹ | 1. éœ€åˆ’åˆ†éªŒè¯é›†<br>2. å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆ | æ·±åº¦å­¦ä¹ ã€æ¢¯åº¦æå‡æ ‘ |\n",
    "| **æ‰¹æ ‡å‡†åŒ–** | è§„èŒƒåŒ–æ¯å±‚è¾“å…¥çš„å‡å€¼å’Œæ–¹å·® | å‡å°‘å†…éƒ¨åå˜é‡åç§»ï¼Œç¨³å®šè®­ç»ƒ | 1. åŠ é€Ÿæ”¶æ•›<br>2. å‡å°‘å¯¹Dropoutä¾èµ– | 1. å°æ‰¹é‡æ•ˆæœå·®<br>2. å¢åŠ è®¡ç®—å¼€é”€ | æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆCNNã€å…¨è¿æ¥ç½‘ç»œï¼‰ |\n",
    "| **å¼¹æ€§ç½‘æ­£åˆ™åŒ–** | ç»“åˆL1å’ŒL2æ­£åˆ™åŒ– | ç»¼åˆç¨€ç–æ€§å’Œå¹³æ»‘æ€§ | 1. å…¼å…·ç‰¹å¾é€‰æ‹©å’Œå¹³æ»‘<br>2. é€‚åˆå¤æ‚ç‰¹å¾ç›¸å…³æ€§ | 1. éœ€è°ƒä¸¤ä¸ªè¶…å‚æ•°<br>2. å¢åŠ è°ƒå‚éš¾åº¦ | é«˜ç»´ç‰¹å¾å›å½’ã€ç¨€ç–çº¿æ€§æ¨¡å‹ |\n",
    "| **å™ªå£°æ³¨å…¥** | åœ¨æ•°æ®/æƒé‡ä¸­æ·»åŠ éšæœºå™ªå£° | å¢å¼ºæ¨¡å‹é²æ£’æ€§ | 1. ç®€å•æ˜“å®ç°<br>2. æé«˜æŠ—å™ªèƒ½åŠ› | 1. éœ€è°ƒæ•´å™ªå£°æ°´å¹³<br>2. å¯èƒ½ç ´åæ•°æ®ç»“æ„ | è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ã€æ—¶é—´åºåˆ— |\n",
    "| **å‚æ•°å…±äº«** | å¼ºåˆ¶æ¨¡å‹å…±äº«éƒ¨åˆ†å‚æ•°ï¼ˆå¦‚å·ç§¯æ ¸ï¼‰ | å‡å°‘è‡ªç”±å‚æ•°ï¼Œé™ä½å¤æ‚åº¦ | 1. é™ä½è®¡ç®—é‡<br>2. é€‚åˆç»“æ„åŒ–æ•°æ® | 1. ä»…é™ç‰¹å®šæ¨¡å‹<br>2. é€‚ç”¨èŒƒå›´æœ‰é™ | å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0616aec-e8f0-4238-917e-0a7d02df0513",
   "metadata": {},
   "source": [
    "### L1å’ŒL2æ­£åˆ™åŒ–ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe429c62-4bff-4008-bc2c-f08ebf9b46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# å®šä¹‰ç®€å•æ¨¡å‹\n",
    "model = nn.Linear(10, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰\n",
    "\n",
    "# è®­ç»ƒæ•°æ®\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randn(32, 1)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "\n",
    "# æ‰‹åŠ¨æ·»åŠ  L1 æ­£åˆ™åŒ–\n",
    "l1_lambda = 0.01\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())  # è®¡ç®—L1èŒƒæ•°\n",
    "loss += l1_lambda * l1_norm\n",
    "\n",
    "# åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92120618-5ce8-4210-8c09-d07e930a84cf",
   "metadata": {},
   "source": [
    "### Dropoutç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fba890-1595-469b-a6cb-ff237ec0a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰å¸¦ Dropout çš„æ¨¡å‹\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # 50% æ¦‚ç‡ä¸¢å¼ƒ\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # åº”ç”¨ Dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "model = Net()\n",
    "model.train()  # å¯ç”¨ Dropout\n",
    "x = torch.randn(32, 10)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59596db2-9965-44ec-a0ff-540b2c9ae861",
   "metadata": {},
   "source": [
    "### Batch Normalizationç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213aa06-2cf6-4c7e-94a5-676e082f7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰å¸¦æ‰¹æ ‡å‡†åŒ–çš„æ¨¡å‹\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(16)  # æ‰¹æ ‡å‡†åŒ–ï¼Œ16ä¸ªé€šé“\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(16 * 32 * 32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)  # åº”ç”¨æ‰¹æ ‡å‡†åŒ–\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "model = Net()\n",
    "model.train()  # å¯ç”¨æ‰¹æ ‡å‡†åŒ–\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542700a2-b908-49cb-9677-88d331dc2a03",
   "metadata": {},
   "source": [
    "### æ—©åœæ³•ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a741581-73ae-4548-a33f-451f89e78815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹å’Œæ•°æ®\n",
    "model = nn.Linear(10, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_loader = ...  # å‡è®¾å·²æœ‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "val_loader = ...\n",
    "\n",
    "# æå‰åœæ­¢å‚æ•°\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # éªŒè¯\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            output = model(x)\n",
    "            val_loss += criterion(output, y).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # æå‰åœæ­¢é€»è¾‘\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb82c27-ddde-4464-9c3e-8fb291b4e9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ¨¡å‹ä¿å­˜ä¸åŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1517d-da0b-4df8-9cf1-24c0a4285de7",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b5e2a-78e5-471a-a8ad-8767ebac4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œ (FNN)\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # å…¨è¿æ¥å±‚ 1\n",
    "        self.relu = nn.ReLU()                         # æ¿€æ´»å‡½æ•° (æ²¡æœ‰å‚æ•°)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # å…¨è¿æ¥å±‚ 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "model = SimpleFNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# è·å–æ¨¡å‹çš„ state_dict\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# æ‰“å° state_dict çš„ç±»å‹\n",
    "print(f\"State dict type: {type(model_state_dict)}\")\n",
    "\n",
    "# æ‰“å° state_dict ä¸­çš„é”®å’Œå¯¹åº”çš„å¼ é‡å½¢çŠ¶\n",
    "print(\"\\nKeys and shapes in state_dict:\")\n",
    "for key, value in model_state_dict.items():\n",
    "    print(f\"Key: {key}, Shape: {value.shape}\")\n",
    "\n",
    "# æ‰“å°å…¶ä¸­ä¸€ä¸ªå‚æ•°çš„å¼ é‡å†…å®¹ (ä»…éƒ¨åˆ†å±•ç¤º)\n",
    "print(f\"\\nContent of '{list(model_state_dict.keys())[0]}':\")\n",
    "print(model_state_dict[list(model_state_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975eaac6-4ec2-4d45-b048-410849b62f71",
   "metadata": {},
   "source": [
    "### æŸ¥çœ‹æŒç»­ç¼“å†²åŒº\n",
    "\n",
    "ç»™ä¸Šé¢çš„ç¤ºä¾‹æ·»åŠ ä¸€ä¸ªBatchNormå±‚ï¼Œstate_dictå­—å…¸ä¸­åœ¨å¯å­¦ä¹ å‚æ•°ä¹‹å¤–ï¼Œè¿˜ä¼šå¤šå‡ºæŒä¹…ç¼“å†²åŒºã€‚\n",
    "\n",
    "ä¸‹é¢ç¤ºä¾‹ä¸­çš„bn1å±‚ä¸ä»…æœ‰å¯å­¦ä¹ å‚æ•°weightå’Œbiasï¼ˆå¯¹åº”BatchNormçš„$\\gamma$å’Œ$\\beta$å‚æ•°ï¼Œå®ƒä»¬æ˜¯å¯å­¦ä¹ çš„ï¼‰ï¼Œè¿˜æœ‰ä¸‰ä¸ªç¼“å†²åŒºï¼šrunning_meanã€running_varå’Œnum_batches_trackedã€‚è¿™äº›ç¼“å†²åŒºæ˜¯æ¨¡å‹çŠ¶æ€çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œåœ¨æ¨ç†æ—¶éœ€è¦ä½¿ç”¨ï¼Œå› æ­¤ä¹ŸåŒ…å«åœ¨state_dictä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df4b0f-a310-4152-b6d9-845c8fafa460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FNNWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # BatchNorm 1D å¸¸ç”¨äºå…¨è¿æ¥å±‚ä¹‹å\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x) # åº”ç”¨ BatchNorm\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "model_bn = FNNWithBatchNorm(input_size, hidden_size, output_size)\n",
    "\n",
    "# è·å–æ¨¡å‹çš„ state_dict\n",
    "model_bn_state_dict = model_bn.state_dict()\n",
    "\n",
    "# æ‰“å° state_dict ä¸­çš„é”®å’Œå¯¹åº”çš„å¼ é‡å½¢çŠ¶\n",
    "print(\"\\nKeys and shapes in state_dict (with BatchNorm):\")\n",
    "for key, value in model_bn_state_dict.items():\n",
    "    print(f\"Key: {key}, Shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c216b9-067c-4bab-9342-9a83305bf856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ä¿å­˜å’ŒåŠ è½½state_dictçš„å®Œæ•´çš„ç¤ºä¾‹\n",
    "\n",
    "#### ä¿å­˜state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf247d46-84d8-4f4f-96e8-2a840cc606eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os # ç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "\n",
    "# å‡è®¾æˆ‘ä»¬å·²ç»å®šä¹‰å¹¶ï¼ˆå¯èƒ½ï¼‰è®­ç»ƒäº†ä¸€ä¸ª SimpleFNN æ¨¡å‹å®ä¾‹\n",
    "# ï¼ˆè¿™é‡Œæˆ‘ä»¬ä¸ºäº†æ¼”ç¤ºï¼Œåªå®ä¾‹åŒ–ï¼Œä¸è¿›è¡Œå®é™…è®­ç»ƒï¼‰\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "model = SimpleFNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# å®šä¹‰ä¿å­˜æ–‡ä»¶çš„è·¯å¾„\n",
    "model_params_path = 'simple_fnn_params.pth' # å¸¸ç”¨çš„æ–‡ä»¶æ‰©å±•åæ˜¯ .pt æˆ– .pth\n",
    "\n",
    "# --- ä¿å­˜æ¨¡å‹çš„ state_dict ---\n",
    "print(f\"Saving model state_dict to {model_params_path}...\")\n",
    "try:\n",
    "    # è·å–æ¨¡å‹çš„ state_dict\n",
    "    state_dict_to_save = model.state_dict()\n",
    "\n",
    "    # ä½¿ç”¨ torch.save() ä¿å­˜ state_dict\n",
    "    torch.save(state_dict_to_save, model_params_path)\n",
    "\n",
    "    print(\"Model state_dict saved successfully!\")\n",
    "    print(f\"Checking if file exists: {os.path.exists(model_params_path)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6b4d0-5cf4-4241-b0c5-ba569bbf9765",
   "metadata": {},
   "source": [
    "#### åŠ è½½state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5ad36-ed0a-42dd-b287-733007d2f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- åŠ è½½ state_dict ---\n",
    "print(f\"\\nLoading state_dict from {model_params_path}...\")\n",
    "try:\n",
    "    # ä½¿ç”¨ torch.load() åŠ è½½ state_dict\n",
    "    loaded_state_dict = torch.load(model_params_path)\n",
    "\n",
    "    print(\"State dict loaded successfully!\")\n",
    "    print(f\"Type of loaded object: {type(loaded_state_dict)}\")\n",
    "\n",
    "    # æˆ‘ä»¬å¯ä»¥åƒæŸ¥çœ‹åŸå§‹ state_dict ä¸€æ ·æŸ¥çœ‹åŠ è½½çš„å­—å…¸\n",
    "    print(\"\\nKeys in loaded state_dict:\")\n",
    "    for key in loaded_state_dict.keys():\n",
    "        print(key)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{model_params_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading: {e}\")\n",
    "\n",
    "# --- å°†åŠ è½½çš„ state_dict è½½å…¥åˆ°æ–°çš„æ¨¡å‹å®ä¾‹ ---\n",
    "print(\"\\nLoading state_dict into a new model instance...\")\n",
    "\n",
    "# !!! å…³é”®æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹ !!!\n",
    "# è¿™ä¸ªå®ä¾‹çš„ç»“æ„ (å±‚ã€å±‚çš„å¤§å°ã€å±‚ä¹‹é—´çš„è¿æ¥) å¿…é¡»ä¸ä¿å­˜ state_dict æ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´\n",
    "new_model = SimpleFNN(input_size, hidden_size, output_size)\n",
    "print(\"New model instance created.\")\n",
    "\n",
    "try:\n",
    "    # !!! å…³é”®æ­¥éª¤ 2: å°†åŠ è½½çš„state_dictè½½å…¥åˆ°æ–°æ¨¡å‹å®ä¾‹ä¸­ !!!\n",
    "    # load_state_dict å°†å­—å…¸ä¸­çš„å‚æ•°å¤åˆ¶åˆ°æ¨¡å‹çš„å¯¹åº”ä½ç½®\n",
    "    # é»˜è®¤æƒ…å†µä¸‹ (strict=True), åŠ è½½çš„state_dictçš„é”®å¿…é¡»ç²¾ç¡®åŒ¹é…æ¨¡å‹å®ä¾‹çš„state_dict\n",
    "    new_model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "    print(\"State dict loaded into new model instance successfully!\")\n",
    "\n",
    "    # åŠ è½½å®Œæˆåï¼Œæ–°æ¨¡å‹çš„å‚æ•°å°±å’Œä¿å­˜æ—¶çš„æ¨¡å‹å‚æ•°å®Œå…¨ä¸€æ ·äº†\n",
    "\n",
    "except RuntimeError as e:\n",
    "    # å¦‚æœæ¨¡å‹ç»“æ„ä¸åŒ¹é…ï¼Œload_state_dictä¼šæŠ›å‡ºRuntimeError\n",
    "    print(f\"Error loading state dict: Model structure mismatch. Details:\\n{e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6069f7-767c-4b01-9046-518f19f43d51",
   "metadata": {},
   "source": [
    "#### æ¨ç†æ¨¡å¼çš„ç‰¹æ®Šè®¾å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8006433-9506-4e2c-b9e5-e903fe192be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»§ç»­ä¸Šé¢çš„ä¾‹å­...\n",
    "\n",
    "# å°†åŠ è½½å‚æ•°åçš„æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "new_model.eval()\n",
    "print(\"\\nModel set to evaluation mode.\")\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€ä¸ªè¾“å…¥å¼ é‡ (é€šå¸¸æ˜¯ä¸€ä¸ª batch çš„æ•°æ®)\n",
    "dummy_input = torch.randn(1, input_size) # batch size = 1\n",
    "\n",
    "# åœ¨ no_grad ä¸Šä¸‹æ–‡ä¸‹è¿›è¡Œæ¨ç†\n",
    "print(\"Performing inference with the loaded model...\")\n",
    "with torch.no_grad():\n",
    "    output = new_model(dummy_input)\n",
    "\n",
    "print(\"Inference successful.\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "# æ¨¡å‹çš„è¾“å‡ºç°åœ¨æ˜¯åŸºäºåŠ è½½çš„å‚æ•°è®¡ç®—å¾—åˆ°çš„ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb15049-c58e-4250-9ae3-a11d1c8effff",
   "metadata": {},
   "source": [
    "#### state_dictçš„ä¸¥æ ¼æ¨¡å¼ä¸éä¸¥æ ¼æ¨¡å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cbe99-82b7-49bb-a5e9-d1464d96683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å®šä¹‰ FNN ç‰ˆæœ¬ 1\n",
    "class SimpleFNN_v1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFNN_v1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®šä¹‰ FNN ç‰ˆæœ¬ 2 (æ¯”ç‰ˆæœ¬ 1 å¤šä¸€å±‚)\n",
    "class SimpleFNN_v2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, intermediate_size, output_size):\n",
    "        super(SimpleFNN_v2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # æ³¨æ„è¿™é‡Œçš„è¾“å‡ºå±‚å¤§å°ä¸SimpleFNN_v1æœ‰æ‰€å·®åˆ«ï¼Œå³ä½¿stirct=Falseï¼Œ\n",
    "        # ä»ä¼šå¯¼è‡´åŠ è½½å¤±è´¥ï¼Œå› ä¸ºéä¸¥æ ¼æ¨¡å¼å¹¶ä¸èƒ½æ¥å—å½¢çŠ¶ä¸Šçš„ä¸å…¼å®¹ï¼Œæ‰€ä»¥éœ€è¦åŠ è½½å‰è¿›è¡Œç‰¹æ®Šå¤„ç†ã€‚\n",
    "        self.fc2 = nn.Linear(hidden_size, intermediate_size) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(intermediate_size, output_size) # æ–°å¢ä¸€å±‚\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# æ¨¡å‹å‚æ•°\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "intermediate_size = 15\n",
    "output_size = 3\n",
    "\n",
    "# å®ä¾‹åŒ–å¹¶ä¿å­˜ v1 æ¨¡å‹çš„ state_dict\n",
    "model_v1 = SimpleFNN_v1(input_size, hidden_size, output_size)\n",
    "v1_state_dict = model_v1.state_dict()\n",
    "\n",
    "# å®ä¾‹åŒ– v2 æ¨¡å‹\n",
    "model_v2 = SimpleFNN_v2(input_size, hidden_size, intermediate_size, output_size)\n",
    "\n",
    "print(\"Loading V1 state_dict into V2 model with strict=False and checking return info...\")\n",
    "# è·å–ç›®æ ‡æ¨¡å‹ (v2) çš„ state_dictï¼Œç”¨äºæ£€æŸ¥å½¢çŠ¶\n",
    "v2_model_dict = model_v2.state_dict()\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ–°çš„ state_dictï¼ŒåªåŒ…å« v1 ä¸­å­˜åœ¨ä¸”åœ¨ v2 ä¸­å½¢çŠ¶åŒ¹é…çš„é”®å€¼å¯¹\n",
    "# è¿™æ˜¯è§£å†³ size mismatch é—®é¢˜çš„å…³é”®\n",
    "filtered_state_dict = {k: v for k, v in v1_state_dict.items() if\n",
    "                       k in v2_model_dict and v.shape == v2_model_dict[k].shape}\n",
    "\n",
    "print(\"\\nFiltered state_dict keys to load (matching shapes):\")\n",
    "if filtered_state_dict:\n",
    "    for k in filtered_state_dict.keys():\n",
    "        print(f\"  {k}\")\n",
    "else:\n",
    "    print(\"  No keys with matching names and shapes found.\")\n",
    "\n",
    "# ä½¿ç”¨ strict=False åŠ è½½è¿‡æ»¤åçš„ state_dictï¼Œæ­¤æ—¶ä¸ä¼šå†æœ‰ size mismatchï¼Œå› ä¸ºæˆ‘ä»¬åªåŠ è½½å½¢çŠ¶åŒ¹é…çš„å±‚ã€‚\n",
    "# strict=False ä¼šå¿½ç•¥ v2 ä¸­å­˜åœ¨ä½† filtered_state_dict ä¸­ä¸å­˜åœ¨çš„é”® (fc2, fc3)ã€‚\n",
    "try:\n",
    "    load_info = model_v2.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "    print(\"\\nLoading completed successfully.\")\n",
    "    # load_info ä¼šåŒ…å«åŠ è½½è¿‡ç¨‹ä¸­è¢«å¿½ç•¥çš„é”® (missing_keys) å’Œ state_dict ä¸­å­˜åœ¨ä½†åœ¨æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„é”® (unexpected_keys)ã€‚\n",
    "    # åœ¨è¿™é‡Œï¼Œfc2 å’Œ fc3 ä¼šæ˜¯ missing_keysï¼Œå› ä¸ºå®ƒä»¬åœ¨ model_v2 ä¸­ï¼Œä½†ä¸åœ¨ filtered_state_dict ä¸­ã€‚\n",
    "    print(f\"Missing keys (exist in model_v2 but not in filtered_state_dict): {load_info.missing_keys}\")\n",
    "    # unexpected_keys åº”è¯¥ä¸ºç©ºï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è¿‡æ»¤æ‰äº†æ‰€æœ‰ä¸åœ¨ model_v2 ä¸­çš„é”®\n",
    "    print(f\"Unexpected keys (exist in filtered_state_dict but not in model_v2): {load_info.unexpected_keys}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # ç°åœ¨åº”è¯¥ä¸ä¼šå› ä¸º size mismatch è€Œè¿›å…¥è¿™ä¸ª except å—äº†\n",
    "    print(f\"An unexpected error occurred during loading: {e}\")\n",
    "    # åœ¨å¼‚å¸¸å‘ç”Ÿæ—¶ï¼Œload_info å¯èƒ½æœªå®šä¹‰ï¼Œè¿™é‡Œä¸ºäº†å®‰å…¨ï¼Œå¯ä»¥æ•è·ä¸€ä¸‹æˆ–è€…åœ¨delå‰æ£€æŸ¥\n",
    "    load_info = None # ç¡®ä¿å³ä½¿å‡ºé”™ load_info ä¹Ÿæœ‰ä¸€ä¸ªå€¼ï¼Œé¿å… NameError\n",
    "\n",
    "# æ¸…ç†\n",
    "del load_info, model_v1, model_v2, v1_state_dict, filtered_state_dict, v2_model_dict\n",
    "print(\"\\nCleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f1562-e2b5-4e35-a6f4-34356df2bfa8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹\n",
    "\n",
    "#### æ¨¡å‹æ•´ä¸ªæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec94f60-fc19-4e0b-bd9b-be9117b7e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os # ç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "\n",
    "# å‡è®¾æˆ‘ä»¬å·²ç»å®šä¹‰å¹¶å®ä¾‹åŒ–äº†ä¸€ä¸ª SimpleFNN æ¨¡å‹\n",
    "# ï¼ˆè¿™é‡Œæˆ‘ä»¬ä¸ºäº†æ¼”ç¤ºï¼Œåªå®ä¾‹åŒ–ï¼‰\n",
    "\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹å‚æ•°\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "\n",
    "# å®ä¾‹åŒ–è¦ä¿å­˜çš„æ¨¡å‹\n",
    "model_to_save = SimpleFNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# å®šä¹‰ä¿å­˜æ•´ä¸ªæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„\n",
    "entire_model_path = 'entire_simple_fnn_model.pth' # ä¸åŒçš„æ–‡ä»¶åä»¥ç¤ºåŒºåˆ«\n",
    "\n",
    "# --- ä¿å­˜æ•´ä¸ªæ¨¡å‹å®ä¾‹ ---\n",
    "print(f\"Saving entire model instance to {entire_model_path}...\")\n",
    "try:\n",
    "    # ä½¿ç”¨ torch.save() ä¿å­˜æ•´ä¸ªæ¨¡å‹å¯¹è±¡\n",
    "    torch.save(model_to_save, entire_model_path)\n",
    "\n",
    "    print(\"Entire model instance saved successfully!\")\n",
    "    print(f\"Checking if file exists: {os.path.exists(entire_model_path)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d1c28-9b26-461e-99eb-a54ee1577b5e",
   "metadata": {},
   "source": [
    "#### åŠ è½½æ•´ä¸ªæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccdeb9-98f7-4681-80de-b8947e7b2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- åŠ è½½æ•´ä¸ªæ¨¡å‹å®ä¾‹ ---\n",
    "print(f\"\\nLoading entire model instance from {entire_model_path}...\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨torch.load()ç›´æ¥åŠ è½½æ•´ä¸ªæ¨¡å‹å¯¹è±¡\n",
    "    # æ³¨æ„ï¼šPytorch 2.6ç‰ˆæœ¬ä¹‹å‰ï¼Œweights_onlyå‚æ•°çš„é»˜è®¤å€¼ä¸ºFalse,ä½†è¯¥ç‰ˆæœ¬ä¹‹åçš„é»˜è®¤å€¼ä¸ºTrueï¼Œ\n",
    "    # å› æ­¤ï¼Œè‹¥è¦åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œéœ€è¦æ‰‹åŠ¨å°†å…¶è®¾ç½®ä¸ºFalseã€‚\n",
    "    loaded_model_instance = torch.load(entire_model_path, weights_only=False)\n",
    "\n",
    "    print(\"Entire model instance loaded successfully!\")\n",
    "    print(f\"Type of loaded object: {type(loaded_model_instance)}\")\n",
    "\n",
    "    # æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨åŠ è½½åçš„æ¨¡å‹å®ä¾‹è¿›è¡Œæ¨ç†\n",
    "    print(\"\\nLoaded model instance created.\")\n",
    "\n",
    "    # åŠ è½½å®Œæˆåï¼Œæ¨¡å‹å·²ç»åŒ…å«ç»“æ„å’Œå‚æ•°\n",
    "\n",
    "    # ç±»ä¼¼ state_dict æ–¹æ³•ï¼ŒåŠ è½½ç”¨äºæ¨ç†çš„æ¨¡å‹é€šå¸¸éœ€è¦è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    loaded_model_instance.eval()\n",
    "    print(\"Loaded model set to evaluation mode.\")\n",
    "\n",
    "    # æ¨¡æ‹Ÿè¾“å…¥è¿›è¡Œæ¨ç†\n",
    "    dummy_input = torch.randn(1, input_size)\n",
    "    with torch.no_grad():\n",
    "        output = loaded_model_instance(dummy_input)\n",
    "\n",
    "    print(\"Inference successful with loaded entire model.\")\n",
    "    print(f\"Inference output shape: {output.shape}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{entire_model_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    # !!! æ³¨æ„ï¼šå¦‚æœæ¨¡å‹ç±»å®šä¹‰ä¸å¯ç”¨æˆ–ä¸åŒ¹é…ï¼Œè¿™é‡Œä¼šæŠ›å‡ºå„ç§é”™è¯¯ !!!\n",
    "    print(f\"An error occurred while loading or inferring: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9785d4-051a-4a8b-bcd1-9f781a895789",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶ç¤ºä¾‹\n",
    "\n",
    "#### æ„å»ºæ£€æŸ¥ç‚¹å­—å…¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af13ce-9f2b-47ba-aac9-e8f4c22d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å¯¹è±¡å’Œå˜é‡\n",
    "# model: å½“å‰è®­ç»ƒä¸­çš„æ¨¡å‹å®ä¾‹\n",
    "# optimizer: å½“å‰è®­ç»ƒä¸­çš„ä¼˜åŒ–å™¨å®ä¾‹\n",
    "# epoch: å½“å‰å®Œæˆçš„ epoch æ•°\n",
    "# step: å½“å‰å®Œæˆçš„æ€»æ­¥æ•° (å¯é€‰)\n",
    "# loss: å½“å‰çš„è®­ç»ƒ loss (å¯é€‰)\n",
    "# best_metric: å†å²æœ€ä½³è¯„ä¼°æŒ‡æ ‡ (å¯é€‰)\n",
    "# scheduler: å½“å‰ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹ (å¦‚æœä½¿ç”¨çš„è¯)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ç¤ºä¾‹ï¼šå‡è®¾æˆ‘ä»¬å·²ç»å®šä¹‰äº†æ¨¡å‹ã€ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”è¿›è¡Œäº†ä¸€æ®µè®­ç»ƒ\n",
    "class SimpleFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨ (è¿™é‡Œåªæ˜¯ä¸ºäº†æ„å»º state_dictï¼Œä¸è¿›è¡Œå®é™…è®­ç»ƒ)\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 3\n",
    "lr=0.001\n",
    "model = SimpleFNN(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€äº›è®­ç»ƒçŠ¶æ€\n",
    "current_epoch = 5\n",
    "current_step = 500\n",
    "current_loss = 0.15\n",
    "best_validation_accuracy = 0.88\n",
    "\n",
    "# å¦‚æœä½¿ç”¨äº†å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä¹Ÿè·å–å…¶çŠ¶æ€å­—å…¸\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "# scheduler_state = scheduler.state_dict() # éœ€è¦å…ˆå®šä¹‰å’Œåˆå§‹åŒ– scheduler\n",
    "\n",
    "# --- æ„å»ºæ£€æŸ¥ç‚¹å­—å…¸ ---\n",
    "checkpoint = {\n",
    "    'epoch': current_epoch,\n",
    "    'step': current_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': current_loss,\n",
    "    'best_accuracy': best_validation_accuracy,\n",
    "    # 'scheduler_state_dict': scheduler_state, # å¦‚æœä½¿ç”¨è°ƒåº¦å™¨åˆ™åŒ…å«æ­¤é¡¹\n",
    "    # 'torch_rng_state': torch.get_rng_state(), # å¦‚æœéœ€è¦ä¸¥æ ¼å¤ç°ï¼Œä¿å­˜ RNG çŠ¶æ€\n",
    "    # 'numpy_rng_state': np.random.get_state(), # å¦‚æœä½¿ç”¨ NumPyï¼Œä¿å­˜ RNG çŠ¶æ€\n",
    "    # 'random_rng_state': random.getstate(),  # å¦‚æœä½¿ç”¨ Python randomï¼Œä¿å­˜ RNG çŠ¶æ€\n",
    "}\n",
    "\n",
    "print(\"Checkpoint dictionary built:\")\n",
    "print(checkpoint.keys())\n",
    "# æ‰“å°å…¶ä¸­ä¸€ä¸ª state_dict çš„é”®æ¥ç¡®è®¤\n",
    "print(\"\\nKeys in model_state_dict within checkpoint:\")\n",
    "print(checkpoint['model_state_dict'].keys())\n",
    "\n",
    "# æ³¨æ„ï¼šå¦‚æœæ¨¡å‹å’Œä¼˜åŒ–å™¨æ²¡æœ‰è¿›è¡Œè¿‡å®é™…çš„è®­ç»ƒæ­¥éª¤ï¼Œå®ƒä»¬çš„ state_dict å¯èƒ½ä¸å®Œæ•´ï¼ˆç‰¹åˆ«æ˜¯ä¼˜åŒ–å™¨ï¼‰\n",
    "# åœ¨å®é™…è®­ç»ƒä¸­ï¼Œç¡®ä¿åœ¨ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ï¼Œæ¨¡å‹å’Œä¼˜åŒ–å™¨å·²ç»è¿›è¡Œäº†è‡³å°‘ä¸€æ­¥æ›´æ–°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abb967-3697-4039-ae5b-59836549fab4",
   "metadata": {},
   "source": [
    "#### ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8939459-eca9-41cd-9de8-c562488b82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»§ç»­ä¸Šé¢çš„ä¾‹å­...\n",
    "\n",
    "# å®šä¹‰æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜è·¯å¾„\n",
    "checkpoint_path = 'my_training_checkpoint.pth' # æˆ–è€…æ ¹æ® epoch/æ€§èƒ½å‘½å\n",
    "\n",
    "# --- ä¿å­˜æ£€æŸ¥ç‚¹å­—å…¸ ---\n",
    "print(f\"\\nä¿å­˜æ£€æŸ¥ç‚¹ä¿¡æ¯è‡³æ–‡ä»¶ {checkpoint_path} ä¸­...\")\n",
    "try:\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(\"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜æˆåŠŸ!\")\n",
    "    print(f\"æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(checkpoint_path)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving checkpoint: {e}\")\n",
    "\n",
    "# æ¸…ç†\n",
    "# del model, optimizer, checkpoint # åœ¨å®é™…è„šæœ¬ä¸­ï¼Œè¿™äº›é€šå¸¸ä¼šä¸€ç›´å­˜åœ¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649bf14-d7da-42d1-83f9-8f6062d19b74",
   "metadata": {},
   "source": [
    "#### ä»æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625917d0-45de-4d40-950b-05295b75d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»§ç»­ä¸Šé¢çš„ä¾‹å­...\n",
    "\n",
    "# å®šä¹‰æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ (å‡è®¾å®ƒå·²ç»å­˜åœ¨)\n",
    "checkpoint_path = 'my_training_checkpoint.pth'\n",
    "\n",
    "# --- åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ¢å¤è®­ç»ƒ ---\n",
    "print(f\"\\næ­£åœ¨ä»æ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_path} å¼€å§‹æ¢å¤è®­ç»ƒ...\")\n",
    "\n",
    "# ç¡®å®šåŠ è½½è®¾å¤‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Loading onto device: {device}\")\n",
    "\n",
    "try:\n",
    "    # 1. åŠ è½½æ£€æŸ¥ç‚¹å­—å…¸ï¼Œå¹¶æŒ‡å®šæ˜ å°„è®¾å¤‡\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(\"æ£€æŸ¥ç‚¹å­—å…¸è£…è½½å®Œæˆ...\")\n",
    "\n",
    "    # 2. å®ä¾‹åŒ–æ¨¡å‹ (ç»“æ„å¿…é¡»ä¸ä¿å­˜æ—¶ä¸€è‡´)\n",
    "    loaded_model = SimpleFNN(input_size, hidden_size, output_size)\n",
    "    # å°†æ¨¡å‹ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡\n",
    "    loaded_model.to(device)\n",
    "    print(\"åˆ›å»ºæ–°æ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡...\")\n",
    "\n",
    "    # 3. å®ä¾‹åŒ–ä¼˜åŒ–å™¨ (ç±»å‹å’Œé…ç½®åº”ä¸ä¿å­˜æ—¶ä¸€è‡´)\n",
    "    # !!! ä½¿ç”¨åŠ è½½å‚æ•°åçš„æ¨¡å‹çš„å‚æ•°åˆå§‹åŒ–ä¼˜åŒ–å™¨ !!!\n",
    "    loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=lr) # åˆå§‹ lr ä¼šè¢« state_dict è¦†ç›–\n",
    "    print(\"åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨å®ä¾‹...\")\n",
    "\n",
    "    # 4. åŠ è½½æ¨¡å‹çŠ¶æ€\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½æ¨¡å‹çš„state_dict...\")\n",
    "\n",
    "    # 5. åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€\n",
    "    loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(\"ä»æ£€æŸ¥ç‚¹æ–‡ä»¶åŠ è½½ä¼˜åŒ–å™¨çš„state_dict...\")\n",
    "\n",
    "    # 6. æ¢å¤å…¶ä»–çŠ¶æ€\n",
    "    start_epoch = checkpoint['epoch'] + 1 # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹\n",
    "    recovered_step = checkpoint['step']\n",
    "    recovered_loss = checkpoint['loss']\n",
    "    recovered_best_accuracy = checkpoint['best_accuracy']\n",
    "\n",
    "    # å¦‚æœä½¿ç”¨äº†è°ƒåº¦å™¨\n",
    "    # loaded_scheduler = optim.lr_scheduler.StepLR(loaded_optimizer, step_size=30, gamma=0.1)\n",
    "    # loaded_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    # print(\"Scheduler state loaded.\")\n",
    "\n",
    "    # å¦‚æœä¿å­˜äº† RNG çŠ¶æ€å¹¶éœ€è¦ä¸¥æ ¼å¤ç°\n",
    "    # torch.set_rng_state(checkpoint['torch_rng_state'])\n",
    "    # np.random.set_state(checkpoint['numpy_rng_state'])\n",
    "    # random.setstate(checkpoint['random_rng_state'])\n",
    "    # print(\"RNG states restored.\")\n",
    "\n",
    "    print(f\"\\nä» Epoch {start_epoch}å’Œ Step {recovered_step} ç»§ç»­è¿›è¡Œè®­ç»ƒ...\")\n",
    "    print(f\"Recovered Loss: {recovered_loss}, Best Accuracy: {recovered_best_accuracy}\")\n",
    "\n",
    "\n",
    "    # 7. è®¾ç½®æ¨¡å‹æ¨¡å¼ (æ¢å¤è®­ç»ƒåˆ™è®¾ç½®ä¸º train())\n",
    "    loaded_model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    print(\"å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼...\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Checkpoint file not found at {checkpoint_path}. Starting training from scratch.\")\n",
    "    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½ ä¼šé€‰æ‹©ä»å¤´å¼€å§‹è®­ç»ƒ\n",
    "    # start_epoch = 0\n",
    "    # loaded_model = SimpleFNN(...)\n",
    "    # loaded_optimizer = optim.Adam(...)\n",
    "    # etc.\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during loading checkpoint: {e}\")\n",
    "     # æ ¹æ®éœ€è¦å¤„ç†åŠ è½½å¤±è´¥çš„æƒ…å†µ\n",
    "finally:\n",
    "     # æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶ (å¯é€‰)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(f\"\\næ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼š{checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
