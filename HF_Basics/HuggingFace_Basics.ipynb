{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1390694c-8e78-4a32-a71e-3ce817a28aa7",
   "metadata": {},
   "source": [
    "## HF加载Tokenizer的方法\n",
    "\n",
    "- transformers库中的加载方法：对tokenizers库的高层封装，提供与模型（如BERT、Llama）无缝集成的接口\n",
    "- tokenizers库中的加载方法：底层实现，提供更高效的分词（Rust-based）机制，常用于transformers库的快速Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fad84f-1b8c-4168-b20b-78938929c83d",
   "metadata": {},
   "source": [
    "### transformers库中的加载方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7156059-53da-4d1f-8f29-152cb727be38",
   "metadata": {},
   "source": [
    "#### AutoTokenizer.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6a5af-7d46-4c43-bfb4-ec6daf5d7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # 自动加载 BertTokenizerFast\n",
    "inputs = tokenizer(\"Hello, MageEdu!\", return_tensors=\"pt\")  # 输出: {'input_ids': tensor([[101, 7592, 1010, 2088, 999, 102]]), 'attention_mask': ...}\n",
    "print(inputs)  # 输出{'input_ids': tensor([[  101,  7592,  1010, 17454,  2098,  2226,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))  # \" [CLS] hello, MageEdu! [SEP] \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0460a8-f776-4b61-a1f1-c98fb3e14782",
   "metadata": {},
   "source": [
    "#### 具体Tokenizer类.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367dd5d-c17a-476d-840a-fe352dec407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.add_tokens([\"new_token1\", \"new_token2\"])  # 添加自定义标记\n",
    "inputs = tokenizer(\"Hello, [new_token1]!\", return_tensors=\"pt\")\n",
    "print(inputs)  # 输出包含新标记的 input_ids\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d75e1f-8b1e-47cf-8c23-8647fcf4fc1b",
   "metadata": {},
   "source": [
    "### tokenizers库中的加载方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14027bec-c741-4c30-a508-571a77842565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoding = tokenizer.encode(\"Hello, world!\")\n",
    "print(encoding.tokens)  # ['[CLS]', 'hello', ',', 'world', '!', '[SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b00a3-13eb-424d-9051-5c6e34c7e7c1",
   "metadata": {},
   "source": [
    "## HF加载模型进行任务推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3e458-28c8-451a-91f2-5d8885e8581b",
   "metadata": {},
   "source": [
    "### 使用模型类进行自动加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5a03b-a974-40f1-872a-d7bce93832e9",
   "metadata": {},
   "source": [
    "#### AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7aa7c-b79b-4caa-b738-df4889ee5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. & 2. 加载分词器和模型\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    # CausalLM模型如GPT-2默认没有pad token，需要手动设置\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. 预处理：将文本转换为模型可理解的输入张量\n",
    "input_text = \"The future of AI is\"\n",
    "# 使用 __call__ 方法 (更推荐) 编码，并返回PyTorch张量\n",
    "inputs = tokenizer(input_text, return_tensors='pt')  # 编码并返回PyTorch张量（包括input_ids和attention_mask）\n",
    "# 从inputs字典中提取出input_ids张量\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# 4. 模型推理：获得原始输出（logits）\n",
    "# 在推理时禁用梯度，节省内存并加速\n",
    "with torch.no_grad():\n",
    "    # 使用 generate 方法进行生成，并添加采样策略以提高多样性\n",
    "    # - do_sample=True: 启用采样\n",
    "    # - top_k=30: 从概率最高的30个词中采样\n",
    "    # - temperature=0.9: 控制随机性\n",
    "    #outputs = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id)  # 使用generate方法进行生成\n",
    "    #outputs = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id, do_sample=True)\n",
    "    #outputs = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id, do_sample=True, top_k=30)\n",
    "    outputs = model.generate(input_ids, max_length=100, pad_token_id=tokenizer.pad_token_id, do_sample=True, top_k=30, temperature=0.9)\n",
    "\n",
    "# 5. 后处理：将模型输出的token IDs解码回文本\n",
    "# outputs[0] 是生成的 token ID 序列\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934343d-51fc-4f22-b511-65efed26fb82",
   "metadata": {},
   "source": [
    "#### AutoModel指定Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b0c88-8770-40c0-8e8a-3103c0fbfe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- 1. 设备和模型定义 ---\n",
    "# 自动选择最佳设备（GPU优先，否则CPU）\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# --- 2. 加载分词器和模型 ---\n",
    "# 加载模型时直接将其移动到选定的设备\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 设置分词器的 padding token，这对批处理很重要\n",
    "if tokenizer.pad_token is None:\n",
    "    # CausalLM模型如GPT-2默认没有pad token，需要手动设置\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- 3. 预处理：使用更安全的 __call__ 方法 ---\n",
    "input_text = \"The future of AI is\"\n",
    "# 使用 __call__ 方法 (更推荐) 编码，并返回PyTorch张量\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# --- 4. 模型推理：使用更灵活的 generate 方法 ---\n",
    "# 在推理时禁用梯度，节省内存并加速\n",
    "with torch.no_grad():\n",
    "    # 使用 generate 方法进行生成，并添加采样策略以提高多样性\n",
    "    # - do_sample=True: 启用采样\n",
    "    # - top_k=50: 从概率最高的50个词中采样\n",
    "    # - temperature=0.9: 控制随机性\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "# --- 5. 后处理：解码回文本 ---\n",
    "# outputs[0] 是生成的 token ID 序列\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"输入设备: {device}\")\n",
    "print(\"---\")\n",
    "print(f\"生成的文本:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e91de-f8cf-4238-95ef-e93d0f35042f",
   "metadata": {},
   "source": [
    "### Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92a0b8-6cc3-4dbb-a10a-b321de5d4d47",
   "metadata": {},
   "source": [
    "#### 极简示例\n",
    "\n",
    "最小可运行示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893df209-35bd-4c85-8f83-f5f7543d82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 最简单：情感分析（会自动选择默认模型）\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "text_input = [\"I love MageEdu.\", \"A terrible day.\"]\n",
    "\n",
    "results = classifier(text_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7397bd-d27a-44f1-b0a2-9ae38cbd829d",
   "metadata": {},
   "source": [
    "#### 指定模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa249b1-4516-498c-aef4-a61e8842bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 最简单：情感分析（会自动选择默认模型）\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "text_input = [\"I love MageEdu.\", \"A terrible day.\"]\n",
    "\n",
    "results = classifier(text_input)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c670fa-12e5-4483-b743-05f819a64d7b",
   "metadata": {},
   "source": [
    "#### 指定模型、设备和深度学习框架的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6093b-3481-4f99-9141-c05f1741f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"gpt2\",           # 或者本地路径 \"./my_model\"\n",
    "    device=0,               # -1 为 CPU，非负 int 表示 CUDA 设备编号\n",
    "    framework=\"pt\",         # \"pt\" 或 \"tf\"（需已安装相应框架）\n",
    ")\n",
    "\n",
    "text_input = \"Once upon a time\"\n",
    "\n",
    "print(pipe(text_input, max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068725f-4691-475e-b9f0-f4e810c21b01",
   "metadata": {},
   "source": [
    "#### 指定模型、设备和深度学习框架的示例2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9b189-14d3-43d9-ae9f-dad54112c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"Qwen/Qwen3-0.6B\",           # 或者本地路径 \"./my_model\"\n",
    "    device=0,               # -1 为 CPU，非负 int 表示 CUDA 设备编号\n",
    "    framework=\"pt\",         # \"pt\" 或 \"tf\"（需已安装相应框架）\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "text_input = \"从前有座山，\"\n",
    "results = pipe(text_input, max_length=256)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9ceca-ed42-4a44-9769-4ded9dac7cac",
   "metadata": {},
   "source": [
    "#### 任务的专用参数示例\n",
    "\n",
    "文本生成（text-generation）的专用参数\n",
    "- max_length：控制生成文本的最大长度（输入+输出）\n",
    "- max_new_tokens：控制模型新生成的最大token数量\n",
    "- do_sample：是否使用采样策略（如设置为 True可生成更多样化的文本，False则为贪婪解码）\n",
    "- temperature：控制采样随机性，值越高输出越随机，越低则越确定\n",
    "- top_k：在采样时，仅从概率最高的k个token中选取\n",
    "- top_p：(nucleus sampling) 仅从累积概率超过p的最小token集合中选取\n",
    "- num_return_sequences：指定生成多少个不同的序列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdc9c2-14b0-49e2-b67a-5e1e932a50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"Qwen/Qwen3-0.6B\",           # 或者本地路径 \"./my_model\"\n",
    "    device=0,               # -1 为 CPU，非负 int 表示 CUDA 设备编号\n",
    "    framework=\"pt\",         # \"pt\" 或 \"tf\"（需已安装相应框架）\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "text_input = \"从前有座山，\"\n",
    "results = pipe(text_input, max_new_tokens=256, top_k=50, temperature=0.8)  # 文本生成任务的专用参数max_new_tokens、top_k和temperature\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d1663-31fe-4ec8-9994-8ddb971fe45e",
   "metadata": {},
   "source": [
    "#### 将前面AutoModel的示例改为Pipeline的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50f875-95cd-4135-a472-a73d4dccabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# --- 1. 设备和模型定义 ---\n",
    "# 推荐：自动选择最佳设备（GPU优先，否则CPU）\n",
    "# 注意：pipeline 默认在支持的情况下会使用 GPU，但最好显式指定\n",
    "if torch.cuda.is_available():\n",
    "    # 使用第一个 GPU (device=0)\n",
    "    device = 0 \n",
    "else:\n",
    "    # 否则使用 CPU (device=-1)\n",
    "    device = -1 \n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "input_text = \"欢迎来到马哥教育学习云计算/SRE、大模型、网络安全、云原生、DevOps等课程\"\n",
    "\n",
    "# --- 2. 实例化 pipeline 对象 (一步完成加载、配置和设备分配) ---\n",
    "generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# --- 3. 调用 pipeline 进行推理 (一步完成预处理、推理和后处理) ---\n",
    "# 将原始模型推理时的参数直接传递给 pipeline\n",
    "# Pipeline 内部会将这些参数传给底层的 model.generate() 方法\n",
    "results = generator(\n",
    "    input_text,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,         # 启用采样\n",
    "    top_k=50,               # 从概率最高的 50 个词中采样\n",
    "    temperature=0.9,        # 控制随机性\n",
    "    num_return_sequences=1  # 只返回一个生成的序列\n",
    ")\n",
    "\n",
    "# --- 4. 打印最终结果 ---\n",
    "# Pipeline 返回一个列表，其中包含一个或多个字典\n",
    "generated_text = results[0]['generated_text']\n",
    "\n",
    "print(f\"使用的设备索引: {device} (0为GPU, -1为CPU)\")\n",
    "print(\"---\")\n",
    "print(f\"生成的文本:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bea326-79d0-4c09-9ccd-f94ae382c14c",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b01c07-71fe-458e-a8ca-fffb380d86ca",
   "metadata": {},
   "source": [
    "### 数据集基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb74e5d-76ef-45ab-8269-d4cdc50821cd",
   "metadata": {},
   "source": [
    "#### 安装datasets\n",
    "\n",
    "```bash\n",
    "pip install datasets\n",
    "```\n",
    "\n",
    "还有两个可选的扩展，若需要，可以安装\n",
    "```bash\n",
    "pip install datasets[audio]   # 音频数据集\n",
    "pip install datasets[vision]  # 图像/视频数据集\n",
    "```\n",
    "\n",
    "下面的代码如果能够执行成功，则代码datasets库安装成功，且能够正常加载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124fcff-eb2a-4b78-aa49-1a3446f96537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset;ds = load_dataset('wikitext', 'wikitext-2-raw-v1');print(ds['train'], ds['validation'], ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5b767-08c2-4c3e-a448-08e331f4a96b",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a875bc-8e4a-4783-993a-6569382df600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载整个数据集（通常返回包含train、test等split的DatasetDict）\n",
    "dataset_dict = load_dataset(\"imdb\")\n",
    "print(dataset_dict['train'][0])     # 访问训练集第一条数据\n",
    "\n",
    "# 加载特定子集切分\n",
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "print(f\"\\nimdb训练集:\\n {train_dataset}\")\n",
    "\n",
    "# 加载数据集的特定配置和部分数据\n",
    "wikitext_103_subset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train[:100]\")\n",
    "print(f\"\\nwikitext_103训练集的前100行:\\n {wikitext_103_subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dc8d5-2d1e-4705-8831-63b4258d8247",
   "metadata": {},
   "source": [
    "#### 流式加载超大数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6eba1-9d36-477a-ba17-17aafdd3715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=\"train\", streaming=True)\n",
    "\n",
    "# 流式数据集可迭代，并可结合skip, take, shuffle等进行操作\n",
    "for example in streamed_dataset.take(10):  # 只取前10个样本\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28e7ec-4f5e-4a92-a21b-7eef26ae39d4",
   "metadata": {},
   "source": [
    "#### 基本信息与结构检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd12622-b7aa-4657-acb8-75d1a55943a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载数据集\n",
    "dataset_dict = load_dataset(\"imdb\")\n",
    "\n",
    "# 1. 查看DatasetDict的键（ splits）\n",
    "print(\"数据划分:\", list(dataset_dict.keys()))\n",
    "\n",
    "# 2. 获取训练集Dataset对象\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "\n",
    "# 3. 查看数据集形状（样本数, 列数）\n",
    "print(\"数据集形状 (行, 列):\", train_dataset.shape)\n",
    "\n",
    "# 4. 查看列名\n",
    "print(\"列名:\", train_dataset.column_names)\n",
    "\n",
    "# 5. 查看特征详情\n",
    "print(\"特征详情:\", train_dataset.features)\n",
    "\n",
    "# 6. 查看样本数量\n",
    "print(\"训练集样本数:\", train_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fabb4c-6451-428e-9022-5a8584b325d7",
   "metadata": {},
   "source": [
    "#### 数据访问与预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2e6b9-28b7-4211-b112-56019dc4bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd # 引入 pandas 确保 to_pandas 可用\n",
    "\n",
    "# 1. 加载 IMDb 数据集（情感分析）\n",
    "# load_dataset默认返回DatasetDict，我们选择'train'切分进行操作\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "dataset = raw_datasets['train']\n",
    "\n",
    "print(f\"--- 成功加载 IMDb 训练集，包含 {len(dataset)} 条记录 ---\")\n",
    "print(f\"--- dataset 变量类型检查: {type(dataset)} ---\\n\") \n",
    "# 确保 dataset 是 <class 'datasets.arrow_dataset.Dataset'>\n",
    "\n",
    "# 1. 索引访问 (dataset[0])\n",
    "print(\"1. 索引访问 (dataset[0]): 查看第一个样本\")\n",
    "example_0 = dataset[0]\n",
    "print(f\"评论 (text): {example_0['text'][:70]}...\")\n",
    "print(f\"标签 (label): {example_0['label']} (0=负面, 1=正面)\\n\")\n",
    "\n",
    "# 2. 切片访问 (dataset[:3])\n",
    "# 直接切片操作，返回一个新的 Dataset 对象\n",
    "first_three_examples = dataset[:3]\n",
    "print(\"2. 切片访问 (dataset[:3]): 查看前三个样本\")\n",
    "print(f\"类型: {type(first_three_examples)}\")\n",
    "print(f\"第三条评论: {first_three_examples['text'][2][:50]}...\\n\")\n",
    "\n",
    "# 3. 访问单列 (dataset['column_name'][index])\n",
    "print(\"3. 访问单列 (dataset['label'][:5]): 查看前五个样本的标签\")\n",
    "labels_list = dataset['label'][:5]\n",
    "print(f\"前五个标签: {labels_list}\\n\")\n",
    "\n",
    "# 4. dataset.select(indices)\n",
    "random_indices = random.sample(range(len(dataset)), 3)\n",
    "selected_subset = dataset.select(random_indices)\n",
    "\n",
    "print(f\"4. dataset.select({random_indices}): 查看随机选择的 3 个样本\")\n",
    "print(f\"用于选择的索引: {random_indices}\") \n",
    "print(f\"第一个随机样本评论: {selected_subset['text'][0][:50]}...\\n\")\n",
    "\n",
    "\n",
    "# 5. dataset.to_pandas()\n",
    "# 使用 select(range(10)) 获取前10条记录的子集，确保它是一个Dataset对象\n",
    "df_subset = dataset.select(range(10)) \n",
    "df = df_subset.to_pandas()\n",
    "\n",
    "print(\"5. dataset.to_pandas() (前 10 条): 转换为DataFrame便于表格化预览\")\n",
    "print(f\"子集类型检查: {type(df_subset)}\") # 再次检查子集类型\n",
    "print(df[['text', 'label']])\n",
    "print(\"\\n\")\n",
    "\n",
    "# 6. dataset.data.table\n",
    "arrow_table = dataset.data.table\n",
    "\n",
    "print(\"6. dataset.data.table: 访问底层 Arrow 表\")\n",
    "print(f\"Arrow 表的 Schema: {arrow_table.schema}\")\n",
    "# 仅展示 Arrow 头部信息\n",
    "print(f\"前 2 条 Arrow 数据: {arrow_table.slice(0, 2).to_pydict()['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb6a74-6932-4990-b9e7-8b01222de3aa",
   "metadata": {},
   "source": [
    "#### 数据处理与转换 - map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b45db-bf8b-47b4-82f9-04c7246fc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载IMDB数据集示例\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:10]\")  # 取前10条样本\n",
    "\n",
    "#-----------------\n",
    "# 处理单个样本\n",
    "#-----------------\n",
    "\n",
    "# 示例1: 将文本转换为小写\n",
    "def lowercase_function(example):\n",
    "    example[\"text\"] = example[\"text\"].lower()\n",
    "    return example\n",
    "\n",
    "dataset_lower = dataset.map(lowercase_function)\n",
    "print(dataset_lower[0][\"text\"])  # 将第1行输出为小写文本\n",
    "\n",
    "# 示例2: 添加新特征（文本长度）\n",
    "def add_text_length(example):\n",
    "    example[\"text_length\"] = len(example[\"text\"].split())  # 计算词数\n",
    "    return example\n",
    "\n",
    "dataset_with_length = dataset.map(add_text_length)\n",
    "print(dataset_with_length[0][\"text_length\"])  # 输出文本长度\n",
    "\n",
    "#--------------------\n",
    "# 处理批次样本\n",
    "#--------------------\n",
    "# 示例3: 批量文本小写化\n",
    "def lowercase_batch(batch):\n",
    "    batch[\"text\"] = [text.lower() for text in batch[\"text\"]]\n",
    "    return batch\n",
    "\n",
    "dataset_lower_batch = dataset.map(lowercase_batch, batched=True)\n",
    "\n",
    "# 示例4: 批量添加文本长度\n",
    "def add_length_batch(batch):\n",
    "    batch[\"text_length\"] = [len(text.split()) for text in batch[\"text\"]]\n",
    "    return batch\n",
    "\n",
    "dataset_with_length_batch = dataset.map(add_length_batch, batched=True, batch_size=500)  # 自定义批次大小\n",
    "\n",
    "# 示例5: 使用Tokenizer进行批量分词 (常见且高效)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    # tokenizer会自动处理列表输入\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset.column_names)  # 查看新增的input_ids, attention_mask等列\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda9465-cd46-4151-a3ce-fab5ec194e7d",
   "metadata": {},
   "source": [
    "#### 数据处理与转换 - filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc51f3c-b87d-415d-acd3-a09d9e1b7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载IMDB数据集示例\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")   # train子集中的全部样本\n",
    "\n",
    "# 示例1： 保留文本长度超过100个字符的评论\n",
    "long_reviews = dataset.filter(lambda example: len(example[\"text\"]) > 100)\n",
    "print(\"满足条件的行数（文本长度超过100个字符）：\", len(long_reviews))\n",
    "print(f\"第一个样本评论: {long_reviews[0]['text']}\")\n",
    "\n",
    "# 示例2： 仅保留正面评论（假设标签label为 1 代表正面）\n",
    "positive_reviews = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "print(\"满足条件的行数（正面评论）：\", len(positive_reviews))\n",
    "\n",
    "# 示例3： 多条件过滤，保留正面且文本较长的评论\n",
    "positive_long_reviews = dataset.filter(\n",
    "    lambda example: example[\"label\"] == 1 and len(example[\"text\"]) > 500\n",
    ")\n",
    "print(\"满足条件的行数（正面评论且文本长度大于500）：\", len(positive_long_reviews))\n",
    "\n",
    "# 示例4： 根据文本内容筛选（包含特定关键词）\n",
    "# 筛选出评论中包含特定关键词的样本，例如包含 \"disaster\"（灾难）的评论\n",
    "# 定义筛选函数：文本中包含 \"disaster\" 关键词\n",
    "def contains_keyword(example):\n",
    "    # 使用 Python 的 in 操作符进行字符串查找（不区分大小写）\n",
    "    return \"disaster\" in example['text'].lower()\n",
    "\n",
    "disaster_dataset = dataset.filter(contains_keyword)\n",
    "print(f\"筛选后的样本数（包含disaster关键词）: {len(disaster_dataset)}\")\n",
    "print(f\"第一个样本评论: {disaster_dataset[0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c85c63-ab44-4a86-8117-0962069a7076",
   "metadata": {},
   "source": [
    "#### 数据集拆分与合并\n",
    "\n",
    "将IMDB的原始训练集拆分为训练集和验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92403b-ba7c-4420-accf-bd306865ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "#----------------------\n",
    "# 数据集拆分示例\n",
    "#----------------------\n",
    "\n",
    "# 1. 加载IMDb训练集 (25000行)\n",
    "dataset = load_dataset(\"imdb\", split='train')\n",
    "\n",
    "# 2. 将数据集划分为80%训练集和20%验证集\n",
    "split_dict = dataset.train_test_split(\n",
    "    test_size=0.2, \n",
    "    seed=42,\n",
    "    # 假设需要分层抽样以保持 label (0/1) 比例一致\n",
    "    stratify_by_column='label' \n",
    ")\n",
    "\n",
    "# 3. 结果是一个 DatasetDict\n",
    "train_ds = split_dict['train']   # 20000 行\n",
    "val_ds = split_dict['test']      # 5000 行\n",
    "\n",
    "print(f\"训练集大小: {len(train_ds)}\")\n",
    "print(f\"验证集大小: {len(val_ds)}\")\n",
    "\n",
    "#----------------------\n",
    "# 数据集合并示例\n",
    "#----------------------\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# 4. 模拟两个具有相同结构的子集\n",
    "ds_part1 = dataset.select(range(10)) # 通过指定索引列表，从数据集中精确选择一个或多个样本子集（前10行）\n",
    "ds_part2 = dataset.select(range(10, 20)) # 10-19 行\n",
    "\n",
    "print(f\"Part 1 大小: {len(ds_part1)}\")\n",
    "print(f\"Part 2 大小: {len(ds_part2)}\")\n",
    "\n",
    "# 5. 按行合并 (axis=0)\n",
    "merged_dataset = concatenate_datasets([ds_part1, ds_part2])\n",
    "\n",
    "print(f\"\\n按行合并后的总大小: {len(merged_dataset)}\") # 20 行\n",
    "\n",
    "# 6. 验证合并结果（第一个样本来自 Part 1, 第 11 个样本来自 Part 2）\n",
    "print(f\"第一个样本的 text: {merged_dataset[0]['text'][:30]}...\")\n",
    "print(f\"第十一个样本的 text: {merged_dataset[10]['text'][:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4da83-0b5f-4a46-922e-371af3c7666d",
   "metadata": {},
   "source": [
    "#### 列操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcb8f8-7566-402a-9e0c-abc491cc634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Value\n",
    "\n",
    "# 1. 加载数据\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\") # 取前1000条作为示例\n",
    "print(\"数据集原有列的列名:\", dataset.column_names)\n",
    "print(\"数据集原有列的Features:\", dataset.features)\n",
    "\n",
    "# 2. 重命名列（标准化）\n",
    "dataset = dataset.rename_column(\"text\", \"review\")\n",
    "print(\"改字字段名称后（text->review）的数据集列名:\", dataset.column_names)\n",
    "\n",
    "# 3. 添加新特征列（文本长度）\n",
    "text_lengths = [len(review.split()) for review in dataset[\"review\"]]\n",
    "dataset = dataset.add_column(\"review_length\", text_lengths)\n",
    "print(\"添加列后的数据集列名:\", dataset.column_names)\n",
    "print(\"各个列的Features（含新增列review_length）:\", dataset.features)\n",
    "\n",
    "# 4. 转换新列的数据类型\n",
    "new_features = dataset.features.copy()\n",
    "new_features[\"review_length\"] = Value(\"int32\") # 使用更节省空间的int32\n",
    "dataset = dataset.cast(new_features)\n",
    "print(\"新增列（review_length）Features修改后的结果:\", dataset.features)\n",
    "\n",
    "# 5. 最终移除不必要的中间列（或原始列）\n",
    "# 假设我们只需要清洗后的评论和标签\n",
    "dataset = dataset.remove_columns([\"review_length\"]) # 移除临时列\n",
    "\n",
    "print(\"最终数据集列名:\", dataset.column_names)\n",
    "print(\"最终第一条数据:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc76d11-478b-4e2d-b015-974a2f4137cb",
   "metadata": {},
   "source": [
    "#### 动态格式设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba192647-0a01-4c48-8801-30d16c99c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 加载数据集\n",
    "ds = load_dataset(\"imdb\", split=\"train[:100]\") # 取前100条样本以便快速演示\n",
    "\n",
    "# 2. 加载分词器 (例如，使用BERT的分词器)\n",
    "model_name = \"bert-base-uncased\" # 可选: \"distilbert-base-uncased\", \"roberta-base\" 等\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    # 对文本进行分词处理。返回一个包含'input_ids', 'attention_mask'等的字典\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=True, # 填充到批次内最大长度，更高效，也可用 \"max_length\" 填充到固定长度\n",
    "        truncation=True, # 截断到模型最大长度\n",
    "        max_length=512, # 指定最大长度（例如512）\n",
    "        # return_tensors=\"pt\"  # 通常不在map中设置，用set_format统一转换更高效\n",
    "    )\n",
    "\n",
    "# 4. 应用分词函数到整个数据集\n",
    "# batched=True 可以显著加速处理\n",
    "tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# 5. 查看处理后的数据集特征（确认新增了分词器产生的列）\n",
    "# 注意其中的attention_mask是填充掩码，不是因果掩码\n",
    "print(f\"处理后的数据集特征：{tokenized_ds.column_names}\")  # 通常输出: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "# 6. 移除原始文本列（如果不需要），以节省内存和显存\n",
    "tokenized_ds = tokenized_ds.remove_columns([\"text\"])\n",
    "\n",
    "# 7. 设置数据集格式，转换为PyTorch张量，并指定需要的列\n",
    "tokenized_ds.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"] # 选择模型需要的列\n",
    ")\n",
    "\n",
    "# 8. 创建DataLoader\n",
    "dataloader = DataLoader(\n",
    "    tokenized_ds, \n",
    "    batch_size=8, \n",
    "    shuffle=True, # 训练时通常打乱数据\n",
    "    # num_workers=4,  # 可选：使用多进程加载数据以加速（Windows下有时需设为0）\n",
    "    # pin_memory=True # 可选：如果使用GPU，设置为True可以加速数据从CPU到GPU的传输\n",
    ")  \n",
    "\n",
    "# 9. 迭代DataLoader\n",
    "for batch in dataloader:\n",
    "    # batch 是一个字典，键是列名，值是对应的张量（形状为 [batch_size, ...]）\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"label\"]\n",
    "    \n",
    "    # 接下来，通常在这里将批次数据送入模型进行前向传播\n",
    "    # outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    # loss = outputs.loss\n",
    "    # ... 其余训练步骤（计算损失、反向传播、优化器步进等）\n",
    "    \n",
    "    # 以下是打印信息以供调试和理解\n",
    "    print(f\"Input ID的形状: {input_ids.shape}\")\n",
    "    print(f\"注意力掩码的开关：: {attention_mask.shape}\")\n",
    "    print(f\"标签: {labels}\")\n",
    "    \n",
    "    # 通常在这里执行训练循环，这里只是一个示例，所以只运行一个批次就退出\n",
    "    break # 移除这个 break 来遍历整个数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2859a2a-3d98-4f68-9bb3-8eb97e704b35",
   "metadata": {},
   "source": [
    "## Evalute库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19caf681-f332-4ee1-996b-4479ec05d029",
   "metadata": {},
   "source": [
    "### 基础使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c809df-4ecd-43c0-ad95-710a5eff07df",
   "metadata": {},
   "source": [
    "#### Metric API基础\n",
    "\n",
    "**核心是两个方法：load()和compute()**\n",
    "- evaluate.load(metric_name, **kwargs)​：用于加载指定的评估指标，常用参数\n",
    "  - metric_name（必选）：指标名称，如 \"accuracy\"、\"f1\"、\"bleu\"等\n",
    "  - module_type：指定评估类型，默认为 'metric'，可选 'comparison'或 'measurement'，用于防止名称冲突\n",
    "  - cache_dir：缓存路径\n",
    "  - keep_in_memory：是否将数据保留在内存中\n",
    "- metric.compute(predictions, references, **kwargs)：传入预测值和参考值，计算指标结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa51b96-563a-44fc-89c6-b3de1041a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# 1. 加载准确率指标\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 2. 准备数据：真实标签（references）和模型预测标签（predictions）\n",
    "references = [0, 1, 0, 1, 1]  # 真实标签\n",
    "predictions = [0, 1, 1, 1, 1]  # 模型预测的标签\n",
    "\n",
    "# 3. 计算准确率\n",
    "results = accuracy_metric.compute(references=references, predictions=predictions)\n",
    "\n",
    "# 4. 查看结果\n",
    "print(results)  # 输出：{'accuracy': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff76152-8dda-4bbb-ae8c-9cb3a245edac",
   "metadata": {},
   "source": [
    "#### Evaluate的典型工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0444a-3a68-48d1-9aef-8432c1ae15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# 1. 加载指标\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 2. 循环添加批次\n",
    "for batch in dataloader:\n",
    "    preds = model(batch[\"input\"])\n",
    "    metric.add_batch(predictions=preds, references=batch[\"label\"])\n",
    "\n",
    "# 3. 计算总结果\n",
    "final_score = metric.compute()\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1419d-cda7-4e5f-92df-cbf38d492ff6",
   "metadata": {},
   "source": [
    "#### Evaluate与Trainer集成示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2db80d-40e1-48ff-be47-6beeb119964e",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa8a5f-d0f5-4ca5-a1b8-7ffa0e1bf272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate  # 用于评估指标\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 0. 设置训练时要使用的GPU，请注意要对照实际的硬件环境进行修改\n",
    "# 设置环境变量，只允许脚本看到并使用系统中的 GPU 0，以避免自动启用DPP（Jupyter环境不支持）\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "print(f\"可见GPU数: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# 1. 加载IMDB数据集与tokenizer\n",
    "# IMDB数据集包含50000条电影评论，标记为正面或负面情感\n",
    "dataset = load_dataset(\"imdb\")  # 加载IMDB数据集\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # 加载BERT的tokenizer\n",
    "\n",
    "# 数据预处理函数：将文本转换为模型可接受的输入格式\n",
    "def preprocess(examples):\n",
    "    # 使用tokenizer对文本进行编码，包括截断和填充到最大长度\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],           # IMDB数据集中文本字段名为\"text\"\n",
    "        truncation=True,            # 截断超过模型最大长度的文本\n",
    "        padding=\"max_length\",       # 填充到最大长度以确保统一尺寸\n",
    "        max_length=256              # 设置最大序列长度为256\n",
    "    )\n",
    "\n",
    "# 对数据集进行批量预处理\n",
    "encoded_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 2. 加载预训练模型\n",
    "# 使用BERT模型进行序列分类，IMDB是二分类任务（正面/负面）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2  # 设置分类标签数为2\n",
    ")\n",
    "\n",
    "# 3. 加载评估指标\n",
    "# 使用准确率作为评估指标 \n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 定义计算指标的函数，用于训练过程中的评估\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # 解包预测结果和真实标签\n",
    "    preds = logits.argmax(-1)   # 取logits中最大值的位置作为预测结果\n",
    "    return metric.compute(predictions=preds, references=labels)  # 计算准确率\n",
    "\n",
    "# 4. 配置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # 输出目录，用于保存模型和结果\n",
    "    eval_strategy=\"epoch\",          # 每轮结束后进行评估\n",
    "    save_strategy=\"epoch\",          # 每轮结束后保存模型\n",
    "    load_best_model_at_end=True,    # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",  # 用于选择最佳模型的指标\n",
    "    greater_is_better=True,         # 准确率越高越好\n",
    "    learning_rate=2e-5,             # 学习率，常用BERT微调的学习率\n",
    "    per_device_train_batch_size=16, # 每个设备的训练批量大小\n",
    "    per_device_eval_batch_size=16,  # 每个设备的评估批量大小\n",
    "    num_train_epochs=3,             # 训练轮数\n",
    "    weight_decay=0.01,              # 权重衰减，防止过拟合\n",
    ")\n",
    "\n",
    "# 创建Trainer实例，用于管理整个训练过程\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 要训练的模型\n",
    "    args=training_args,              # 训练参数\n",
    "    train_dataset=encoded_dataset[\"train\"],        # 训练集\n",
    "    eval_dataset=encoded_dataset[\"test\"],          # 评估集（使用测试集作为验证）\n",
    "    tokenizer=tokenizer,             # tokenizer\n",
    "    compute_metrics=compute_metrics  # 计算评估指标的函数\n",
    ")\n",
    "\n",
    "# 5. 启动训练与评估\n",
    "trainer.train()  # 开始训练模型\n",
    "\n",
    "# 训练完成后进行评估\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"评估结果: {eval_result}\")  # 打印评估结果\n",
    "\n",
    "# 可选：保存最终模型\n",
    "trainer.save_model(\"./imdb_sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc82533-09fb-4a23-900d-e127c8207ce6",
   "metadata": {},
   "source": [
    "### 训练示例\n",
    "\n",
    "运行下面的训练脚本，需要事先安装transformers、datasets、torch、numpy、evaluate、accelerate等库。\n",
    "\n",
    "另外，在一些特定的Hugging Face环境中（尤其是较新的transformers版本），即使accelerate版本符合要求，也需要安装accelerate的PyTorch后端依赖，或者transformers在导入时，没有找到accelerate库中所需的某些特定组件或路径。\n",
    "\n",
    "更常见的原因是，系统环境中安装了多个版本的PyTorch或accelerate，或者Python路径（sys.path）存在问题，导致transformers导入时找不到正确的accelerate安装。因此，如果遇到了无法找到accelerate的问题，可运行如下命令进行重新安装来解决问题。\n",
    "\n",
    "```bash\n",
    "pip install transformers[torch] --upgrade\n",
    "# 或者 确保 accelerate 被正确链接：\n",
    "pip install accelerate --upgrade\n",
    "```\n",
    "\n",
    "如果有必要，可以通过如下代码先检查python环境。\n",
    "```python\n",
    "import accelerate\n",
    "import transformers\n",
    "print(accelerate.__version__)\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__file__) # 确保路径正确\n",
    "```\n",
    "\n",
    "下面是一个Evaluate与Trainer集成的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e6a45-7808-4a1e-8d29-7061c0fa3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset             # 用于加载 Hugging Face 数据集\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import evaluate                             # Hugging Face 官方评估库，用于加载指标\n",
    "\n",
    "\n",
    "# 2. 数据准备与预处理\n",
    "# 定义使用的模型名称和超参数\n",
    "MODEL_NAME = \"bert-base-uncased\" # 选择一个 BERT 风格模型\n",
    "MAX_LENGTH = 128                 # 序列最大长度\n",
    "TRAIN_SAMPLE_PERCENT = 5         # 仅使用 1% 的训练数据进行快速演示\n",
    "EVAL_SAMPLE_PERCENT = 5          # 仅使用 1% 的测试数据进行评估\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 加载 IMDB 数据集，并按百分比截取\n",
    "print(f\"--- 正在加载数据集 (仅使用 {TRAIN_SAMPLE_PERCENT}% 训练集和 {EVAL_SAMPLE_PERCENT}% 测试集) ---\")\n",
    "# 标签列名为 'label'，文本列名为 'text'\n",
    "raw_train_datasets = load_dataset(\"imdb\", split=f\"train[:{TRAIN_SAMPLE_PERCENT}%]\")\n",
    "raw_eval_datasets = load_dataset(\"imdb\", split=f\"test[:{EVAL_SAMPLE_PERCENT}%]\")\n",
    "\n",
    "# 定义预处理（分词）函数\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"将文本转换为模型所需的 token ID 和注意力掩码\"\"\"\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# 对数据集应用预处理（并行处理）\n",
    "tokenized_train_datasets = raw_train_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_eval_datasets = raw_eval_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# 将原始标签列 'label' 重命名为 'labels'，这是 Trainer 默认识别的标签列名\n",
    "tokenized_train_datasets = tokenized_train_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval_datasets = tokenized_eval_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 移除原始文本列，只保留模型训练需要的 columns: input_ids, token_type_ids, attention_mask, labels\n",
    "tokenized_train_datasets = tokenized_train_datasets.remove_columns([\"text\"])\n",
    "tokenized_eval_datasets = tokenized_eval_datasets.remove_columns([\"text\"])\n",
    "\n",
    "# 最终训练和评估数据集\n",
    "train_dataset = tokenized_train_datasets\n",
    "eval_dataset = tokenized_eval_datasets\n",
    "\n",
    "# 3. 模型加载与指标函数定义 (Evaluate 库功能演示)\n",
    "# 3.1 加载模型：使用 AutoModelForSequenceClassification 进行二分类任务\n",
    "NUM_LABELS = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "# 3.2 加载评估指标：使用 evaluate 库加载准确率 (accuracy) 指标\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Trainer 在评估时会调用此函数。\n",
    "    输入 p 是一个 EvalPrediction 对象 (包含 predictions 和 label_ids)\n",
    "    \"\"\"\n",
    "    # 提取模型的 logits 或其他预测结果\n",
    "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    # 转换为最终的类别预测 ID\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # 使用 evaluate 库计算指标\n",
    "    return metric.compute(predictions=predictions, references=p.label_ids)\n",
    "\n",
    "# 4. 实例化 Trainer 并训练 (Trainer 库核心功能演示)\n",
    "# 4.1 定义训练参数 (TrainingArguments)\n",
    "OUTPUT_DIR = \"./results_trainer_demo\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                   # 检查点和输出的目录\n",
    "    num_train_epochs=1,                      # 训练周期数，设置为 1 快速完成\n",
    "    per_device_train_batch_size=8,           # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=8,            # 每个设备上的评估批次大小\n",
    "    warmup_steps=100,                        # 学习率预热步数\n",
    "    weight_decay=0.01,                       # 权重衰减\n",
    "    logging_dir='./logs_trainer_demo',       # TensorBoard 日志目录\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",                   # 评估策略: 每个 epoch 结束时进行评估\n",
    "    save_strategy=\"epoch\",                   # 保存策略: 每个 epoch 结束时保存模型\n",
    "    load_best_model_at_end=True,             # 训练结束后加载基于 metric_for_best_model 的最佳模型\n",
    "    metric_for_best_model=\"accuracy\",        # 定义衡量最佳模型的指标\n",
    "    report_to=\"none\",                        # 关闭外部报告工具以保持简洁\n",
    ")\n",
    "\n",
    "# 4.2 实例化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                             # 传入加载的模型\n",
    "    args=training_args,                      # 传入训练参数\n",
    "    train_dataset=train_dataset,             # 传入处理后的训练集\n",
    "    eval_dataset=eval_dataset,               # 传入处理后的评估集\n",
    "    tokenizer=tokenizer,                     # 传入分词器（用于数据整理和保存）\n",
    "    compute_metrics=compute_metrics,         # 传入指标计算函数\n",
    ")\n",
    "\n",
    "# 4.3 开始训练\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 启动模型训练 (Trainer.train()) ---\")\n",
    "print(\"=\"*50)\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 5. 评估和结果打印\n",
    "# 5.1 记录训练统计信息（如总步数、总时间）\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state() # 保存 Trainer 状态\n",
    "\n",
    "# 5.2 使用 Trainer.evaluate() 进行最终评估\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 启动模型评估 (Trainer.evaluate()) ---\")\n",
    "print(\"=\"*50)\n",
    "eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "# 5.3 打印评估结果，包括由 compute_metrics 计算的准确率\n",
    "print(\"\\n最终评估结果:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# 6. 模型保存（可选）\n",
    "# 保存最终模型和分词器\n",
    "FINAL_SAVE_PATH = f\"{OUTPUT_DIR}/final_model\"\n",
    "print(f\"\\n--- 保存最终模型到: {FINAL_SAVE_PATH} ---\")\n",
    "trainer.save_model(FINAL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(FINAL_SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- 任务流程已完成 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
